#!/opt/bin/python3.7

# Python header
__author__ = "Benjamin Herson-Macarel"
#__copyright__ = ""
__credits__ = ["Who", "wants", "to", "be", "added", "???"]
#__license__ = ""
__version__ = "1.0.1"
__maintainer__ = "Benjamin Herson-Macarel"
__email__ = "benjamin.herson-macarel-isc.france@ibm.com"
__status__ = "Stable"

# Imports
import os
import sys
import getopt
import time
import subprocess
import threading
import datetime
import socket
import ssl
import math
import json
import select
import re
import random
import signal
import traceback
import shutil
import logging

# The following are python packages from other developers. Dependencies need to be installed as a prerequisite
# requests for handling HTTP requets in efficient and simple way
import requests
from requests.exceptions import HTTPError
# Parsing ping results and get output in JSON (efficient and no time to waste developing the same...)
import pingparsing

# Classes

# Functions
def LoadDaemonConfig():
    """This function will get all the required parameters from config file and load them into vars   
    
    TODO: Describe the fcuntion, all vars and its content
    """
    # Switching vars to global
    global BaseDir
    global LogFilePath
    global ConfigFilePath
    global ELKMonitoringVersion
    global ECSVersion
    global ELKServers
    global ELKUsername
    global ELKPassword
    global ELKPort
    global ELKWebProtocol
    global ELKServers
    global ELKMetricIndexName
    global ELKLogIndexName
    global ELKServersAvailable
    global ELKServersFailed
    global FQDN
    global ReprocessingValueAtOnce
    global TailRefreshValue
    global CycleSleepTime
    global DiskSampleRate
    global TopProcesses
    global SystemProcessWaitValue
    global SystemFilesystemAndFstatWaitValue
    global SystemDiskIOWaitValue
    global SystemProcessSummaryWaitValue
    global SystemLoadWaitValue
    global SystemFcWaitValue
    global SystemSocketSummaryWaitValue
    global SystemSocketWaitValue
    global SystemMemoryWaitValue
    global SystemNetworkWaitValue
    global SystemCoreAndCpuWaitValue
    global IfSystemHPMStatEnable
    global IfSystemHypervisorEnable
    global SystemHPMStatWaitValue
    global SystemHypervisorWaitValue
    global TailReloadValue
    global PingPlotterTargets
    global PingPlotterWaitValue
    global PingSamples
    global PingTimeout
    global LPARAppName
    global HostingSite
    global QueueingTimerLimit
    global EntRestricted
    global FcsRestricted
    global HdiskRestricted
    
    # Checking if Configuration file exists
    if os.path.exists(ConfigFilePath):
        # Executing with Try/Catch to handle any error in file
        try:
            with open(ConfigFilePath, 'r') as ConfigFile:
                # Loading JSOn file content
                JSONValues = json.load(ConfigFile)

                # Set variables for ELK server.
                ELKMonitoringVersion = JSONValues["ELKMonitoringVersion"]
                ECSVersion = JSONValues["ECSVersion"]
                ELKServers = JSONValues["ELKServers"]
                ELKUsername = JSONValues["ELKUsername"]
                ELKPassword = JSONValues["ELKPassword"]
                ELKPort = JSONValues["ELKPort"]
                ELKWebProtocol = JSONValues["ELKWebProtocol"]
                # Duplicating ELKServers to ELKServersFailed making all ELK servers failed by default
                ELKServersFailed = ELKServers[:]
                ELKMetricIndexName = JSONValues["ELKMetricIndexName"]
                ELKLogIndexName = JSONValues["ELKLogIndexName"]
                FQDN = JSONValues["FQDN"]

                # os._exit(2)
                # Script functional variables
                # Maximum historical JSON messages (historized when ELK server was down) to proceed at each script Cycle
                ReprocessingValueAtOnce = int(JSONValues["ReprocessingValueAtOnce"])
                # Time to wait before refreshing tail's processes status (in seconds)
                TailRefreshValue = int(JSONValues["TailRefreshValue"])
                # Time to wait between two execution of the Main Loop (in seconds)
                CycleSleepTime = int(JSONValues["CycleSleepTime"])
                # How much time to run metric commands and get averaged results
                DiskSampleRate = int(JSONValues["DiskSampleRate"])
                # If different than 0, set the limit of processes that must be taken into account while checking TOP processes CPU activity
                TopProcesses = int(JSONValues["TopProcesses"])

                # Topic timers variables (in seconds)
                # Time to wait before sending new metric values to ELK server
                SystemProcessWaitValue = int(JSONValues["SystemProcessWaitValue"])
                SystemFilesystemAndFstatWaitValue = int(JSONValues["SystemFilesystemAndFstatWaitValue"])
                SystemDiskIOWaitValue = int(JSONValues["SystemDiskIOWaitValue"])
                SystemProcessSummaryWaitValue = int(JSONValues["SystemProcessSummaryWaitValue"])
                SystemLoadWaitValue = int(JSONValues["SystemLoadWaitValue"])
                SystemFcWaitValue = int(JSONValues["SystemFcWaitValue"])
                SystemSocketSummaryWaitValue = int(JSONValues["SystemSocketSummaryWaitValue"])
                SystemSocketWaitValue = int(JSONValues["SystemSocketWaitValue"])
                SystemMemoryWaitValue = int(JSONValues["SystemMemoryWaitValue"])
                SystemNetworkWaitValue = int(JSONValues["SystemNetworkWaitValue"])
                SystemCoreAndCpuWaitValue = int(JSONValues["SystemCoreAndCpuWaitValue"])

                # IMPROVEMENT / SAFETY - Time to wait before reloading a Tail process not modified since 1 day (in minutes)
                TailReloadValue = int(JSONValues["TailReloadValue"])
                # Calculating TailReloadValue depending on CycleSleepTime
                TailReloadValue = ((TailReloadValue * 60) / CycleSleepTime)
                
                # Loading PingPlotter targets and configuration values
                PingPlotterTargets = JSONValues["PingPlotterTargets"]
                PingPlotterWaitValue = int(JSONValues["PingPlotterWaitValue"])
                PingSamples = int(JSONValues["PingSamples"])
                PingTimeout = JSONValues["PingTimeout"]
                
                # Checking other JSON values linked to custom plugins
                IfSystemHPMStatEnable = JSONValues["IfSystemHPMStatEnable"]
                IfSystemHypervisorEnable = JSONValues["IfSystemHypervisorEnable"]
                SystemHPMStatWaitValue = int(JSONValues["SystemHPMStatWaitValue"])
                SystemHypervisorWaitValue = int(JSONValues["SystemHypervisorWaitValue"])
                                
                # Loading Application Name and DC site values
                LPARAppName = JSONValues["LPARAppName"]
                HostingSite = JSONValues["HostingSite"]
                
                # Loading QueueingTimerLimit to recycle JSON queues content after this amount of minutes (CPU performances)
                QueueingTimerLimit = int(JSONValues["QueueingTimerLimit"])
                
                # Loading some parameter that can restrict the number of devices checked for some cases
                # Network Adapters
                EntRestricted = JSONValues["EntRestricted"]
                # Fiber Channel Adapters
                FcsRestricted = JSONValues["FcsRestricted"]
                # HDISK devices
                HdiskRestricted = JSONValues["HdiskRestricted"]
        except:
            # problem encountered while loading JSON daemon configuration file, exiting
            logging.info("            ==> Error while loading " + ConfigFilePath + " daemon config file !")
            logging.info("                Please check the Configuration file JSON parameters, format and validity")
            traceback.print_exc()
            os._exit(2)
    else:
        # Daemon configuration file does not exist, exiting
        logging.info("            ==> Error while loading " + ConfigFilePath + " daemon config file !")
        logging.info("                Config file does not exist")
        os._exit(2)

    # Log
    # Log
    logging.info(" - Loading Daemon configuration... Done !")

    # Debug
    # print('\nDEBUG\n')
    # print('<!> DEBUG ', ELKMonitoringVersion, 'ELKMonitoringVersion')
    # print('<!> DEBUG ', ECSVersion, 'ECSVersion')
    # print('<!> DEBUG ', BaseDir, 'BaseDir')
    # print('<!> DEBUG ', Metricbeat_LogStash_HostName, 'Metricbeat_LogStash_HostName')
    # print('<!> DEBUG ', Filebeat_LogStash_HostName, 'Filebeat_LogStash_HostName')
    # print('<!> DEBUG ', Metricbeat_LogStash_PORT, 'Metricbeat_LogStash_PORT')
    # print('<!> DEBUG ', Filebeat_LogStash_PORT, 'Filebeat_LogStash_PORT')
    # print('<!> DEBUG ', FQDN, 'FQDN')
    # print('<!> DEBUG ', ReprocessingValueAtOnce, 'ReprocessingValueAtOnce')
    # print('<!> DEBUG ', TailRefreshValue, 'TailRefreshValue')
    # print('<!> DEBUG ', CycleSleepTime, 'CycleSleepTime')
    # print('<!> DEBUG ', DiskSampleRate, 'DiskSampleRate')
    # print('<!> DEBUG ', TopProcesses, 'TopProcesses')
    # print('<!> DEBUG ', SystemProcessWaitValue, 'SystemProcessWaitValue')
    # print('<!> DEBUG ', SystemFilesystemAndFstatWaitValue, 'SystemFilesystemAndFstatWaitValue')
    # print('<!> DEBUG ', SystemDiskIOWaitValue, 'SystemDiskIOWaitValue')
    # print('<!> DEBUG ', SystemProcessSummaryWaitValue, 'SystemProcessSummaryWaitValue')
    # print('<!> DEBUG ', SystemLoadWaitValue, 'SystemLoadWaitValue')
    # print('<!> DEBUG ', SystemFcWaitValue, 'SystemFcWaitValue')
    # print('<!> DEBUG ', SystemMemoryWaitValue, 'SystemMemoryWaitValue')
    # print('<!> DEBUG ', SystemNetworkWaitValue, 'SystemNetworkWaitValue')
    # print('<!> DEBUG ', SystemCoreAndCpuWaitValue, 'SystemCoreAndCpuWaitValue')
    # print('<!> DEBUG ', TailReloadValue, 'TailReloadValue')
    # print('\nFINDEBUG\n')
    pass

def LoadPluginConfig():
    """This function will get all the required information from Plugin config files and load them into vars   
    All files analyzed need to be in the same folder than main Parameters.conf file and have specific naming convention
    
    Please refer to README.md
    
    TODO: Describe the function, all vars and its content
    """
    # Log
    logging.info(" - Loading Filebeat plugin's config files...")

    # Using global FilebeatConfigsArray and CustomMetricsConfigsArray dictionary for follow up purpose
    global FilebeatConfigsArray
    global CustomMetricsConfigsArray
    global PingPlotterArray

    # Switching vars to global
    global ConfigFilePath
    
    # Defining config path from ConfigFilePath
    ConfigPathtmp = ConfigFilePath.split('/').pop()
    ConfigPath = ConfigFilePath.replace(ConfigPathtmp,'')

    # Loading filebeat plugin configs
    for f_name in os.listdir(ConfigPath):
        if f_name.endswith('filebeat.conf'):
            # Log
            logging.info("           " + f_name + ":")

            # Generating full path name for current file
            FullPathFileName = ConfigPath + f_name

            # Loading JSON file content
            # We load JSON content into try/catch to avoid failure if JSON formatting is not good.
            try:
                with open(FullPathFileName, 'r') as ConfigFile:
                    JSONValues = json.load(ConfigFile)

                    # Log
                    logging.info("            * Target file        = " + JSONValues["TargetFile"])
                    logging.info("            * Patterns       = " + str(JSONValues["Patterns"]))
                    logging.info("            * Multiline sparator = " + JSONValues["MultilineSeparator"])
                    
                    # Generating EGREP string from list of patterns
                    EGREPString = ""
                    for Pattern in JSONValues["Patterns"]:
                        EGREPString = EGREPString + Pattern + "|"
                    EGREPString = EGREPString[:-1]
                    
                    # Converting date format, if any, in target file  
                    CurrentTargetFile = ConvertFileName(JSONValues["TargetFile"])

                    # Storing the last line of the Target file to start tail on
                    with open(CurrentTargetFile, "r") as LastLine:
                        TargetFileLastLine = len(LastLine.readlines()) + 1

                    # Storing the inode and size of the Target file to start tail on
                    TargetFileInode = os.stat(CurrentTargetFile).st_ino
                    TargetFileSize = os.stat(CurrentTargetFile).st_size
                    
                    # Storing results into FilebeatConfigsArray dictionnary
                    ConfigString = CurrentTargetFile + ',' + JSONValues["TargetFile"] + ',' + EGREPString + ',' + str(TargetFileLastLine) + ',' + str(TargetFileInode) + ',' + str(TargetFileSize) + ',' + JSONValues["MultilineSeparator"]
                    FilebeatConfigsArray[f_name] = ConfigString
            except:
                # If error, bypassing this plugin
                # Log
                logging.info("            ==> Error while loading " + f_name + " config file !")
                logging.info("                Please check that target file is existing, JSON format and validity")
                # traceback.print_exc()
            # pass

    # All filebeat plugins loaded
    # Log
    logging.info("            ==> All Filebeat plugin's config files loaded !")

    # Log
    logging.info(" - Loading Custom Metric plugin's config files...")

    # Loading custom exec plugin configs
    for f_name in os.listdir(ConfigPath):
        if f_name.endswith('custom.conf'):
            # Log
            logging.info("           " + f_name + ":")

            # Generating full path name for current file
            FullPathFileName = ConfigPath + f_name

            # Loading JSON file content
            # We load JSON content into try/catch to avoid failure if JSON formatting is not good.
            try:
                with open(FullPathFileName, 'r') as ConfigFile:
                    JSONValues = json.load(ConfigFile)

                    # Log
                    logging.info("            * OS Script            = " + JSONValues["OSScript"])
                    logging.info("            * Refresh value in sec = " + str(JSONValues["Refresh"]))


                    # Storing results into FilebeatConfigsArray dictionnary
                    ConfigString = JSONValues["OSScript"] + ',' + str(JSONValues["Refresh"])
                    CustomMetricsConfigsArray[f_name] = ConfigString
            except:
                # If error, bypassing this plugin
                # Log
                logging.info("            ==> Error while loading " + f_name + " config file !")
                logging.info("                Please check that target file is existing, JSON format and validity")
                traceback.print_exc()
                os._exit(2)

            # Debug
            # print('\nDEBUG\n')
            # print('<!> DEBUG ', 'CustomMetricsConfigsArray[f_name]', CustomMetricsConfigsArray[f_name])
            # print('<!> DEBUG ', 'f_name ', f_name)
            # print('<!> DEBUG ', 'OSScript ', JSONValues["OSScript"])
            # print('<!> DEBUG ', 'Refresh ', str(JSONValues["Refresh"]))
            # print('\nFINDEBUG\n')
    # All Custom Metric plugins loaded
    # Log
    logging.info("            ==> All Custom Metric plugin's config files loaded !")
    
    # Loading PingPlotter plugin configs
    # Log
    logging.info("            Ping check:")
    for ping_dest in PingPlotterTargets.split(','):
        # Log
        logging.info("            * " + ping_dest)
    # Log
    logging.info("            ==> All Ping Check destinations are loaded !")
    pass

def GenerateEphemeralID():
    """ Generate specific format of random number for ELK internal purpose.

        Generating Ephemeral ID for current daemon execution
        or new LPAR id file creation (random string format 8-4-4-4-12).
    """

    # Generating Ephemeral ID
    # IMPROVEMENT / urandom can be converted into python style
    ShortRandom2 = subprocess.Popen("tr -dc a-z0-9 </dev/urandom |  head -c 4", shell=True, stdout=subprocess.PIPE).stdout
    ShortRandom2 = ShortRandom2.read().decode().replace("\n","")
    ShortRandom1 = subprocess.Popen("tr -dc a-z0-9 </dev/urandom |  head -c 4", shell=True, stdout=subprocess.PIPE).stdout
    ShortRandom1 = ShortRandom1.read().decode().replace("\n","")
    ShortRandom3 = subprocess.Popen("tr -dc a-z0-9 </dev/urandom |  head -c 4", shell=True, stdout=subprocess.PIPE).stdout
    ShortRandom3 = ShortRandom3.read().decode().replace("\n","")
    MediumRandom = subprocess.Popen("tr -dc a-z0-9 </dev/urandom |  head -c 8", shell=True, stdout=subprocess.PIPE).stdout
    MediumRandom = MediumRandom.read().decode().replace("\n","")
    LongRandom = subprocess.Popen("tr -dc a-z0-9 </dev/urandom |  head -c 12", shell=True, stdout=subprocess.PIPE).stdout
    LongRandom = LongRandom.read().decode().replace("\n","")

    # Formating string
    global EphemeralID
    EphemeralID = MediumRandom + "-" + ShortRandom1 + "-" + ShortRandom2 + "-" + ShortRandom3 + "-" + LongRandom

def GetLPARInformations():
    """ Get necessary LPAR informations.
        All those values are used to format JSON messages sent to ELK.

        FQDN:
        Adding DNS suffix on hostname which doesn't have one.

        LPARArch and AIXVersion:
        Get LPAR AIX version and Power architecture (Power8/7/6+).

        NetworkCards:
        List of network cards is taken from "ifconfig -a" OS command.

        FcCards:
        List of Fiber Channel cards which are correctly connected and linked.

        IDs:
        LPARRndID is fixed across reboot. Defined one time if ID file does not exist.
        AgentID is fixed across reboot. Defined one time if ID file does not exist.
        EphemeralID is generated on each daemon execution and generated by 'GenerateEphemeralID' method.

        NumProc:
        Define the number of CPU threads available on LPAR.
    """

    # Switching vars to global
    global BaseDir
    global LPARName
    global LPARSMTMode
    global LPARArch
    global LPARHost
    global AIXVersion
    global LPARRndID
    global AgentID
    global NetworkCards
    global FcCards
    # FcCards list need to be formated before being used
    FcCards = []
    global NumProc
    global NumProcString

    # Checking if LPAR name contains DNS suffix. Adding if not
    # Checking first if var FQDN is not null
    if FQDN != "":
        if not FQDN in LPARName:
            LPARName = LPARName + FQDN

    # Log
    logging.info(" - Checking LPAR hostname... Done !")

    # Get LPAR hardware architecture
    CmdLine = "prtconf | egrep \"Processor Type|Serial Number\" | awk '{print $NF}'"
    LPARPrtConf =  subprocess.Popen(CmdLine, shell=True, stdout=subprocess.PIPE).stdout
    LPARPrtConf = LPARPrtConf.read().decode().split('\n')
    LPARArch = LPARPrtConf[1]
    LPARHost = LPARPrtConf[0]
    
    # Log
    logging.info(" - Checking LPAR Architecture... Done !")
    
    # Get LPAR AIX OS Version
    AIXVersion =  subprocess.Popen("oslevel -s", shell=True, stdout=subprocess.PIPE).stdout
    AIXVersion =  AIXVersion.read().decode().replace("\n","")

    # Log
    logging.info(" - Checking LPAR AIX version ... Done !")
    
    # Get current SMT mode  
    LPARSMTModeCmdLine = "smtctl | grep has | awk '{print $3}' | head -1"    
    LPARSMTMode =  subprocess.Popen(LPARSMTModeCmdLine, shell=True, stdout=subprocess.PIPE).stdout
    LPARSMTMode =  LPARSMTMode.read().decode().replace("\n","")
    
    # Defining config path from ConfigFilePath
    ConfigPathtmp = ConfigFilePath.split('/').pop()
    ConfigPath = ConfigFilePath.replace(ConfigPathtmp,'')

    
    # Defining LPAR ID file name
    RandomIDFile = ConfigPath + 'Host.ID'

    # If LPAR ID file exists, let's get content as defined scripts vars
    if os.path.exists(RandomIDFile):
        # Get LPAR Unique ID
        # IMPROVEMENT / Can be converted into python style
        LPARRndIDCmd = "cat " + RandomIDFile + " | grep LPARRndID | awk '{print $2}'"
        LPARRndID = subprocess.Popen(LPARRndIDCmd, shell=True, stdout=subprocess.PIPE).stdout
        LPARRndID = LPARRndID.read().decode().replace('\r','')
        LPARRndID = LPARRndID.replace('\n','')
        # Get LPAR ephemeral ID
        # IMPROVEMENT / Can be converted into python style
        AgentIDCmd = "cat " + RandomIDFile + " | grep AgentID | awk '{print $2}'"
        AgentID = subprocess.Popen(AgentIDCmd, shell=True, stdout=subprocess.PIPE).stdout
        AgentID = AgentID.read().decode().replace('\r','')
        AgentID = AgentID.replace('\n','')

    # If not, let's create it, fill it and define scripts vars
    else:
        # Generating LPAR ID in specific format
        # IMPROVEMENT / Can be converted into python style
        LPARRndID = subprocess.Popen("cat /dev/urandom | tr -dc 'a-z0-9' | fold -w 32 | head -n 1", shell=True, stdout=subprocess.PIPE).stdout
        LPARRndID = LPARRndID.read().decode().replace("\n","")

        # Generating unique formated ID for uniqueAgent ID value
        # as it is same format than ephemeral ID
        GenerateEphemeralID()
        AgentID = EphemeralID

        # Creating the LPAR ID file
        OpenedFile = open(RandomIDFile,"w+")

        # Filling it with data
        OpenedFile.write("LPARRndID ")
        OpenedFile.write(LPARRndID)
        OpenedFile.write("\r\n")
        OpenedFile.write("AgentID ")
        OpenedFile.write(AgentID)
        OpenedFile.write("\r\n")

        # Close the LPAR ID file
        OpenedFile.close()

    # Generating unique and ephemeral ID for current script execution
    GenerateEphemeralID()

    # Log
    logging.info(" - Checking LPAR Metricbeat unique ID... Done !")

    # Defining the name of network adapters
    if EntRestricted == 'all':
        # No restriction, taking all network adapters
        NetworkCards = subprocess.Popen("ifconfig -a | grep ': ' | awk -F ':' '{print $1}' | grep -v 'lo0'", shell=True, stdout=subprocess.PIPE).stdout
        NetworkCards = NetworkCards.read().decode().split('\n')
    else:
        # Restriction are specified. Setting Network card list to the given list
        NetworkCards = []
        NetworkCards = EntRestricted.split(',')

    # Log
    logging.info(" - Checking LPAR Network interface list ... Done !")

    # Defining the name of FC adapters
    if FcsRestricted == 'all':
        FCCardsTemp = subprocess.Popen("lsdev -Cc adapter | grep fcs | awk '{print $1}'", shell=True, stdout=subprocess.PIPE).stdout
        FCCardsTemp = FCCardsTemp.read().decode().split('\n')
    else:
        # Restriction are specified. Setting Network card list to the given list
        FCCardsTemp = []
        FCCardsTemp = FcsRestricted.split(',')

    # Log
    logging.info(" - Checking LPAR FC interface list ... Done !")

    # Checking FC link status and exclude unlinked cards
    for CurrLine in FCCardsTemp:
        if(len(CurrLine) != 0):
            FCCardFcStatCmd = "fcstat " + CurrLine
            FCCardFcStatState = subprocess.Popen(FCCardFcStatCmd, shell=True, stdout = subprocess.PIPE, stderr = subprocess.PIPE)
            stdout, stderr = FCCardFcStatState.communicate()
            # If link is ok, adding the FC card into definitive array
            if(FCCardFcStatState.returncode == 0):
                FcCards.append(CurrLine)

    # Log
    logging.info(" - Checking LPAR FC link status... Done !")

    # Defining number of CPU threads
    NumProc = subprocess.Popen("bindprocessor -q | awk '{print $NF}'", shell=True, stdout=subprocess.PIPE).stdout
    NumProc = NumProc.read().decode().split()
    NumProc = int(NumProc[0]) + 1
    NumProcString = str(NumProc)

    # Log
    logging.info(" - Checking LPAR CPU configuration... Done !")
    
    # Generate fix JSON values for LPAR
    GenerateStaticJSON()

def CompareTimer(TimedTopic, MetricsWaitValue, CustomMetric = ''):
    """ This function compare the last execution time and the specific timer for an element.
        If Timer is reached, the function return True and will trigger the underlying function.

        - TimedTopic is the metric on which the timer needed to be checked
        - MetricsWaitValue is the number of seconds to wait before work for metric is triggered
    """

    # Using global ExecutionTimers dictionary for follow up purpose
    global ExecutionTimers
    global devnull

    # Get current timestamp
    CurrentTimer = datetime.datetime.now()

    # Try/Catch to avoid errors on non defined dictionary entrie / First execution
    try:
        # Dictionary entrie is not empty
        # Get last execution date/time
        LastTopicExecTime = ExecutionTimers[TimedTopic]

        # Comparing current and last execution date/time to get actual delay
        TimerLatency = CurrentTimer - LastTopicExecTime

        # Converting TimerLatency in seconds
        TimerLatency = TimerLatency.total_seconds()
    except:
        # Dictionary entrie is empty
        # We set TimerLatency very high (3600) to force the work for this first execution
        TimerLatency = 3600

    # Comparing last execution value with current timestamp
    # and take decision to schedule the work or not
    if TimerLatency > MetricsWaitValue:
        ExecutionTimers[TimedTopic] = datetime.datetime.now()
        # Log
        LogString = "==> " + TimedTopic + " " + CustomMetric + " : New execution triggered ! (" + str(round(TimerLatency,2)) + " sec elapsed for " + str(MetricsWaitValue) + " seconds wait time)."
        logging.info(LogString)
        # print('YES ExecutionTimers[TimedTopic] and TimerLatency and MetricsWaitValue: ' + str(ExecutionTimers[TimedTopic]) + ' ' + str(round(TimerLatency,2)) + ' ' + str(MetricsWaitValue))
        return str(TimerLatency).replace('.', '')
    else:
        RemainingTime = MetricsWaitValue - TimerLatency
        # Log
        LogString = "  ! (" + str(round(RemainingTime,2)) + "/" + str(MetricsWaitValue) + " sec) " + TimedTopic + CustomMetric + " has NOT reached timer value."
        logging.info(LogString)
        # print('NO ExecutionTimers[TimedTopic] and TimerLatency and MetricsWaitValue: ' + str(ExecutionTimers[TimedTopic]) + ' ' + str(round(TimerLatency,2)) + ' ' + str(MetricsWaitValue))
        return devnull

def ConvertFileName(FileName):
    """This function convert date time for easy use and tracking while making tail on log files
    
    TODO: Describe the function, all vars and its content
    """
    
    # Depending of the config file, TargetFile name may contain reference to date. Let's convert it !
    # We will ad new conversion if required by users...

    # Converting %yyyy to 4 digits years format
    FileNameC = str(FileName.replace('%YYYY', datetime.datetime.now().strftime("%Y")))
    
    # Converting %yy to 2 digits years format
    FileNameC = FileNameC.replace('%YY', datetime.datetime.now().strftime("%y"))
    
    # Converting %mm to 2 digits month format
    FileNameC = FileNameC.replace('%MM', datetime.datetime.now().strftime("%m"))
    
    # Converting %dd to 2 digits day format
    FileNameC = FileNameC.replace('%DD', datetime.datetime.now().strftime("%d"))
    
    # Converting %dd to 2 digits hour format
    FileNameC = FileNameC.replace('%hh', datetime.datetime.now().strftime("%H"))
    
    # Converting %dd to 2 digits minute format
    FileNameC = FileNameC.replace('%mm', datetime.datetime.now().strftime("%M"))
    
    # Converting %dd to 2 digits second format
    FileNameC = FileNameC.replace('%ss', datetime.datetime.now().strftime("%S"))
    
    # Returning converted string
    # print('\nconverted: ', FileNameC,'\n')
    return FileNameC

def GenerateStaticJSON():
    """This function generate a static JSON message containing the pieces that will remain unchange until daemon restart
    
    Please refer to README.md
    
    TODO: Describe the function, all vars and its content
    """
    
    global StaticJSON
    
    # Define Agent JSON
    JSONAgent = (''
    '\"agent\":{'
    '\"version\":\"' + ELKMonitoringVersion + '\",'
    '\"type\":\"metricbeat\",'
    '\"ephemeral_id\":\"' + EphemeralID + '\",'
    '\"hostname\":\"' + LPARName + '\",'
    '\"id\":\"' + AgentID + '\"'
    '},')

    # Define Host JSON
    JSONHost = (''
    '\"host\":{'
    '\"frame\":\"' + LPARHost + '\",'
    '\"site\":\"' + HostingSite + '\",'
    '\"app\":\"' + LPARAppName + '\",'
    '\"name\":\"' + LPARName + '\",'
    '\"hostname\":\"' + LPARName + '\",'
    '\"architecture\":\"' + LPARArch + '\",'
    '\"id\":\"' + LPARRndID + '\",'
    '\"containerized\":false,'
    '\"os\":{'
    '\"platform\":\"AIX\",'
    '\"version\":\"' + AIXVersion + '\",'
    '\"family\":\"AIX\",'
    '\"name\":\"AIX\",'
    '\"kernel\":\"' + AIXVersion + '\",'
    '\"codename\":\"AIX\"'
    '}},')

    # Define ECS JSON
    JSONEcs = (''
    '\"ecs\":{'
    '\"version\":\"' + ECSVersion + '\"'
    '}')
	
    StaticJSON = JSONAgent + JSONHost + JSONEcs

    pass

def GenerateDynamicJson(EventDataset, ServiceType, MetricSet, ComparisonTimer):
    """This function generate a static JSON message containing the pieces that will change for each message
    
    Please refer to README.md
    
    TODO: Describe the function, all vars and its content
    """
    JSONServiceType = (''
    '\"service\":{'
    '\"type\":\"' + ServiceType + '\"'
    '},')

    JSONEventDataset = (''
    '\"event\":{'
    '\"module\":\"' + ServiceType + '\",'
    '\"duration\":' + ComparisonTimer + ','
    '\"dataset\":\"' + EventDataset + '\"'
    '},')

    JSONMetricSet = (''
    '\"metricset\":{'
    '\"name\":\"' + MetricSet + '\",'
    '\"period\":1'
    '}')

    DynamicJSON = JSONServiceType + JSONEventDataset + JSONMetricSet

    return DynamicJSON

def CheckELKServers():
    """ This function check the state of the Metricbeat Elasticsearch server.

        It also handles the retransmit queues for Metricbeat and Filebeat items.
        On each script loop execution, it will resend some queued messages up to a given number.
        
    Please refer to README.md
    
    TODO: Describe the function, all vars and its content
    """

    # Setting global vars
    global SendQueueArray
    global FilebeatSendQueueArray
    global ELKUsername
    global ELKPassword
    global ELKServers
    global ELKServersAvailable
    global ELKServersFailed
    global ELKMetricIndexName
    global ELKLogIndexName
    # global JSONCheckMessage
    global ELKServer
    global ELKUsername
    global ELKPassword
    global ElasticSearchServer
    global AllServersFailedDate
    global QueueingTimerLimit

    # Checking for available ELK servers
    if len(ELKServersAvailable) == 0:
        # Checking AllServersFailedDate definition state
        try:
            # Checking if var is defined
            AllServersFailedDate
            
            # If yes, checking if Queueing max timer is reached
            CurrentDate = datetime.datetime.now()
            
            # Checking if the queue is older than 24H
            TimeDelta = CurrentDate - AllServersFailedDate 
            
            # Converting Time Delta in minutes
            TimeDeltaMinutes = TimeDelta.seconds / 60
            
            # Log
            # logging.info('INFO     ==> Timer limit: ' + str(QueueingTimerLimit)  + ', TimeDeltaMinutes: ' + str(TimeDeltaMinutes))
            
            # We check if timer is reached
            if TimeDeltaMinutes > QueueingTimerLimit:
                # Timer is reached, we remove all items from all queues
                SendQueueArray.clear()
                FilebeatSendQueueArray.clear()
                
                # Reseting AllServersFailedDate var with current datetime
                AllServersFailedDate = datetime.datetime.now()
                
                # Log
                logging.info('INFO     ==> Maximum queueing timer is reached. All historical JSON messages removed from queues !')
            
        except:
            LogString = str(traceback.format_exc())
            # If not, setting it with current date
            AllServersFailedDate = datetime.datetime.now()
        
        # Choosing randomly one ELK server in the pool
        ELKServer = random.choice(ELKServersFailed)
        
        # Send Data to ELK server with a Try/Catch
        try:
            # If ELK server is unavailable, we send a get request
            
            # ELKServerURL = ELKWebProtocol + '://' + ELKServer + ':' + ELKPort + '/' + CurrentELKIndex + '/_doc'
            ELKCreds = (ELKUsername, ELKPassword)
            
            # Defining credential for web request
            ELKServerURL = ELKWebProtocol + '://' + ELKServer + ':' + ELKPort + '/'
            
            # Connect and send encoded JSON message     
            ElasticSearchServer = requests.session()
            ELKAnswer = ElasticSearchServer.get(ELKServerURL, auth=ELKCreds, timeout=1)

            # If the response was successful, no Exception will be raised
            ELKAnswer.raise_for_status()

        except HTTPError as http_err:
            # ELK server is still unavailable
            # Log
            logging.info("WARNING  ==> " + ELKServer +  " is unvailable")
            
            # Creating the error stack file and filling error stack informations
            with open(CrashDumpCheckLog, 'a') as f:
                f.write('\n\n')
                f.write(str(datetime.datetime.now()))
                f.write(':\n')
                f.write(str(http_err))
                f.write(traceback.format_exc())
                f.write('\n')

            # Print and log error stack for debug purpose
            logging.info('WARNING  ==> Please check ' + CrashDumpCheckLog + ' for more informations on HTTP errors')
            # traceback.print_exc()

        except Exception as err:
            # ELK server is still unavailable
            # Log
            logging.info("WARNING  ==> " + ELKServer +  " is unvailable")
            
            # Creating the error stack file and filling error stack informations
            with open(CrashDumpCheckLog, 'a') as f:
                f.write('\n\n')
                f.write(str(datetime.datetime.now()))
                f.write(':\n')
                f.write(str(err))
                f.write(traceback.format_exc())
                f.write('\n')

            # Print and log error stack for debug purpose
            logging.info('WARNING  ==> Please check ' + CrashDumpCheckLog + ' for more informations on HTTP errors')
            # traceback.print_exc()
        
        else:
            # ELK server is available, let's add it into the Available pool 
            # Into a try/catch because some background processes can modify these value
            try:
                ELKServersFailed.remove(ELKServer)
                ELKServersAvailable.append(ELKServer)
                logging.info("INFO     ==> " + ELKServer +  " is Online ! Processing messages on this ELK node")
            except:
                # Nothing to do
                pass
    else:
        # Checking if AllServersFailedDate var is defined
        try:
            # If var is already setted, unsetting it for queueing timer reset
            AllServersFailedDate
            del AllServersFailedDate

        except:
            # If not defined, then nothing to do
            pass
    
    # Checking if queued messages need to be sent in SendQueueArray and if ELK servers are available
    if (len(SendQueueArray) > 0 and len(ELKServersAvailable) > 0):
        # If yes, we send historical message uppon maximum configured value
        for i in range(ReprocessingValueAtOnce):
            # Poping the message from list with a try/catch to detect the end of the list
            try:
                # Poping list
                JSONToSend = SendQueueArray.pop()
                # Sending the message to ELK
                SendToELK(JSONToSend)
            except:
                # The list is empty and all historical messages has been delivered
                # Log
                logging.info("INFO ==> All historical JSON metric messages has been processed")
                # Exiting from loop
                break
    
    # Checking if queued messages need to be sent in FilebeatSendQueueArray and if ELK servers are available
    if (len(FilebeatSendQueueArray) > 0 and len(ELKServersAvailable) > 0):
        # If yes, we send historical message uppon maximum configured value
        for i in range(ReprocessingValueAtOnce):
            # Poping the message from list with a try/catch to detect the end of the list
            try:
                # Poping list
                JSONToSend = FilebeatSendQueueArray.pop()
                # Sending the message to ELK
                SendToELK(JSONToSend, "log")
            except:
                # The list is empty and all historical messages has been delivered
                # Log
                logging.info("INFO ==> All historical JSON log messages has been processed")
                # Exiting from loop
                break

    # Logging a message if the queues are not empty and all ELK servers are in failed state
    TotalQueueLengt = 0
    if len(SendQueueArray) > 0:
        # Incrementing total queue lengt counter
        TotalQueueLengt = TotalQueueLengt + len(SendQueueArray)
    if len(FilebeatSendQueueArray) > 0:
        # Incrementing total queue lengt counter
        TotalQueueLengt = TotalQueueLengt + len(FilebeatSendQueueArray)
    
    # Logging queue count into log file
    if TotalQueueLengt > 0:
        LogString = "WARNING  ==>  " + str(TotalQueueLengt) + " historical JSON messages queued"
        logging.info(LogString)

def SendToELK(JSONToSend, LogType = "metric"):
    """ This function send a JSON message to the Metricbeat Logstash server.
        If Logstash is not available, the JSON message is queued for further reprocessing.
        
        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """
    # Setting global vars
    global SendQueueArray
    global FilebeatSendQueueArray
    global ELKCreds
    global ELKServers
    global ELKServer
    global ELKServersAvailable
    global ELKServersFailed
    global ELKPort
    global ELKWebProtocol
    global ELKMetricIndexName
    global ELKLogIndexName
    global ElasticSearchServer

    # Choosing one availale ELK server in the pool, if any
    if ELKServersAvailable:
        # ELKServer = random.choice(ELKServersAvailable)
        # print('ELK server choosen: ', ELKServer)
        # print('Available list: ', ELKServersAvailable)

        # Send Data to ELK server with a Try/Catch
        try:
            # If ELK server is available, we send the message
            # Defining index name depending of current JSON data
            if LogType == "metric":
                ELKIndexName = ELKMetricIndexName
            elif LogType == "log":
                ELKIndexName = ELKLogIndexName
 
            # Define ELK url depending of index and choosen ELK server
            # CurrentELKIndex = ELKIndexName + '-aix-' + ELKMonitoringVersion + '-' + datetime.datetime.today().strftime('%Y.%m.%d')
            # CurrentELKIndex = ELKIndexName + '-aix-' + ELKMonitoringVersion + '-' + datetime.datetime.today().strftime('%Y.%m.%d') + '-000001'
            CurrentELKIndex = ELKIndexName + '-aix-' + ELKMonitoringVersion
            ELKServerURL = ELKWebProtocol + '://' + ELKServer + ':' + ELKPort + '/' + CurrentELKIndex + '/_doc'
            # Defining credential for web request
            ELKCreds = (ELKUsername, ELKPassword)
            
            # print('\nJSONToSend', str(JSONToSend))
            
            # Connect and send encoded data
            # ElasticSearchServer = requests.post(ELKServerURL, data = JSONToSend.encode("utf8"), verify=True, headers={"Content-Type" : "application/json; charset=utf-8"}, auth=ELKCreds, timeout=1)
            ELKAnswer = ElasticSearchServer.post(ELKServerURL, data = JSONToSend.encode("utf8"), verify=True, headers={"Content-Type" : "application/json; charset=utf-8"}, auth=ELKCreds, timeout=1)
            # print('ELKAnswer', str(ELKAnswer), '\n')
            # If the response was successful, no Exception will be raised
            ELKAnswer.raise_for_status()

        except HTTPError as http_err:
            # We received HTTP exception
            # ELK server has fall down, queing message and setting ELK server as unavailable
            try:
                ELKServersAvailable.remove(ELKServer)
                ELKServersFailed.append(ELKServer)
            except:
                # Nothing to do, server already removed
                pass

            # Send the failed work to the ELK Send queue depending of doc type
            if LogType == "metric":
                SendQueueArray.append(JSONToSend)
                # Get a copy of the document for further check
                # if len(JSONCheckMessage) == 0:
                    # JSONCheckMessage = ('metric', JSONToSend)
                    
            # elif LogType == "log":
                FilebeatSendQueueArray.append(JSONToSend)
                # Get a copy of the document for further check
                # if len(JSONCheckMessage) == 0:
                    # JSONCheckMessage = ('log', JSONToSend)
           
            # Log
            logging.info("WARNING  ==> " + ELKServer +  " became unvailable, queueing JSON messages")
            
            # Creating the error stack file and filling error stack informations
            with open(CrashDumpSendLog, 'a') as f:
                f.write('\n\n')
                f.write(str(datetime.datetime.now()))
                f.write(':\n')
                f.write(str(http_err))
                f.write(traceback.format_exc())
                f.write('\nJSON message:\n')
                f.write(JSONToSend)
                f.write('\n')

            # Print and log error stack for debug purpose
            logging.info('WARNING  ==> Please check ' + CrashDumpSendLog + ' for more informations on HTTP errors')
            # traceback.print_exc()
            

        except Exception as err:
            # We received another type of exception
            # ELK server has fall down, queing message and setting ELK server as unavailable
            # Making that into a try/catch because some background threads can act on that also and clea the content
            try:
                ELKServersAvailable.remove(ELKServer)
                ELKServersFailed.append(ELKServer)
            except:
                # Nothing to do, server already removed
                pass

            # Send the failed work to the ELK Send queue depending of doc type
            if LogType == "metric":
                SendQueueArray.append(JSONToSend)
                # Get a copy of the metric document for further check
                # if len(JSONCheckMessage) == 0:
                    # JSONCheckMessage = ('metric', JSONToSend)
            elif LogType == "log":
                FilebeatSendQueueArray.append(JSONToSend)
                # Get a copy of the log document for further check
                # if len(JSONCheckMessage) == 0:
                    # JSONCheckMessage = ('log', JSONToSend)
                    
            # Log
            logging.info("WARNING  ==> " + ELKServer +  " became unvailable, queueing JSON messages")
            
            # Creating the error stack file and filling error stack informations
            with open(CrashDumpSendLog, 'a') as f:
                f.write('\n\n')
                f.write(str(datetime.datetime.now()))
                f.write(':\n')
                f.write(str(err))
                f.write(traceback.format_exc())
                f.write('\nJSON message:\n')
                f.write(JSONToSend)
                f.write('\n')

            # Print and log error stack for debug purpose
            logging.info('WARNING  ==> Please check ' + CrashDumpSendLog + ' for more informations on HTTP errors')
            # traceback.print_exc()

        # else:
            # Message sent sucessfully
            # pass

    else:
        # No ELK server available, waiting for next status check
        # and send the failed work to the ELK Send queue depending of doc type
        if LogType == "metric":
            SendQueueArray.append(JSONToSend)
        elif LogType == "log":
            FilebeatSendQueueArray.append(JSONToSend)
        # Log
        # logging.info("WARNING  ==> No ELK server available at this time, queueing JSON messages...")
        pass

def WorkOnMetrics():
    """ This function will check execution timers and, if necessary, execute item work.
        Items can be builtin Metricbeat Metricsets or Custom Metricsets.

        Some work on specific items are executed in background because execution duration is too long.
    
        TODO: Describe the function, all vars and its content
    """
    
    # Sharing CustomMetricsConfigsArray with main script
    global CustomMetricsConfigsArray
    global CrashDumpDaemon

    # Launch metricset timer comparison and schedule the work if timer is reached, insinde try/catch for debug and stability purpose
    
    try:
        # Checking timer for SystemCoreAndCpu
        ComparisonTimer = CompareTimer("SystemCoreAndCpu", SystemCoreAndCpuWaitValue)
        if ComparisonTimer != devnull:
            SystemCoreAndCpu(ComparisonTimer)
    except Exception as e:
        # Metricset has crashed
        # Creating the crash dump file and filling it with crash dump data
        with open(CrashDumpDaemon, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(e))
            f.write(traceback.format_exc())
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('\n Metraixbeat encountered an error ! See ' + CrashDumpDaemon + ' for more informations\n')
        print('\n Metraixbeat encountered an error ! See ', CrashDumpDaemon, ' for more informations\n')
        # traceback.print_exc()
        
        # Exiting
        # os._exit(2)
    
    try:
        # Checking timer for SystemLoad
        ComparisonTimer = CompareTimer("SystemLoad", SystemLoadWaitValue)
        if ComparisonTimer != devnull:
            SystemLoad(ComparisonTimer)
    except Exception as e:
        # Metricset has crashed
        # Creating the crash dump file and filling it with crash dump data
        with open(CrashDumpDaemon, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(e))
            f.write(traceback.format_exc())
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('\n Metraixbeat encountered an error ! See ' + CrashDumpDaemon + ' for more informations\n')
        print('\n Metraixbeat encountered an error ! See ', CrashDumpDaemon, ' for more informations\n')
        # traceback.print_exc()
        
        # Exiting
        # os._exit(2)
    
    try:
        # Checking timer for SystemMemory
        global SystemMemoryThread
        ComparisonTimer = CompareTimer("SystemMemory", SystemMemoryWaitValue)
        if (ComparisonTimer != devnull) and (SystemMemoryThread.isAlive() != True):
            SystemMemoryThread = threading.Thread(target=SystemMemory, args=(ComparisonTimer,))
            SystemMemoryThread.start()
    except Exception as e:
        # Metricset has crashed
        # Creating the crash dump file and filling it with crash dump data
        with open(CrashDumpDaemon, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(e))
            f.write(traceback.format_exc())
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('\n Metraixbeat encountered an error ! See ' + CrashDumpDaemon + ' for more informations\n')
        print('\n Metraixbeat encountered an error ! See ', CrashDumpDaemon, ' for more informations\n')
        # traceback.print_exc()
        
        # Exiting
        # os._exit(2)
    
    try:
        # Checking timer for SystemNetwork
        ComparisonTimer = CompareTimer("SystemNetwork", SystemNetworkWaitValue)
        if ComparisonTimer != devnull:
            SystemNetwork(ComparisonTimer)
    except Exception as e:
        # Metricset has crashed
        # Creating the crash dump file and filling it with crash dump data
        with open(CrashDumpDaemon, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(e))
            f.write(traceback.format_exc())
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('\n Metraixbeat encountered an error ! See ' + CrashDumpDaemon + ' for more informations\n')
        print('\n Metraixbeat encountered an error ! See ', CrashDumpDaemon, ' for more informations\n')
        # traceback.print_exc()
        
        # Exiting
        # os._exit(2)
    
    try:
        # Checking timer for SystemFc
        ComparisonTimer = CompareTimer("SystemFc", SystemFcWaitValue)
        if ComparisonTimer != devnull:
            SystemFc(ComparisonTimer)
    except Exception as e:
        # Metricset has crashed
        # Creating the crash dump file and filling it with crash dump data
        with open(CrashDumpDaemon, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(e))
            f.write(traceback.format_exc())
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('\n Metraixbeat encountered an error ! See ' + CrashDumpDaemon + ' for more informations\n')
        print('\n Metraixbeat encountered an error ! See ', CrashDumpDaemon, ' for more informations\n')
        # traceback.print_exc()
        
        # Exiting
        # os._exit(2)
    
    try:        
        # Checking timer for SystemSocketSummary
        ComparisonTimer = CompareTimer("SystemSocketSummary", SystemSocketSummaryWaitValue)
        if ComparisonTimer != devnull:
            SystemSocketSummary(ComparisonTimer)
    except Exception as e:
        # Metricset has crashed
        # Creating the crash dump file and filling it with crash dump data
        with open(CrashDumpDaemon, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(e))
            f.write(traceback.format_exc())
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('\n Metraixbeat encountered an error ! See ' + CrashDumpDaemon + ' for more informations\n')
        print('\n Metraixbeat encountered an error ! See ', CrashDumpDaemon, ' for more informations\n')
        # traceback.print_exc()
        
        # Exiting
        # os._exit(2)
    
    try:        
        # Checking timer for SystemSocket
        global SystemSocketThread
        ComparisonTimer = CompareTimer("SystemSocket", SystemSocketWaitValue)
        if (ComparisonTimer != devnull) and (SystemSocketThread.isAlive() != True):
            SystemSocketThread = threading.Thread(target=SystemSocket, args=(ComparisonTimer,))
            SystemSocketThread.start()
    except Exception as e:
        # Metricset has crashed
        # Creating the crash dump file and filling it with crash dump data
        with open(CrashDumpDaemon, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(e))
            f.write(traceback.format_exc())
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('\n Metraixbeat encountered an error ! See ' + CrashDumpDaemon + ' for more informations\n')
        print('\n Metraixbeat encountered an error ! See ', CrashDumpDaemon, ' for more informations\n')
        # traceback.print_exc()
        
        # Exiting
        # os._exit(2)
    
    try:
        # Checking timer for SystemProcessSummary
        ComparisonTimer = CompareTimer("SystemProcessSummary", SystemProcessSummaryWaitValue)
        if ComparisonTimer != devnull:
            SystemProcessSummary(ComparisonTimer)
    except Exception as e:
        # Metricset has crashed
        # Creating the crash dump file and filling it with crash dump data
        with open(CrashDumpDaemon, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(e))
            f.write(traceback.format_exc())
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('\n Metraixbeat encountered an error ! See ' + CrashDumpDaemon + ' for more informations\n')
        print('\n Metraixbeat encountered an error ! See ', CrashDumpDaemon, ' for more informations\n')
        # traceback.print_exc()
        
        # Exiting
        # os._exit(2)
    
    try:
        # Checking timer for SystemProcess
        global SystemProcessThread
        ComparisonTimer = CompareTimer("SystemProcess", SystemProcessWaitValue)
        if (ComparisonTimer != devnull) and (SystemProcessThread.isAlive() != True):
            SystemProcessThread = threading.Thread(target=SystemProcess, args=(ComparisonTimer,))
            SystemProcessThread.start()
    except Exception as e:
        # Metricset has crashed
        # Creating the crash dump file and filling it with crash dump data
        with open(CrashDumpDaemon, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(e))
            f.write(traceback.format_exc())
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('\n Metraixbeat encountered an error ! See ' + CrashDumpDaemon + ' for more informations\n')
        print('\n Metraixbeat encountered an error ! See ', CrashDumpDaemon, ' for more informations\n')
        # traceback.print_exc()
        
        # Exiting
        # os._exit(2)
    
    try:
        # Checking timer for SystemDiskIO
        global SystemDiskIOThread
        ComparisonTimer = CompareTimer("SystemDiskIO", SystemDiskIOWaitValue)
        if (ComparisonTimer != devnull) and (SystemDiskIOThread.isAlive() != True):
            SystemDiskIOThread = threading.Thread(target=SystemDiskIO, args=(ComparisonTimer,))
            SystemDiskIOThread.start()
    except Exception as e:
        # Metricset has crashed
        # Creating the crash dump file and filling it with crash dump data
        with open(CrashDumpDaemon, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(e))
            f.write(traceback.format_exc())
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('\n Metraixbeat encountered an error ! See ' + CrashDumpDaemon + ' for more informations\n')
        print('\n Metraixbeat encountered an error ! See ', CrashDumpDaemon, ' for more informations\n')
        # traceback.print_exc()
        
        # Exiting
        # os._exit(2)
    
    try:
        # Checking timer for SystemFilesystemAndFstat
        ComparisonTimer = CompareTimer("SystemFilesystemAndFstat", SystemFilesystemAndFstatWaitValue)
        if ComparisonTimer != devnull:
            SystemFilesystemAndFstat(ComparisonTimer)
    except Exception as e:
        # Metricset has crashed
        # Creating the crash dump file and filling it with crash dump data
        with open(CrashDumpDaemon, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(e))
            f.write(traceback.format_exc())
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('\n Metraixbeat encountered an error ! See ' + CrashDumpDaemon + ' for more informations\n')
        print('\n Metraixbeat encountered an error ! See ', CrashDumpDaemon, ' for more informations\n')
        # traceback.print_exc()
        
        # Exiting
        # os._exit(2)
    
    try:        
        # Checking timer for PingPlotter
        global PingPlotterThread
        ComparisonTimer = CompareTimer("PingPlotter", PingPlotterWaitValue)
        if (ComparisonTimer != devnull) and (PingPlotterThread.isAlive() != True):
            PingPlotterThread = threading.Thread(target=PingPlotter, args=(ComparisonTimer,))
            PingPlotterThread.start()
    except Exception as e:
        # Metricset has crashed
        # Creating the crash dump file and filling it with crash dump data
        with open(CrashDumpDaemon, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(e))
            f.write(traceback.format_exc())
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('\n Metraixbeat encountered an error ! See ' + CrashDumpDaemon + ' for more informations\n')
        print('\n Metraixbeat encountered an error ! See ', CrashDumpDaemon, ' for more informations\n')
        # traceback.print_exc()
        
        # Exiting
        # os._exit(2)
    
    try:    
        # Checking timer for SystemHypervisor, if enabled in Parameters.conf file
        if IfSystemHypervisorEnable == "yes":
            ComparisonTimer = CompareTimer("SystemHypervisor", SystemHypervisorWaitValue)
            if ComparisonTimer != devnull:
                SystemHypervisor(ComparisonTimer)
    except Exception as e:
        # Metricset has crashed
        # Creating the crash dump file and filling it with crash dump data
        with open(CrashDumpDaemon, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(e))
            f.write(traceback.format_exc())
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('\n Metraixbeat encountered an error ! See ' + CrashDumpDaemon + ' for more informations\n')
        print('\n Metraixbeat encountered an error ! See ', CrashDumpDaemon, ' for more informations\n')
        # traceback.print_exc()
        
        # Exiting
        # os._exit(2)
    
    try:           
        # Checking timer for SystemHPMStat, if enabled in Parameters.conf file
        global SystemHPMStatThread
        if IfSystemHPMStatEnable == "yes":
            ComparisonTimer = CompareTimer("SystemHPMStat", SystemHPMStatWaitValue)
            if (ComparisonTimer != devnull) and (SystemHPMStatThread.isAlive() != True):
                SystemHPMStatThread = threading.Thread(target=SystemHPMStat, args=(ComparisonTimer,))
                SystemHPMStatThread.start()
    except Exception as e:
        # Metricset has crashed
        # Creating the crash dump file and filling it with crash dump data
        with open(CrashDumpDaemon, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(e))
            f.write(traceback.format_exc())
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('\n Metraixbeat encountered an error ! See ' + CrashDumpDaemon + ' for more informations\n')
        print('\n Metraixbeat encountered an error ! See ', CrashDumpDaemon, ' for more informations\n')
        # traceback.print_exc()
        
        # Exiting
        # os._exit(2)
    
    # Checking if any Custom Metric need to be executed
    # IMPROVEMENT - Threading execution of each Custom metric as it will slow down the script loop
    for CustomMetricItem in CustomMetricsConfigsArray:
        # Spliting the result line to get content
        CustomMetricSplitedLine = CustomMetricsConfigsArray[CustomMetricItem].split(',')

        # Creating a short name for the current Custom config
        CustomMetricItemName = CustomMetricItem.replace('.custom.conf','')

        # Gathering config values from splitted line
        CustomMetricItemOSScript = CustomMetricSplitedLine[0]
        CustomMetricItemWaitValue = int(CustomMetricSplitedLine[1])
        
        try:
            # Checking timer for the current Custom Metric into try/catch, third parameter is only for user log formating
            ComparisonTimer = CompareTimer(CustomMetricItemName, CustomMetricItemWaitValue, '(Custom metrics)')
            if ComparisonTimer != devnull:
                CustomMetricExec(CustomMetricItemOSScript, CustomMetricItemName, ComparisonTimer)
        except Exception as e:
            # Metricset has crashed
            # Creating the crash dump file and filling it with crash dump data
            with open(CrashDumpDaemon, 'a') as f:
                f.write('\n\n')
                f.write(str(datetime.datetime.now()))
                f.write(':\n')
                f.write(str(e))
                f.write(traceback.format_exc())
                f.write('\n')

            # Print and log error stack for debug purpose
            logging.info('\n Metraixbeat encountered an error ! See ' + CrashDumpDaemon + ' for more informations\n')
            print('\n Metraixbeat encountered an error ! See ', CrashDumpDaemon, ' for more informations\n')
            # traceback.print_exc()
            
            # Exiting
            # os._exit(2)
    pass
    
def FilebeatTail(FilebeatConfigName):
    """ This function will check if tailed file is still "alive" or "dead" in multiple way

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """
    
    # Using global FilebeatConfigsArray, TargetFileCurrentPosArray and TailStateArray dictionary for follow up purpose
    global FilebeatConfigsArray
    global TailStateArray
    global TargetFileCurrentPosArray
    global TargetFileMTimeCountArray

    # Getting parameters from FilebeatConfigsArray
    ConfigSplitLine = FilebeatConfigsArray[FilebeatConfigName].split(',')
    CurrentTargetFile = ConfigSplitLine[0]
    TargetFile = ConfigSplitLine[1]
    EGREPString = ConfigSplitLine[2]
    LastTailLine = ConfigSplitLine[3]
    StartInode = ConfigSplitLine[4]
    TargetSize = ConfigSplitLine[5]
    MultilineSeparator = ConfigSplitLine[6]

    # Defining name for the thread
    FilebeatConfigThreadName = FilebeatConfigName + 'Thread'.replace('.','')
    FilebeatConfigThreadName = FilebeatConfigThreadName.replace('.','')

    # Checking if FilebeatTail process already running for this config file
    if (bool(re.search(FilebeatConfigThreadName, str(threading.enumerate())))):
        # This config has already a thread running, let's go checking CurrentTargetFile freshness
        # Log
        logging.info("   - Checking " + FilebeatConfigName + " Tail process status")

        # Checking if file is still here
        if not os.path.exists(CurrentTargetFile):
        # The file does not exists
            # Log
            logging.info("            * File is not there anymore: stopping Tail thread !")

            # Sending order to stop thread
            TailStateArray[FilebeatConfigName] = False

            # Resting last tailed line for CurrentTargetFile as the file has changed
            TargetFileCurrentPosArray[FilebeatConfigName] = 0

            # Exiting function
            return
        else:
            # Log
            logging.info("            * Presence check: Ok !")

        # Checking if CurrentTargetFile size is less then starting size
        CurrentTargetFileSize = os.stat(CurrentTargetFile).st_size
        if int(TargetSize) > CurrentTargetFileSize:
            # The reference size is less than the current size.
            # File has changed. Let's send order to stop background tail process on that CurrentTargetFile
            TailStateArray[FilebeatConfigName] = False

            # Resting last tailed line for CurrentTargetFile as the file has changed
            TargetFileCurrentPosArray[FilebeatConfigName] = 0

            # Log
            logging.info("            * The reference size is less than the current size: stopping Tail thread !")
            # Exiting function
            return
        else:
            # Log
            logging.info("            * Size check: Ok !")

        # Checking if CurrentTargetFile inode has changed
        CurrentTargetFileInode = os.stat(CurrentTargetFile).st_ino
        if int(StartInode) != CurrentTargetFileInode:
            # The inode has changed. Let's send order to stop background tail process on that CurrentTargetFile
            TailStateArray[FilebeatConfigName] = False

            # Resting last tailed line for CurrentTargetFile as the file has changed
            TargetFileCurrentPosArray[FilebeatConfigName] = 0

            # Log
            logging.info("            * The reference inode is different than the current inode: stopping Tail thread !")
            # Exiting function
            return
        else:
            # Log
            logging.info("            * Inode check: Ok !")
        
        # Converting date format from original TargetFile string, if any
        IfNewTargetFile = ConvertFileName(TargetFile)
        
        # Checking if CurrentTargetFile name still match with original pattern
        if IfNewTargetFile != CurrentTargetFile:
            # Mismatch detected, file is not the good one anymore
            TailStateArray[FilebeatConfigName] = False
            
            # Resting last tailed line for CurrentTargetFile as the file has changed
            TargetFileCurrentPosArray[FilebeatConfigName] = 0
            
            # Log
            logging.info("            * File name pattern in configuration file does not match anymore with current file: stopping Tail thread !")
            # Exiting function
            return
        
        else:
            # Log
            logging.info("            * Refreshing filename: Ok !")

        # Checking if CurrentTargetFile is older than 1 day
        # IMPROVEMENT /SAFETY - What else ?
        # If yes, reseting Tail thread by safety (maybe something happened...)
        # Get current date
        now  = datetime.datetime.today()

        # Get modification datetime of the CurrentTargetFile
        CurrentTargetFileMTime = datetime.datetime.utcfromtimestamp(os.path.getmtime(CurrentTargetFile))

        # Calculating time diff between actual date and last mdofication date
        dif = (now - CurrentTargetFileMTime)

        # Using try/catch to detect if value exists in dictionary
        try:
            # Testing var
            TargetFileMTimeCountArray[FilebeatConfigName]

            # Chechking if file is older then 1 day
            if dif > datetime.timedelta(days = 1):
                # If yes, checking retry counter
                if int(TargetFileMTimeCountArray[FilebeatConfigName]) < 10:
                    # The modification time is greater than 1 day and retry counter is not reached
                    # Incrementing counter in dictionary
                    TargetFileMTimeCountArray[FilebeatConfigName] = int(TargetFileMTimeCountArray[FilebeatConfigName]) + 1
                    # Log
                    LogString = "            * Modification time is too old: " + str(TargetFileMTimeCountArray[FilebeatConfigName]) + "/" + str(TailReloadValue) + " retries !"
                    logging.info(LogString)
                    # Exiting function
                    return

                elif int(TargetFileMTimeCountArray[FilebeatConfigName]) >= 10:
                    # Count is reached, refreshing Tail thread
                    # Let's send order to stop background tail process on that CurrentTargetFile
                    TailStateArray[FilebeatConfigName] = False

                    # Resting retry count in dictionary
                    TargetFileMTimeCountArray[FilebeatConfigName] = 0

                    # Log
                    logging.info("            * The CurrentTargetFile has not been modified since 1 day: stopping Tail thread by safety !")
                    # Exiting function
                    return
        except:
            # Value not initialized in dictionary, setting it
            TargetFileMTimeCountArray[FilebeatConfigName] = 0
            # Log
            logging.info("            * Modification time follow-up initialization... ")
            # Exiting function
            return

        else:
            # Not older than 1 day
            # Log
            logging.info("            * Modification time: Ok !")

    else:
        # No thread are running for this config, let's start a new one if possible
        
        # Converting date format, if any, in target file  
        CurrentTargetFile = ConvertFileName(TargetFile)
        
        # Checking if file exists or not
        if os.path.exists(CurrentTargetFile):
        # The file currently exists
            # Log
            logging.info("    * Initiating tail process on " + CurrentTargetFile)

            # Setting thread as active
            TailStateArray[FilebeatConfigName] = True

            # Refreshing CurrentTargetFile inode and size
            CurrentTargetFileSize = str(os.stat(CurrentTargetFile).st_size)
            CurrentTargetFileInode = str(os.stat(CurrentTargetFile).st_ino)

            # Formating string and storing into
            ConfigString = CurrentTargetFile + ',' + TargetFile + ',' + EGREPString + ',' + LastTailLine + ',' + CurrentTargetFileInode + ',' + CurrentTargetFileSize + ',' + MultilineSeparator
            FilebeatConfigsArray[FilebeatConfigName] = ConfigString

            # Launching thread
            FilebeatConfigThread = threading.Thread(target=TailPythonStyle, name=FilebeatConfigThreadName, args=(FilebeatConfigName, CurrentTargetFile, EGREPString, MultilineSeparator))
            FilebeatConfigThread.start()
        else:
            # The file does not exists
            logging.info("    * Initiating tail process on Target File " + CurrentTargetFile + " impossible, the file does not exists !")

    # Debug
    # print('\nDEBUG\n')
    # print('<!> DEBUG ', 'ConfigSplitLine', ConfigSplitLine)
    # print('<!> DEBUG ', 'TargetFile', TargetFile)
    # print('<!> DEBUG ', 'TargetInfoPattern', TargetInfoPattern)
    # print('<!> DEBUG ', 'TargetWarningPattern', TargetWarningPattern)
    # print('<!> DEBUG ', 'TargetErrorPattern', TargetErrorPattern)
    # print('<!> DEBUG ', 'EGREPString', EGREPString)
    # print('<!> DEBUG ', 'LastTailLine', LastTailLine)
    # print('<!> DEBUG ', 'StartInode', StartInode)
    # print('<!> DEBUG ', 'TargetSize', TargetSize)
    # print('<!> DEBUG ', 'MultilineSeparator', MultilineSeparator)
    # print('\nFINDEBUG\n')
    pass

def TailPythonStyle(FilebeatConfigName, TargetFile, EGREPString, MultilineSeparator):
    """ This function will tail a txt file in python style, as tail does not esists :-(

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """
    
    # Sharing TailStateArray and TargetFileCurrentPosArray with other threads
    global TailStateArray
    global TargetFileCurrentPosArray

    # Remove "" from input vars
    EGREPString = EGREPString.replace('"','')
    MultilineSeparator = MultilineSeparator.replace('"','')

    # Setting vars for threads follow up
    CurrentThread = threading.current_thread()

    # Opening TargetFile
    OpenedTargetFile = open(TargetFile,'r')

    # Checking TargetFile size to start tail at the last line
    TargetFileStats = os.stat(TargetFile)
    TargetFileSize = TargetFileStats[6]

    # Positionning cursor at the end of the file
    # Checking if TargetFileCurrentPosArray[FilebeatConfigName] is empty with try/catch
    try:
        # If value exists in dictionary, that's to say parsing was already running in this daemon execution
        # We reset it to 0 to start parsing the new file from begining
        TargetFileCurrentPosArray[FilebeatConfigName]
    except:
        # Na value inside dictionnary, that's to say this is the first daemon execution
        # We put the cursor on the last line of the file
        OpenedTargetFile.seek(TargetFileSize)

    # Going to make all work into try/catch to avoid exception and daemon crash
    # if something unexpected happen to the TargetFile
    try:
        # Start looping indefintly until TailStateArray[ThreadName] is different then True
        # It give control on this sub-thread from main thread to stop/clean it when necessary
        while (True and TailStateArray[FilebeatConfigName]):
            # Storing the current TargetFile's "last line" position
            TargetFileCurrentPosArray[FilebeatConfigName] = OpenedTargetFile.tell()
            # Gathering new lines if any
            NewLine = OpenedTargetFile.readline()

            # Checking if there are new lines happened to the TargetFile
            if not NewLine:
                # No new line detected
                # Then we set file position to the previous recoreded one
                # to check any new lines in the next loop execution
                OpenedTargetFile.seek(TargetFileCurrentPosArray[FilebeatConfigName])
                # Going to sleep 1 sec to save CPU
                time.sleep(1)

            else:
                # Removing \n at the end of thr readline() output
                NewLine = re.sub('\n$', '', NewLine)
                # New line has been written, let's store datetime and check for patterns
                SeqIdTimestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")
                
                # First we check if any of the three pattern is detected to save time
                if bool(re.search(EGREPString, NewLine)):
                    
                    # Getting the current line of the file
                    MatchLineOffset = OpenedTargetFile.tell()
                    
                    # If match is detected, we check which pattern it is
                    for Pattern in EGREPString.split('|'):
                        if (bool(re.search(Pattern, NewLine))):
                            MatchPattern = Pattern
                            break

                    # Let's construct JSON output for Filebeat index (ELK 7.4.0)
                    JSONToSend = (''
                    '{'
                    '\"agent\":{'
                    '\"hostname\":\"' + LPARName + '\",'
                    '\"id\":\"' + AgentID + '\",'
                    '\"ephemeral_id\":\"' + EphemeralID + '\",'
                    '\"type\":\"filebeat\",'
                    '\"version\":\"' + ELKMonitoringVersion + '\"'
                    '},'
                    '\"log\":{'
                    '\"file\":{'
                    '\"path\":\"' + TargetFile + '\"'
                    '},'
                    '\"offset\":' + str(MatchLineOffset) + ','
                    '\"pattern\":\"' + MatchPattern + '\"},'
                    
                    '\"message\":\"' + re.sub('[\\\[\]\(\)\{\}"\b\t\n\a\r]', ' ', NewLine) + '\",'
                    '\"input\":{'
                    '\"type\":\"log\"'
                    '},'
                    '\"@timestamp\":\"' + SeqIdTimestamp + '\",'
                    '\"ecs\":{'
                    '\"version\":\"' + ECSVersion + '\"'
                    '},'
                    '\"service\":{'
                    '\"type\":\"customfilebeat\"'
                    '},'
                    '\"host\":{'
                    '\"frame\":\"' + LPARHost + '\",\"site\":\"' + HostingSite + '\",\"app\":\"' + LPARAppName+ '\",\"hostname\":\"' + LPARName + '\",'
                    '\"os\":{'
                    '\"kernel\":\"' + AIXVersion + '\",'
                    '\"codename\":\"AIX\",'
                    '\"name\":\"AIX\",'
                    '\"family\":\"AIX\",'
                    '\"version\":\"' + AIXVersion + '\",'
                    '\"platform\":\"AIX\"'
                    '},'
                    '\"containerized\":false,'
                    '\"name\":\"' + LPARName + '\",'
                    '\"id\":\"' + LPARRndID + '\",'
                    '\"architecture\":\"' + LPARArch + '\"'
                    '},'
                    '\"event\":{'
                    '\"timezone\":\"+00:00\",'
                    '\"module\":\"customfilebeat\",'
                    '\"dataset\":\"custom.' + FilebeatConfigName + '\"'
                    '}'
                    '}'
                    '')
                    # Let's send JSON message to ELK Filebeat server
                    SendToELK(JSONToSend, "log")
                    # print('JSONToSend: ', JSONToSend)

        # When we reached that point, thread is dead by order
        # print('thread ' + CurrentThread.name + ' is dead by order')
        pass
    except:
        # Something happened to the thread, thread is dead by crash
        # print('thread ' + CurrentThread.name + ' is dead by crash')
        pass
    pass

def CustomMetricExec(OSScript, ItemName, ComparisonTimer):
    """ This function analyse Disks activity and send metrics to ElasticSearch server

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """

    # Generating current ELK timestamp for JSON message
    SeqIdTimestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")

    # Gathering data from OS command
    OSCmdResult = subprocess.Popen(OSScript, shell=True, stdout=subprocess.PIPE).stdout
    OSCmdResult = OSCmdResult.read().decode().split('\n')
    # print('OSCmdResult: ', str(OSCmdResult))

    for CurrLine in OSCmdResult:
        if CurrLine != '':
            CurrLine = CurrLine.split(' ')

            # Let's construct JSON output for Custom script execution (ELK 7.5.0)
            EmptyString = " "
            JSONToSendValues = (''
            '\"system\":{\"custom\":{\"' + ItemName + '\":{\"name\":\"' + CurrLine[0] + '\",\"value\":\"' + EmptyString.join(CurrLine[1:]).replace('"','') + '\"}}}')
           
            # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
            JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
            
            # Adding JSON metrics values gathered from OS command
            JSONToSend = JSONToSend + ',' + JSONToSendValues
            
            # Gathering "ELK formatted" dynamic JSON values
            RetDynamicJSON = GenerateDynamicJson("system.customexec", "system", "customexec", ComparisonTimer)
            
            CustomExec = "system.customexec." + ItemName
            CustomExecShort = "customexec_" + ItemName
            RetDynamicJSON = GenerateDynamicJson(CustomExec, "system", CustomExecShort, ComparisonTimer)
            
            # Adding dynamic "ELK Formating" JSON values into final JSON message
            JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
            
            # Let's send JSON message to ELK servers
            SendToELK(JSONToSend)
            # print('\JSONToSend\n', JSONToSend, '\n')
            pass

def PingPlotter(ComparisonTimer):
    """ This function analyse ping against multiple configured targets and send metrics to ElasticSearch server

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """
    
    # Setting vars for threads follow up
    global PingPlotterTargets
    global PingPlotterThread
    global PingSamples
    global PingTimeout
    PingPlotterThread = threading.currentThread()
    
    # Making test ping for all target in config file
    for PingTarget in PingPlotterTargets.split(','):
        # Generating current ELK timestamp for JSON message
        SeqIdTimestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")
        
        # To move both fields to Parameter.conf file
        PingSamples = 5
        PingTimeout = "1s"
        
        # Configuring pingparsin package
        ping_parser = pingparsing.PingParsing()
        transmitter = pingparsing.PingTransmitter()

        # Setting ping destination and timeout
        transmitter.destination = PingTarget
        transmitter.timeout = PingTimeout

        # Setting the number of ping to execute
        transmitter.count = PingSamples
        PingResult = transmitter.ping()

        if 'returncode=0' in str(PingResult):
            # Ping is ok, let's format output and JSON message
            ResultDict = json.loads(json.dumps(ping_parser.parse(PingResult).as_dict()))
            
            # Let's construct JSON output for SystemPingPlotter(ELK 7.5.0)
            JSONToSendValues = (''
            '\"system\":{\"pingplotter\":{\"destination\":\"' + str(ResultDict["destination"]) + '\",'
            '\"packet_loss_rate\":' + str(float(ResultDict["packet_loss_rate"]) / 100) + ',\"rtt_avg\":' + str(ResultDict["rtt_avg"]) + ','
            '\"rtt_max\":' + str(ResultDict["rtt_max"]) + ',\"packet_duplicate_rate\":' + str(float(ResultDict["packet_duplicate_rate"]) / 100) + '}}')
           
            # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
            JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
            
            # Adding JSON metrics values gathered from OS command
            JSONToSend = JSONToSend + ',' + JSONToSendValues
            
            # Gathering "ELK formatted" dynamic JSON values
            RetDynamicJSON = GenerateDynamicJson("system.pingplotter", "system", "pingplotter", ComparisonTimer)
            
            # Adding dynamic "ELK Formating" JSON values into final JSON message
            JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
            
            # Let's send JSON message to ELK servers
            SendToELK(JSONToSend)
            
            # print('\JSONToSend\n', JSONToSend, '\n')
            
        else:
            # An error has been thrown by pingparsing, maybe resolution error. 
            # Setting all counters to 0 and packet lost to 100%
            
            # Let's construct JSON output for SystemPingPlotter(ELK 7.5.0)
            JSONToSendValues = (''
            '\"system\":{\"pingplotter\":{\"destination\":\"' + PingTarget + '\",\"packet_loss_rate\":100,\"rtt_avg\":0,\"rtt_max\":0,\"packet_duplicate_rate\":0}}')
           
            # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
            JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
            
            # Adding JSON metrics values gathered from OS command
            JSONToSend = JSONToSend + ',' + JSONToSendValues
            
            # Gathering "ELK formatted" dynamic JSON values
            RetDynamicJSON = GenerateDynamicJson("system.customping", "system", "customping", ComparisonTimer)
            
            # Adding dynamic "ELK Formating" JSON values into final JSON message
            JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
            
            # Let's send JSON message to ELK servers
            SendToELK(JSONToSend)
            
            # print('\JSONToSend\n', JSONToSend, '\n')
            pass

def SystemCoreAndCpu(ComparisonTimer):
    """ This function analyse CPU activity and send metrics to ElasticSearch server

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """
    # Generating current ELK timestamp for JSON message
    SeqIdTimestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")
    
    # Gathering data from OS command for LPAR allocations
    CmdLine = 'lparstat -i | egrep "Online Virtual CPUs|Entitled Capacity" | head -2'
    LPARStatCmd = subprocess.Popen(CmdLine, shell=True, stdout=subprocess.PIPE).stdout
    LPARStatCmd = LPARStatCmd.read().decode().split('\n')

    # LPARSTAT Custom metrics
    CpuEntAlloc = str(LPARStatCmd[0].split(' ')[-1])
    VCPUAlloc = str(LPARStatCmd[1].split(' ')[-1])
    
    # Gathering current number of threads
    OnlineCPUThreads = subprocess.Popen("mpstat -w 1 1 | sed '1,4d' | grep -v U", shell=True, stdout=subprocess.PIPE).stdout
    OnlineCPUThreads = OnlineCPUThreads.read().decode().split('\n')

    # Removing last line if empty
    if OnlineCPUThreads[-1] == '':
        OnlineCPUThreads = OnlineCPUThreads[:-1]
       
    # Calculating number of online threads
    OnlineCPUThreadsCount = len(OnlineCPUThreads) - 1
    CurrOnlineVCPU = round(OnlineCPUThreadsCount / int(LPARSMTMode))
    
    # Gather data for system CPU internal values
    AllmpstatLine = OnlineCPUThreads[-1].split()
    CpuInterrupt = AllmpstatLine[4]
    ContextSwitch = AllmpstatLine[5]
    InvolContextSwitch = AllmpstatLine[6]
    VolContextSwitch = int(ContextSwitch) - int(InvolContextSwitch)
    SysCall = AllmpstatLine[10]
    LogicalContextSwitch = AllmpstatLine[-1]

    # IMPROVEMENT - To generalize -  Check in all script if casting value is necessary or not !!!

    # MPSTAT Custom metrics
    CpuEntAlloc =  str(CpuEntAlloc)
    VCPUAlloc = str(VCPUAlloc)
    OnlineCPUThreadsCount = str(OnlineCPUThreadsCount)
    CurrOnlineVCPU = str(CurrOnlineVCPU)
    CpuInterrupt = str(CpuInterrupt)
    ContextSwitch = str(ContextSwitch)
    InvolContextSwitch = str(InvolContextSwitch)
    VolContextSwitch = str(VolContextSwitch)
    SysCall = str(SysCall)
    LogicalContextSwitch = str(LogicalContextSwitch)
    
    # Gathering data from OS command for CPU usage
    SystemCoreAndCpuCmd = subprocess.Popen("vmstat -IWwt 1:kthr:r 1 | tail -1", shell=True, stdout=subprocess.PIPE).stdout
    SystemCoreAndCpuCmd = SystemCoreAndCpuCmd.read().decode().split()

    # Defining vars from command output
    CPUIdlePCTNorm = str(float(SystemCoreAndCpuCmd[17]) / 100)
    CPUUserPCTNorm = float(SystemCoreAndCpuCmd[15]) / 100
    CPUSystemPCTNorm = float(SystemCoreAndCpuCmd[16]) / 100
    CPUIOWaitPCTNorm = str(float(SystemCoreAndCpuCmd[18]) / 100)
    CPUTotalNorm = 1 - CPUUserPCTNorm - CPUSystemPCTNorm
    CPUTotalNorm = str(CPUTotalNorm)
    CPUUserPCTNorm = str(CPUUserPCTNorm)
    CPUSystemPCTNorm = str(CPUSystemPCTNorm)

    # VMSTAT Custom metrics
    CPUPhysc = str(SystemCoreAndCpuCmd[19])
    CPUEntcPCT = str(float(SystemCoreAndCpuCmd[20]) / 100)
    CPURunQueue = str(float(SystemCoreAndCpuCmd[0]))
    CPUBlockQueue = str(float(SystemCoreAndCpuCmd[1]))
    CPURawIOQueue = str(float(SystemCoreAndCpuCmd[2]))
    CPUWaitQueue = str(float(SystemCoreAndCpuCmd[3]))

    # IMPROVEMENT - Don't know how to translate,  = "0" (type str for string manipulation)
    CPUSoftIRQPCT = "0"
    CPUSoftIRQPCTNorm = "0"
    CPUStealPCT = "0"
    CPUStealPCTNorm = "0"
    CPUIRQPCT = "0"
    CPUIRQPCTNorm = "0"
    CPUNicePCT = "0"
    CPUNicePCTNorm = "0"
    
    # Let's construct JSON output for SystemCore (ELK 7.5.0)
    JSONToSendValues = (''
    '\"system\":{'
    '\"core\":{'
    '\"softirq\":{'
    '\"pct\":' + CPUSoftIRQPCTNorm + '},'
    '\"steal\":{'
    '\"pct\":' + CPUStealPCTNorm + '},'
    '\"idle\":{'
    '\"pct\":' + CPUIdlePCTNorm + '},'
    '\"system\":{'
    '\"pct\":' + CPUSystemPCTNorm + '},'
    '\"irq\":{'
    '\"pct\":' + CPUIRQPCTNorm + '},'
    '\"nice\":{'
    '\"pct\":' + CPUNicePCTNorm + '},'
    '\"id\":1,'
    '\"user\":{'
    '\"pct\":' + CPUUserPCTNorm + '},'
    '\"iowait\":{'
    '\"pct\":' + CPUIOWaitPCTNorm + '}}}')   
   
    # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
    JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
    
    # Adding JSON metrics values gathered from OS command
    JSONToSend = JSONToSend + ',' + JSONToSendValues
    
    # Gathering "ELK formatted" dynamic JSON values
    RetDynamicJSON = GenerateDynamicJson("system.core", "system", "core", ComparisonTimer)
    
    # Adding dynamic "ELK Formating" JSON values into final JSON message
    JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
    
    # Let's send JSON message to ELK servers
    SendToELK(JSONToSend)
    # print('\JSONToSend\n', JSONToSend, '\n')
       
    # Let's construct JSON output for SystemCPU(ELK 7.5.0)
    JSONToSendValues = (''
    '\"system\":{\"cpu\":{\"cores\":' + NumProcString + ','
    '\"custom\":{\"cpuphysusage\":' + CPUPhysc + ','
    '\"cpuentusagepct\":' + CPUEntcPCT + ','
    '\"onlinevcpu\":' + CurrOnlineVCPU + ','
    '\"onlinethreads\":' + OnlineCPUThreadsCount + ','
    '\"cpuentalloc\":' + CpuEntAlloc + ','
    '\"vcpualloc\":' + VCPUAlloc + ','
    '\"cpuinterrupt\":' + CpuInterrupt + ','
    '\"contextswitch\":' + ContextSwitch + ','
    '\"involcontextswitch\":' + InvolContextSwitch + ','
    '\"volcontextswitch\":' + VolContextSwitch + ','
    '\"syscall\":' + SysCall + ','
    '\"LogicalContextSwitch\":' + LogicalContextSwitch + ','
    '\"cpurunqueue\":' + CPURunQueue + ','
    '\"cpublockqueue\":' + CPUBlockQueue + ','
    '\"cpurawioqueue\":' + CPURawIOQueue + ','
    '\"cpuwaitqueue\":' + CPUWaitQueue + '},'
    '\"steal\":{\"pct\":' + CPUStealPCTNorm + '},'
    '\"idle\":{\"pct\":' + CPUIdlePCTNorm + '},'
    '\"nice\":{\"pct\":' + CPUNicePCTNorm + '},'
    '\"softirq\":{\"pct\":' + CPUSoftIRQPCTNorm + '},'
    '\"system\":{\"pct\":' + CPUSystemPCTNorm + '},'
    '\"iowait\":{\"pct\":' + CPUIOWaitPCTNorm + '},'
    '\"total\":{\"pct\":' + CPUTotalNorm + '},'
    '\"user\":{\"pct\":' + CPUUserPCTNorm + '},'
    '\"irq\":{\"pct\":' + CPUIRQPCTNorm + '}'
    '}}')
   
    # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
    JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
    
    # Adding JSON metrics values gathered from OS command
    JSONToSend = JSONToSend + ',' + JSONToSendValues
    
    # Gathering "ELK formatted" dynamic JSON values
    RetDynamicJSON = GenerateDynamicJson("system.cpu", "system", "cpu", ComparisonTimer)
    
    # Adding dynamic "ELK Formating" JSON values into final JSON message
    JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
    
    # Let's send JSON message to ELK servers
    SendToELK(JSONToSend)
    # print('\JSONToSend\n', JSONToSend, '\n')
    pass

def SystemFc(ComparisonTimer):
    """ This function analyse FC activity and send metrics to ElasticSearch server

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """
    # Setting vars for threads follow up
    # global SystemFcThread
    # SystemFcThread = threading.currentThread()
    
    
    # For each FC card with link ok
    for FcCard in FcCards:
        # Removing empty lines
        if(len(FcCard) != 0):
            # Generating current ELK timestamp for JSON message
            SeqIdTimestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")

            # Gathering data from OS command
            SystemFCCmd = 'fcstat ' + FcCard + ' | egrep -p "FC SCSI Traffic Statistics|FC SCSI Adapter Driver Information"'
            SystemFC = subprocess.Popen(SystemFCCmd, shell=True, stdout=subprocess.PIPE).stdout
            SystemFC = SystemFC.read().decode().split('\n')

            # Get data from multiple lines
            # For No DMA
            NoDMA = SystemFC[1].split()
            # For No Adapter Elements
            NoAdapterElements = SystemFC[2].split()
            # For No Command Resource
            NoCommandResource = SystemFC[3].split()
            # For Input Requests
            InputRequests = SystemFC[6].split()
            # For Output Requests
            OutputRequests = SystemFC[7].split()
            # For Input Bytes
            InputBytes = SystemFC[9].split()
            # Output Bytes
            OutputBytes = SystemFC[10].split()
            CurrFcCardWriteBytes = str(OutputBytes[2])

            # Filling values for this network interface
            # Grouping all kind of errors into one counter
            # IMPROVEMENT- Can be done by error counter but group method is easyer for counters in ELK...
            CurrFcCardErrors1 = int(NoDMA[4])
            CurrFcCardErrors2 = int(NoAdapterElements[4])
            CurrFcCardErrors3 = int(NoCommandResource[4])
            TotalFCError = CurrFcCardErrors1 + CurrFcCardErrors2 + CurrFcCardErrors3
            TotalFCError = str(TotalFCError)
            CurrFcCardErrors1 = str(CurrFcCardErrors1)
            CurrFcCardErrors2 = str(CurrFcCardErrors2)
            CurrFcCardErrors3 = str(CurrFcCardErrors3)
            # No disctinction between read/write errors in FC
            CurrFcCardWriteErrors = TotalFCError
            CurrFcCardWriteDropped = TotalFCError
            CurrFcCardReadErrors = TotalFCError
            CurrFcCardReadDropped = TotalFCError
            # Gathering Bytes and Packets values
            CurrFcCardWriteBytes = str(OutputBytes[2])
            CurrFcCardWritePackets = str(OutputRequests[2])
            CurrFcCardReadBytes = str(InputBytes[2])
            CurrFcCardReadPackets = str(InputRequests[2])
            
            # Let's construct JSON output for SystemFc (ELK 7.5.0)
            JSONToSendValues = (''
            '\"system\":{\"fc\":{\"name\":\"' + FcCard + '\",'
            '\"in\":{\"packets\":' + CurrFcCardReadPackets + ','
            '\"errors\":' + CurrFcCardReadErrors + ','
            '\"dropped\":' + CurrFcCardReadDropped + ','
            '\"bytes\":' + CurrFcCardReadBytes + '},'
            '\"out\":{\"errors\":' + CurrFcCardWriteErrors + ','
            '\"dropped\":' + CurrFcCardWriteDropped + ','
            '\"packets\":' + CurrFcCardWritePackets + ','
            '\"bytes\":' + CurrFcCardWriteBytes + '}}}')

            # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
            JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
            
            # Adding JSON metrics values gathered from OS command
            JSONToSend = JSONToSend + ',' + JSONToSendValues
            
            # Gathering "ELK formatted" dynamic JSON values
            RetDynamicJSON = GenerateDynamicJson("system.fc", "system", "fc", ComparisonTimer)
            
            # Adding dynamic "ELK Formating" JSON values into final JSON message
            JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
            
            # Let's send JSON message to ELK servers
            SendToELK(JSONToSend)
            # print('\JSONToSend\n', JSONToSend, '\n')
            pass

def SystemNetwork(ComparisonTimer):
    """ This function analyse Network activity and send metrics to ElasticSearch server

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """

    # Setting vars for threads follow up
    # global SystemNetworkThread
    # SystemNetworkThread = threading.currentThread()

    for NetworkCard in NetworkCards:
        # Removing empty lines
        if(len(NetworkCard) != 0):
            # Generating current ELK timestamp for JSON message
            SeqIdTimestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")

            # Gathering data from OS command
            SystemNetworkCmd = 'entstat ' + NetworkCard + ' | egrep "Packets|Bytes|Errors" | head -4'
            SystemNetwork = subprocess.Popen(SystemNetworkCmd, shell=True, stdout=subprocess.PIPE).stdout
            SystemNetwork = SystemNetwork.read().decode().split('\n')

            # Get data from multiple lines
            # For input/output packets
            PacketsLine = SystemNetwork[0].split()
            CurrNetCardReadPackets = str(PacketsLine[3])
            CurrNetCardWritePackets = str(PacketsLine[1])

            # For input/output bytes
            BytesLine = SystemNetwork[1].split()
            CurrNetCardReadBytes = str(BytesLine[3])
            CurrNetCardWriteBytes = str(BytesLine[1])

            # For input/output errors packets
            ErrorsLine = SystemNetwork[2].split()
            CurrNetCardReadErrors = str(ErrorsLine[5])
            CurrNetCardWriteErrors = str(ErrorsLine[2])

            # For input/output dropped packets
            DroppedLine = SystemNetwork[3].split()
            CurrNetCardReadDropped = str(DroppedLine[5])
            CurrNetCardWriteDropped = str(DroppedLine[2])

            # Let's construct JSON output for SystemNetwork (ELK 7.5.0)
            JSONToSendValues = (''
            '\"system\":{\"network\":{\"name\":\"' + NetworkCard + '\",'
            '\"in\":{\"packets\":' + CurrNetCardReadPackets + ','
            '\"errors\":' + CurrNetCardReadErrors + ','
            '\"dropped\":' + CurrNetCardReadDropped + ','
            '\"bytes\":' + CurrNetCardReadBytes + '},'
            '\"out\":{\"errors\":' + CurrNetCardWriteErrors + ','
            '\"dropped\":' + CurrNetCardWriteDropped + ','
            '\"packets\":' + CurrNetCardWritePackets + ','
            '\"bytes\":' + CurrNetCardWriteBytes + '}}}')

            
            # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
            JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
            
            # Adding JSON metrics values gathered from OS command
            JSONToSend = JSONToSend + ',' + JSONToSendValues
            
            # Gathering "ELK formatted" dynamic JSON values
            RetDynamicJSON = GenerateDynamicJson("system.network", "system", "network", ComparisonTimer)
            
            # Adding dynamic "ELK Formating" JSON values into final JSON message
            JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
            
            # Let's send JSON message to ELK servers
            SendToELK(JSONToSend)
            # print('\JSONToSend\n', JSONToSend, '\n')
            pass
 
def SystemLoad(ComparisonTimer):
    """ This function analyse CPU load and send metrics to ElasticSearch server

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """

    # Setting vars for threads follow up
    # global SystemLoadThread
    # SystemLoadThread = threading.currentThread()

    # Generating current ELK timestamp for JSON message
    SeqIdTimestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")

    # Gathering data from OS command
    SystemLoadCmd = subprocess.Popen("uptime", shell=True, stdout=subprocess.PIPE).stdout
    SystemLoadCmd = SystemLoadCmd.read().decode().split()

    # Defining vars from command output
    LoadOne = float(SystemLoadCmd[-3].replace(',',''))
    LoadFive = float(SystemLoadCmd[-2].replace(',',''))
    LoadFifteen = float(SystemLoadCmd[-1])

    # Calculating per core values
    LoadOnePerCore = LoadOne / NumProc
    LoadFivePerCore = LoadFive / NumProc
    LoadFifteenPerCore = LoadFifteen / NumProc

    # Converting to string for JSON formating
    LoadOne = str(LoadOne)
    LoadFive = str(LoadFive)
    LoadFifteen = str(LoadFifteen)
    LoadOnePerCore = str(LoadOnePerCore)
    LoadFivePerCore = str(LoadFivePerCore)
    LoadFifteenPerCore = str(LoadFifteenPerCore)
    
    # Let's construct JSON output for SystemLoad (ELK 7.5.0)
    JSONToSendValues = (''
    '\"system\":{\"load\":{\"1\":' + LoadOne + ','
    '\"5\":' + LoadFive + ','
    '\"15\":' + LoadFifteen + ','
    '\"norm\":{\"1\":' + LoadOnePerCore + ','
    '\"5\":' + LoadFivePerCore + ','
    '\"15\":' + LoadFifteenPerCore + '},'
    '\"cores\":' + NumProcString + '}}')
    
    # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
    JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
    
    # Adding JSON metrics values gathered from OS command
    JSONToSend = JSONToSend + ',' + JSONToSendValues
    
    # Gathering "ELK formatted" dynamic JSON values
    RetDynamicJSON = GenerateDynamicJson("system.load", "system", "load", ComparisonTimer)
    
    # Adding dynamic "ELK Formating" JSON values into final JSON message
    JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
    
    # Let's send JSON message to ELK servers
    SendToELK(JSONToSend)
    # print('\JSONToSend\n', JSONToSend, '\n')
    pass

def SystemMemory(ComparisonTimer):
    """ This function analyse Memory activity and send metrics to ElasticSearch server

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """

    # Setting vars for threads follow up
    global SystemMemoryThread
    SystemMemoryThread = threading.currentThread()

    # Generating current ELK timestamp for JSON message
    SeqIdTimestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")

    # Gathering data from OS command
    SystemMemory = subprocess.Popen('svmon -G -O unit=KB | egrep "memory|space"', shell=True, stdout=subprocess.PIPE).stdout
    SystemMemory = SystemMemory.read().decode().split('\n')

    # Get data from multiple lines
    # For Memory line
    MemoryLine = SystemMemory[0].split()
    # print('MemoryLine', MemoryLine)
    # For Space line
    PagingLine = SystemMemory[1].split()
    # print('PagingLine', PagingLine)

    # Get MEMORY usage informations and convert to B
    RealMemTotal = int(MemoryLine[1]) * 1024
    RealMemFree = int(MemoryLine[3]) * 1024
    RealMemUsed = int(MemoryLine[2]) * 1024    

    # Calculating percentage for memory
    # BUG - Removed ' * 100 ' for ELK correct formating but strange...
    RealMemUsedPCT = RealMemUsed / RealMemTotal

    # Custom metrics
    VirtualMemUsed = int(MemoryLine[5]) * 1024
    VirtualMemUsedPCT = VirtualMemUsed / RealMemTotal
    CacheMemUsed = RealMemUsed - VirtualMemUsed
    CacheMemUsedPCT = CacheMemUsed / RealMemTotal

    # Get SWAP usage informations and convert MB to B
    TotalSwapBytes = int(PagingLine[2]) * 1024
    UsedSwapBytes = int(PagingLine[3]) * 1024
    
    FreeSwapBytes = TotalSwapBytes - UsedSwapBytes

    # Calculating percentage for paging
    UsedSwapPCT = UsedSwapBytes / TotalSwapBytes

    # Converting to string
    RealMemTotal = str(RealMemTotal)
    RealMemFree = str(RealMemFree)
    RealMemUsed = str(RealMemUsed)
    RealMemUsedPCT = str(RealMemUsedPCT)
    VirtualMemUsed = str(VirtualMemUsed)
    VirtualMemUsedPCT = str(VirtualMemUsedPCT)
    CacheMemUsed = str(CacheMemUsed)
    CacheMemUsedPCT = str(CacheMemUsedPCT)
    TotalSwapBytes = str(TotalSwapBytes)
    UsedSwapBytes = str(UsedSwapBytes)
    FreeSwapBytes = str(FreeSwapBytes)
    UsedSwapPCT = str(UsedSwapPCT)

    # IMPROVEMENT- If LargePages are used on AIX, this part will need some work to define correct values
    # For now, setting to str = "0"
    hugepagesfree = "0"
    hugepagesreserved = "0"
    hugepagessurplus = "0"
    readaheadpages = "0"
    readaheadcached = "0"
    # IMPROVEMENT - This is the default pagesize for LARGEPAGE AIX, can implement huge page detection but not used for now in our AIX env
    hugepagesdefaultsize = "16384000"
    hugepagestotal = "0"
    hugepagesbytes = "0"
    hugepagespct = "0"

    # Gathering data from OS command N+0
    SystemMemoryA = subprocess.Popen('vmstat -vs', shell=True, stdout=subprocess.PIPE).stdout
    SystemMemoryA = SystemMemoryA.read().decode().split('\n')
    
    # Sleeping for 1 sec to make second measurement
    time.sleep(1)
   
    # Gathering data from OS command N+1
    SystemMemoryB = subprocess.Popen('vmstat -vs', shell=True, stdout=subprocess.PIPE).stdout
    SystemMemoryB = SystemMemoryB.read().decode().split('\n')
    
    # Converting into string and calulating diference between values
    pagefault = str(int(SystemMemoryB[0].split()[0]) - int(SystemMemoryA[0].split()[0]))
    rampagein = str(int(SystemMemoryB[1].split()[0]) - int(SystemMemoryA[1].split()[0]))
    rampageout = str(int(SystemMemoryB[2].split()[0]) - int(SystemMemoryA[2].split()[0]))
    swappagein = str(int(SystemMemoryB[3].split()[0]) - int(SystemMemoryA[3].split()[0]))
    swappageout = str(int(SystemMemoryB[4].split()[0]) - int(SystemMemoryA[4].split()[0]))
    reclaims = str(int(SystemMemoryB[5].split()[0]) - int(SystemMemoryA[5].split()[0]))
    pagescanned = str(int(SystemMemoryB[8].split()[0]) - int(SystemMemoryA[8].split()[0]))
    pagesfreed = str(int(SystemMemoryB[10].split()[0]) - int(SystemMemoryA[10].split()[0]))
    pendingiowait = str(int(SystemMemoryB[14].split()[0]) - int(SystemMemoryA[14].split()[0]))
    pagenumber = str(int(SystemMemoryB[26].split()[0]) - int(SystemMemoryA[26].split()[0]))
    lruablepages = str(int(SystemMemoryB[27].split()[0]) - int(SystemMemoryA[27].split()[0]))
    freepages = str(int(SystemMemoryB[28].split()[0]) - int(SystemMemoryA[28].split()[0]))
    pinnedpages = str(int(SystemMemoryB[30].split()[0]) - int(SystemMemoryA[30].split()[0]))
    numpermpct = str(float(SystemMemoryB[34].split()[0]) / 100)
    filepages = str(int(SystemMemoryB[35].split()[0]) - int(SystemMemoryA[35].split()[0]))
    numclientpct = str(float(SystemMemoryB[38].split()[0]) / 100)
    clientpages = str(int(SystemMemoryB[40].split()[0]) - int(SystemMemoryA[40].split()[0]))
    diskblockedio = str(int(SystemMemoryB[42].split()[0]) - int(SystemMemoryA[42].split()[0]))
    swapblockedio = str(int(SystemMemoryB[43].split()[0]) - int(SystemMemoryA[43].split()[0]))
    fsblockedio = str(int(SystemMemoryB[44].split()[0]) - int(SystemMemoryA[44].split()[0]))
    clientfsblockedio = str(int(SystemMemoryB[45].split()[0]) - int(SystemMemoryA[45].split()[0]))
    externalpagerfsblockedio = str(int(SystemMemoryB[46].split()[0]) - int(SystemMemoryA[46].split()[0]))
    computpagespct = str(float(SystemMemoryB[47].split()[0]) / 100)

    # Formatting custom metrics JSON string
    CustomMetricJSON = (''
    '\"custom\":{\"virtualmemory\":{\"value\":' + VirtualMemUsed + ',\"pct\":' + VirtualMemUsedPCT + '},\"cachedmemory\":{\"value\":' + CacheMemUsed + ',\"pct\":' + CacheMemUsedPCT + '},'
    '\"pagefaultpersec\":' + pagefault + ',\"ram\":{\"pageinpersec\":' + rampagein + ',\"pageoutpersec\":' + rampageout + '},'
    '\"swap\":{\"pageinpersec\":' + swappagein + ',\"pageoutpersec\":' + swappageout + '},\"reclaimspersec\":' + reclaims + ',\"pagescannedpersec\":' + pagescanned + ','
    '\"pagesfreedpersec\":' + pagesfreed + ',\"pendingiowaitpersec\":' + pendingiowait + ',\"pagenumberpersec\":' + pagenumber + ',\"lruablepagespersec\":' + lruablepages + ','
    '\"freepagespersec\":' + freepages + ',\"pinnedpagespersec\":' + pinnedpages + ',\"numpermpct\":' + numpermpct + ',\"filepagespersec\":' + filepages + ','
    '\"numclientpct\":' + numclientpct + ',\"clientpagespersec\":' + clientpages + ',\"blockedio\":{\"diskpersec\":' + diskblockedio + ',\"swappersec\":' + swapblockedio + ','
    '\"fspersec\":' + fsblockedio + ',\"clientfspersec\":' + clientfsblockedio + ',\"externalpagerfspersec\":' + externalpagerfsblockedio + '},\"computpagespct\":' + computpagespct + '},')

    # Let's construct JSON output for SystemMemory (ELK 7.5.0)
    JSONToSendValues = (''
    '\"system\":{'
    '\"memory\":{' + CustomMetricJSON + ''
    '\"total\":' + RealMemTotal + ','
    '\"used\":{'
    '\"bytes\":' + RealMemUsed + ','
    '\"pct\":' + RealMemUsedPCT + '},'
    '\"free\":' + RealMemFree + ','
    '\"actual\":{'
    '\"free\":' + RealMemFree + ','
    '\"used\":{'
    '\"pct\":' + RealMemUsedPCT + ','
    '\"bytes\":' + RealMemUsed + '}},'
    '\"swap\":{'
    '\"in\":{'
    '\"pages\":' + swappagein + '},'
    '\"out\":{'
    '\"pages\":' + swappageout + '},'
    '\"readahead\":{'
    '\"pages\":' + readaheadpages + ','
    '\"cached\":' + readaheadcached + '},'
    '\"total\":' + TotalSwapBytes + ','
    '\"used\":{'
    '\"bytes\":' + UsedSwapBytes + ','
    '\"pct\":' + UsedSwapPCT + '},'
    '\"free\":' + FreeSwapBytes + '},'
    '\"hugepages\":{'
    '\"total\":' + hugepagestotal + ','
    '\"used\":{'
    '\"pct\":' + hugepagespct + ','
    '\"bytes\":' + hugepagesbytes + '},'
    '\"free\":' + hugepagesfree + ','
    '\"reserved\":' + hugepagesreserved + ','
    '\"surplus\":' + hugepagessurplus + ','
    '\"default_size\":' + hugepagesdefaultsize + ','
    '\"swap\":{'
    '\"out\":{'
    '\"fallback\":' + pagefault + ','
    '\"pages\":' + swappageout + '}}}}}')

    
    # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
    JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
    
    # Adding JSON metrics values gathered from OS command
    JSONToSend = JSONToSend + ',' + JSONToSendValues
    
    # Gathering "ELK formatted" dynamic JSON values
    RetDynamicJSON = GenerateDynamicJson("system.memory", "system", "memory", ComparisonTimer)
    
    # Adding dynamic "ELK Formating" JSON values into final JSON message
    JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
    
    # Let's send JSON message to ELK servers
    SendToELK(JSONToSend)
    # print('\JSONToSend\n', JSONToSend, '\n')
    pass

def SystemProcessSummary(ComparisonTimer):
    """ This function analyse processes activity and send metrics to ElasticSearch server

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """
    # Generating current ELK timestamp for JSON message
    SeqIdTimestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")

    # Gathering data from OS command
    SystemProcessSummary = subprocess.Popen("ps -ef -m -o status | awk '{print $NF}'", shell=True, stdout=subprocess.PIPE).stdout
    SystemProcessSummary = SystemProcessSummary.read().decode().split()

    # Defining vars from command output
    # CPUIdlePCTNorm = str(float(SystemCoreAndCpuCmd[17]) / 100)

    # Defining counters for increment loop
    TotalProcessesCount = 0
    ActivePS = 0
    SwappedPS = 0
    IdlePS = 0
    CanceledPS = 0
    StoppedPS = 0
    RunningPS = 0
    SleepingPS = 0
    SwappedPS = 0
    UnknownPS = 0
    ZombiePS = 0
    DeadPS = 0

    # Incrementing counters depending of the type of process
    # IMPROVEMENT - Can be imporved by unsing Python's style swith/case but maybe incompatible with python2.7, to check !
    for State in SystemProcessSummary:
        if "A" in State:
            ActivePS = ActivePS + 1
            TotalProcessesCount = TotalProcessesCount + 1
        elif "W" in State:
            SwappedPS = SwappedPS + 1
            TotalProcessesCount = TotalProcessesCount + 1
        elif "I" in State:
            IdlePS = IdlePS + 1
            TotalProcessesCount = TotalProcessesCount + 1
        elif "Z" in State:
            CanceledPS = CanceledPS + 1
            TotalProcessesCount = TotalProcessesCount + 1
        elif "T" in State:
            StoppedPS = StoppedPS + 1
            TotalProcessesCount = TotalProcessesCount + 1
        elif "R" in State:
            RunningPS = RunningPS + 1
            TotalProcessesCount = TotalProcessesCount + 1
        elif "S" in State:
            SleepingPS = SleepingPS + 1
            TotalProcessesCount = TotalProcessesCount + 1
        else:
            UnknownPS = UnknownPS + 1
            TotalProcessesCount = TotalProcessesCount + 1

    # Gathering data from OS command Zombie processes
    SystemProcessSummaryZombiePS = subprocess.Popen("ps -ef | grep -i defunct | grep -v grep | wc -l | awk '{print $NF}'", shell=True, stdout=subprocess.PIPE).stdout
    ZombiePS = SystemProcessSummaryZombiePS.read().decode().split()[0]

    # Calculating dead process upon Stopped and Canceled states
    DeadPS = CanceledPS + StoppedPS

    # Converting to string
    TotalProcessesCount = str(TotalProcessesCount)
    ActivePS = str(ActivePS)
    SwappedPS = str(SwappedPS)
    IdlePS = str(IdlePS)
    CanceledPS = str(CanceledPS)
    StoppedPS = str(StoppedPS)
    RunningPS = str(RunningPS)
    SleepingPS = str(SleepingPS)
    SwappedPS = str(SwappedPS)
    UnknownPS = str(UnknownPS)
    ZombiePS = str(ZombiePS)
    DeadPS = str(DeadPS)

    # Let's construct JSON output for SystemProcessSummary (ELK 7.5.0)
    JSONToSendValues = (''
    '\"system\":{\"process\":{\"summary\":{\"zombie\":' + ZombiePS + ','
    '\"active\":' + ActivePS + ','
    '\"unknown\":' + UnknownPS + ','
    '\"dead\":' + DeadPS + ','
    '\"total\":' + TotalProcessesCount + ','
    '\"sleeping\":' + SleepingPS + ','
    '\"running\":' + RunningPS + ','
    '\"idle\":' + IdlePS + ','
    '\"stopped\":' + StoppedPS + '}}}')
    
    # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
    JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
    
    # Adding JSON metrics values gathered from OS command
    JSONToSend = JSONToSend + ',' + JSONToSendValues
    
    # Gathering "ELK formatted" dynamic JSON values
    RetDynamicJSON = GenerateDynamicJson("system.process.summary", "system", "process_summary", ComparisonTimer)
    
    # Adding dynamic "ELK Formating" JSON values into final JSON message
    JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
    
    # Let's send JSON message to ELK servers
    SendToELK(JSONToSend)
    # print('\JSONToSend\n', JSONToSend, '\n')
    pass

def SystemDiskIO(ComparisonTimer):
    """ This function analyse disks activity and send metrics to ElasticSearch server

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """

    # Setting vars for threads follow up
    global SystemDiskIOThread
    SystemDiskIOThread = threading.currentThread()

    # Checking if some restrictions are in place for HDISKs list
    if HdiskRestricted == 'all':
        # Gathering disk list from OS command
        SystemDiskIO = subprocess.Popen("lspv | awk '{print $1}'", shell=True, stdout=subprocess.PIPE).stdout
        SystemDiskIO = SystemDiskIO.read().decode().split()
        
        # Getting system command ready
        SystemDiskIOCmd = 'iostat -D ' + str(DiskSampleRate) + ' 1'
        
    else:
        # Restriction is in place,reducing disk list to specified disks
        SystemDiskIO = []
        SystemDiskIOStat = HdiskRestricted.replace(',',' ')
        # logging.info("SystemDiskIOStat " + str(SystemDiskIOStat))
        SystemDiskIO = HdiskRestricted.split(',')
        # logging.info("SystemDiskIO " + str(SystemDiskIO))
        
        # Getting system command ready
        SystemDiskIOCmd = 'iostat -D ' + str(SystemDiskIOStat) + ' ' + str(DiskSampleRate) + ' 1'
        
    # Generating current ELK timestamp for JSON message
    SeqIdTimestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")
    
    # Gathering data for disks
    SystemDiskIOResultArray = subprocess.Popen(SystemDiskIOCmd, shell=True, stdout=subprocess.PIPE).stdout
    SystemDiskIOResultArray = SystemDiskIOResultArray.read().decode().split('\n')
    
    # Initializing start ID's for result lines
    NameSplitedLineId = 3
    XferSplitedLineId = 4
    ReadSplitedLineId = 6
    WriteSplitedLineId = 8
    QueueSplitedLineId = 10

    # Looping on all disks
    for Disk in SystemDiskIO:
        # Split cmd result into different lines
        NameSplitedLine = SystemDiskIOResultArray[NameSplitedLineId].split()
        XferSplitedLine = SystemDiskIOResultArray[XferSplitedLineId].split()
        ReadSplitedLine = SystemDiskIOResultArray[ReadSplitedLineId].split()
        WriteSplitedLine = SystemDiskIOResultArray[WriteSplitedLineId].split()
        QueueSplitedLine = SystemDiskIOResultArray[QueueSplitedLineId].split()
        
        # Incrementing Id's to switch to the next disk on the next diskio loop execution
        NameSplitedLineId = NameSplitedLineId + 8
        XferSplitedLineId = XferSplitedLineId + 8
        ReadSplitedLineId = ReadSplitedLineId + 8
        WriteSplitedLineId = WriteSplitedLineId + 8
        QueueSplitedLineId = QueueSplitedLineId + 8
        
        # Setting disk name
        DiskName = NameSplitedLine[0]

        # Gathering data for write counters
        DiskWriteCount = WriteSplitedLine[0]
        DiskWriteTime = WriteSplitedLine[1]
        DiskWriteBytes = XferSplitedLine[4]

        # Converting value depending of the unit
        if DiskWriteBytes.endswith("K"):
            
            # DiskWriteBytes = DiskWriteBytes.replace(".", "")
            DiskWriteBytes = float(DiskWriteBytes.replace("K", "")) * 1024
            pass
        elif DiskWriteBytes.endswith("M"):
            # DiskWriteBytes = DiskWriteBytes.replace(".", "")
            # DiskWriteBytes = DiskWriteBytes.replace("M", "") + "00000"
            DiskWriteBytes = float(DiskWriteBytes.replace("M", "")) * 1024 * 1024
            pass
        elif DiskWriteBytes.endswith("G"):
            # DiskWriteBytes = DiskWriteBytes.replace(".", "")
            # DiskWriteBytes = DiskWriteBytes.replace("G", "") + "00000000"
            DiskWriteBytes = float(DiskWriteBytes.replace("G", "")) * 1024 * 1024 * 1024
            pass

        # Gathering data for write counters
        DiskReadCount = ReadSplitedLine[0]
        DiskReadTime = ReadSplitedLine[1]
        DiskReadBytes = XferSplitedLine[3]

        # Converting value depending of the unit
        if DiskReadBytes.endswith("K"):
            # DiskReadBytes = DiskReadBytes.replace(".", "")
            # DiskReadBytes = DiskReadBytes.replace("K", "") + "00"
            DiskReadBytes = float(DiskReadBytes.replace("K", "")) * 1024
            pass
        elif DiskReadBytes.endswith("M"):
            # DiskReadBytes = DiskReadBytes.replace(".", "")
            # DiskReadBytes = DiskReadBytes.replace("M", "") + "00000"
            DiskReadBytes = float(DiskReadBytes.replace("M", "")) * 1024 * 1024
            pass
        elif DiskReadBytes.endswith("G"):
            # DiskReadBytes = DiskReadBytes.replace(".", "")
            # DiskReadBytes = DiskReadBytes.replace("G", "") + "00000000"
            DiskReadBytes = float(DiskReadBytes.replace("G", "")) * 1024 * 1024 * 1024
            pass

        # Gathering IO Queue size and busy counters
        IoWaitQueueAVGSize = QueueSplitedLine[3]
        IoServiceQueueAVGSize = QueueSplitedLine[4]
        IoQueueFullSize = QueueSplitedLine[5]
        IoBusy = XferSplitedLine[0]

        # Calculating value depending of disk measurement period
        DiskWriteRequestPerSec = float(DiskWriteCount) * DiskSampleRate
        DiskWriteBytesPerSec = float(DiskWriteBytes) * DiskSampleRate
        DiskReadRequestPerSec = float(DiskReadCount) * DiskSampleRate
        DiskReadBytesPerSec = float(DiskReadBytes) * DiskSampleRate

        # IMPROVEMENT - IoTime - Je sais pas le traduire =0 pour le moment...
        IoTime = 0
        # IMPROVEMENT - IoRequestAVGSize - Je sais pas le traduire =0 pour le moment...
        IORequestAVGSize = 0
        # IMPROVEMENT - IoAwait - Je sais pas le traduire =0 pour le moment...
        IoAwait = 0
        # IMPROVEMENT - DiskWriteAwait - Je sais pas le traduire =0 pour le moment...
        DiskWriteAwait = 0
        # IMPROVEMENT - DiskWriteRequestMergePerSec - Je sais pas le traduire =0 pour le moment...
        DiskWriteRequestMergePerSec = 0
        # IMPROVEMENT - DiskReadAwait - Je sais pas le traduire =0 pour le moment...
        DiskReadAwait = 0
        # IMPROVEMENT - DiskReadRequestMergePerSec - Je sais pas le traduire =0 pour le moment...
        DiskReadRequestMergePerSec = 0

        # Calculating
        Globalawait = int(float(DiskWriteAwait)) + int(float(DiskReadAwait)) / 2
        IoServiceTime = int(float(DiskWriteTime)) + int(float(DiskReadTime)) / 2

        # Converting all vars to string type
        DiskName = str(DiskName)
        XferSplitedLine = str(XferSplitedLine)
        ReadSplitedLine = str(ReadSplitedLine)
        WriteSplitedLine = str(WriteSplitedLine)
        QueueSplitedLine = str(QueueSplitedLine)
        DiskWriteCount = str(DiskWriteCount)
        DiskWriteTime = str(DiskWriteTime)
        DiskWriteBytes = str(DiskWriteBytes)
        DiskReadCount = str(DiskReadCount)
        DiskReadTime = str(DiskReadTime)
        DiskReadBytes = str(DiskReadBytes)
        IoWaitQueueAVGSize = str(IoWaitQueueAVGSize)
        IoServiceQueueAVGSize = str(IoServiceQueueAVGSize)
        IoQueueFullSize = str(IoQueueFullSize)
        IoBusy = str(IoBusy)
        DiskWriteRequestPerSec = str(DiskWriteRequestPerSec)
        DiskWriteBytesPerSec = str(DiskWriteBytesPerSec)
        DiskReadRequestPerSec = str(DiskReadRequestPerSec)
        DiskReadBytesPerSec = str(DiskReadBytesPerSec)
        Globalawait = str(Globalawait)
        IoServiceTime = str(IoServiceTime)
        IoTime = str(IoTime)
        IORequestAVGSize = str(IORequestAVGSize)
        IoAwait = str(IoAwait)
        DiskWriteAwait = str(DiskWriteAwait)
        DiskWriteRequestMergePerSec = str(DiskWriteRequestMergePerSec)
        DiskReadAwait = str(DiskReadAwait)
        DiskReadRequestMergePerSec = str(DiskReadRequestMergePerSec)

        # Let's construct JSON output for SystemDiskIO (ELK 7.5.0)       
        JSONToSendValues = (''
        '\"system\":{\"diskio\":{\"iostat\":{'
        '\"await\":' + Globalawait + ','
        '\"service_time\":' + IoServiceTime + ','
        '\"busy\":' + IoBusy + ','
        '\"read\":{\"per_sec\":{'
        '\"bytes\":' + DiskReadBytes + '},'
        '\"await\":' + DiskReadAwait + ','
        '\"request\":{'
        '\"merges_per_sec\":' + DiskReadRequestMergePerSec + ','
        '\"per_sec\":' + DiskReadCount + '}},'
        '\"write\":{'
        '\"request\":{'
        '\"merges_per_sec\":' + DiskWriteRequestMergePerSec + ','
        '\"per_sec\":' + DiskWriteCount + '},'
        '\"per_sec\":{'
        '\"bytes\":' + DiskWriteBytes + '},'
        '\"await\":' + DiskWriteAwait + ''
        '},\"queue\":{'
        '\"avg_size\":' + IoWaitQueueAVGSize + ','
        '\"sq_avg_size\":' + IoServiceQueueAVGSize + ','
        '\"sq_full_size\":' + IoQueueFullSize + '},'
        '\"request\":{'
        '\"avg_size\":' + IORequestAVGSize + '}},'
        '\"name\":\"' + DiskName + '\",'
        '\"read\":{'
        '\"bytes\":' + DiskReadBytes + ','
        '\"count\":' + DiskReadCount + ','
        '\"time\":' + DiskReadTime + '},'
        '\"write\":{'
        '\"bytes\":' + DiskWriteBytes + ','
        '\"count\":' + DiskWriteCount + ','
        '\"time\":' + DiskWriteTime + '},'
        '\"io\":{\"time\":' + IoAwait + '}}}')
        
        # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
        JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
        
        # Adding JSON metrics values gathered from OS command
        JSONToSend = JSONToSend + ',' + JSONToSendValues
        
        # Gathering "ELK formatted" dynamic JSON values
        RetDynamicJSON = GenerateDynamicJson("system.diskio", "system", "diskio", ComparisonTimer)
        
        # Adding dynamic "ELK Formating" JSON values into final JSON message
        JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
        
        # Let's send JSON message to ELK servers
        SendToELK(JSONToSend)
        # print('\nJSONToSend\n', JSONToSend, '\n')
        pass

    pass

def SystemFilesystemAndFstat(ComparisonTimer):
    """ This function analyse FS activity and send metrics to ElasticSearch server

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """

    # Setting vars for threads follow up
    # global SystemFilesystemAndFstatThread
    # SystemFilesystemAndFstatThread = threading.currentThread()

    # Generating current ELK timestamp for JSON message
    SeqIdTimestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")

    # Gathering disk list from OS command
    SystemFilesystemAndFstat = subprocess.Popen('df -k | egrep -v "(Filesystem|:|/proc)"', shell=True, stdout=subprocess.PIPE).stdout
    SystemFilesystemAndFstat = SystemFilesystemAndFstat.read().decode().split('\n')

    # Initializing counters for Total FS Size message and FS loop
    FSTotalFiles = 0
    FSTotalSizeFree = 0
    FSTotalSizeUsed = 0
    FSTotalSize = 0
    FSTotalCount = 0

    # First, we send message for each FS metrics in SystemFilesystem
    for FSLine in SystemFilesystemAndFstat:
        # Removing empty lines
        if(len(FSLine) != 0):
            # Spitting the current line
            SplitFSLine = FSLine.split()

            # Filling all necessary values
            FSDeviceName = SplitFSLine[0]
            FSNameShort = FSDeviceName.replace('/','')
            FSAvailable = int(SplitFSLine[2]) * 1024
            FSUsedPCT = int(SplitFSLine[3].replace('%','')) / 100
            FSTotal = int(SplitFSLine[1]) * 1024
            FSUsed = FSTotal - FSAvailable
            FSMountPoint = SplitFSLine[6]
            InodeUsed = int(SplitFSLine[4])
            InodeUsedPCTShort = int(SplitFSLine[5].replace('%','')) / 100
            InodeFree = 100 * int(InodeUsed) / InodeUsedPCTShort
            InodeTotal = InodeFree + InodeUsed

            # Incrementing counters for Total FS Size message
            FSTotalFiles = FSTotalFiles + InodeUsed
            FSTotalSizeFree = FSTotalSizeFree + FSAvailable
            FSTotalSizeUsed = FSTotalSizeUsed + FSUsed
            FSTotalSize = FSTotalSize + FSTotal
            FSTotalCount = FSTotalCount + 1

            # Convert vars into strings
            FSDeviceName = str(FSDeviceName)
            FSMountPoint = str(FSMountPoint)
            FSAvailable = str(FSAvailable)
            FSUsedPCT = str(FSUsedPCT)
            FSTotal = str(FSTotal)
            FSUsed = str(FSUsed)
            InodeUsed = str(InodeUsed)
            InodeUsedPCTShort = str(InodeUsedPCTShort)
            InodeFree = str(InodeFree)
            InodeTotal = str(InodeTotal)
            FSNameShort = str(FSNameShort)

            # Let's construct JSON output for SystemFilesystem (ELK 7.5.0)
            JSONToSendValues = (''
            '\"system\": {\"filesystem\": {\"type\": \"jfs\",'
            '\"total\": ' + FSTotal + ','
            '\"mount_point\": \"' + FSMountPoint + '\",'
            '\"available\": ' + FSAvailable + ','
            '\"files\": ' + InodeUsed + ','
            '\"free_files\": ' + InodeFree + ','
            '\"device_name\": \"' + FSDeviceName + '\",'
            '\"free\": ' + FSAvailable + ','
            '\"used\": {\"pct\": ' + FSUsedPCT + ','
            '\"bytes\": ' + FSUsed + '}}}'
            '')
            
            # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
            JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
            
            # Adding JSON metrics values gathered from OS command
            JSONToSend = JSONToSend + ',' + JSONToSendValues
            
            # Gathering "ELK formatted" dynamic JSON values
            RetDynamicJSON = GenerateDynamicJson("system.filesystem", "system", "filesystem", ComparisonTimer)
            
            # Adding dynamic "ELK Formating" JSON values into final JSON message
            JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
            
            # Let's send JSON message to ELK servers
            SendToELK(JSONToSend)
            # print('\JSONToSend\n', JSONToSend, '\n')

    # Convert vars into strings
    FSTotalFiles = str(FSTotalFiles)
    FSTotalSizeFree = str(FSTotalSizeFree)
    FSTotalSizeUsed = str(FSTotalSizeUsed)
    FSTotalSize = str(FSTotalSize)
    FSTotalCount = str(FSTotalCount)

    # Let's construct JSON output for SystemFstat (ELK 7.5.0)
    JSONToSendValues = (''
    '\"system\":{\"fsstat\":{\"total_size\":{\"free\":' + FSTotalSizeFree + ','
    '\"used\":' + FSTotalSizeUsed + ','
    '\"total\":' + FSTotalSize + '},'
    '\"count\":' + FSTotalCount + ','
    '\"total_files\":' + FSTotalFiles + '}}')
    
    # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
    JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
    
    # Adding JSON metrics values gathered from OS command
    JSONToSend = JSONToSend + ',' + JSONToSendValues
    
    # Gathering "ELK formatted" dynamic JSON values
    RetDynamicJSON = GenerateDynamicJson("system.fsstat", "system", "fsstat", ComparisonTimer)
    
    # Adding dynamic "ELK Formating" JSON values into final JSON message
    JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
    
    # Let's send JSON message to ELK servers
    SendToELK(JSONToSend)
    # print('\JSONToSend\n', JSONToSend, '\n')
    pass

def SystemHPMStat(ComparisonTimer):
    """ This function analyse Hypervisor and LPAR activity with hpmstat command and send metrics to ElasticSearch server

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """
    # Intitalize thread for follow up
    global SystemHPMStatThread
    SystemHPMStatThread = threading.currentThread()
    
    # Generating current ELK timestamp for JSON message
    SeqIdTimestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")

    # Defining which values we want to take from output for General group
    # If some fields are added or remove, all the code has to be reviewed as positions will change...
    
    # Checking Power Hardware to get correct counters (all counters are not available on all architecture...)
    if LPARArch == "PowerPC_POWER8":
        # For Power 8
        HPMStatInclude = 'Cycles per instruction|Instructions per cycles|ITLB miss rate per inst|L2 Instruction Miss Rate|L3 Instruction Miss Rate'

        # Gathering data from OS command
        SystemHPMGrpGeneralCmd = 'hpmstat -m General:uk | egrep -w "' + HPMStatInclude + '"'
        
        SystemHPMGrpGeneral = subprocess.Popen(SystemHPMGrpGeneralCmd, shell=True, stdout=subprocess.PIPE).stdout
        SystemHPMGrpGeneral = SystemHPMGrpGeneral.read().decode().split('\n')
        
        CyclePerInst = SystemHPMGrpGeneral[0].split()[-1]
        InstPerCycle = SystemHPMGrpGeneral[1].split()[-1]
        L2MissRate = SystemHPMGrpGeneral[2].split()[-2]
        L3MissRate = SystemHPMGrpGeneral[3].split()[-2]
        ITLBMissRate = SystemHPMGrpGeneral[4].split()[-2]
    
        FinalString = '{\"cpugeneral\":{\"cycleperinst\":' + CyclePerInst + ',\"instpercycle\":' + InstPerCycle + ',\"l2missrate\":' + L2MissRate + ',\"l3missrate\":' + L3MissRate + ',\"itlbmissrate\":' + ITLBMissRate + '},'
    
    elif LPARArch == "PowerPC_POWER9":
        # For Power 9
        HPMStatInclude = 'Run cycles per run instruction|% ITLB miss rate per inst|L2 demand Load Miss Rate |L3 demand Load Miss Rate '
        
        # Gathering data from OS command
        SystemHPMGrpGeneralCmd = 'hpmstat -m General:uk | egrep -w "' + HPMStatInclude + '"'
        
        SystemHPMGrpGeneral = subprocess.Popen(SystemHPMGrpGeneralCmd, shell=True, stdout=subprocess.PIPE).stdout
        SystemHPMGrpGeneral = SystemHPMGrpGeneral.read().decode().split('\n')
        
        CyclePerInst = SystemHPMGrpGeneral[0].split()[-1]
        L2MissRate = SystemHPMGrpGeneral[1].split()[-2]
        L3MissRate = SystemHPMGrpGeneral[2].split()[-2]
        ITLBMissRate = SystemHPMGrpGeneral[3].split()[-2]
    
        FinalString = '{\"cpugeneral\":{\"cycleperinst\":' + CyclePerInst + ',\"l2missrate\":' + L2MissRate + ',\"l3missrate\":' + L3MissRate + ',\"itlbmissrate\":' + ITLBMissRate + '},'
        
    elif LPARArch == "PowerPC_POWER7":
        # For Power 7
        HPMStatInclude = 'Cycles per instruction|Instructions per cycles|L2 Instruction Miss Rate |L3 Instruction Miss Rate ' 
        
        # Gathering data from OS command
        SystemHPMGrpGeneralCmd = 'hpmstat -m General:uk | egrep -w "' + HPMStatInclude + '"'

        SystemHPMGrpGeneral = subprocess.Popen(SystemHPMGrpGeneralCmd, shell=True, stdout=subprocess.PIPE).stdout
        SystemHPMGrpGeneral = SystemHPMGrpGeneral.read().decode().split('\n')

        CyclePerInst = SystemHPMGrpGeneral[0].split()[-1]
        InstPerCycle = SystemHPMGrpGeneral[1].split()[-1]
        L2MissRate = SystemHPMGrpGeneral[2].split()[-2]
        L3MissRate = SystemHPMGrpGeneral[3].split()[-2]

        FinalString = '{\"cpugeneral\":{\"cycleperinst\":' + CyclePerInst + ',\"instpercycle\":' + InstPerCycle + ',\"l2missrate\":' + L2MissRate + ',\"l3missrate\":' + L3MissRate + '},'
    
    
    # No SMT or Cache reload counters exist for Power 9 (Need more investigation maybe...)
    
    # SMT and CPU Cache reload counters exists for Power8
    if LPARArch == "PowerPC_POWER8":
        # Defining which values we want to take from output for group 4
        # If some fields are added or remove, all the code has to be reviewed as positions will change...
        HPMStatInclude = "PM_RUN_CYC_SMT2_MODE|PM_RUN_CYC_SMT4_MODE|PM_RUN_CYC_SMT8_MODE"
        
        # Gathering data from OS command
        SystemHPMGrp4Cmd = "/usr/bin/hpmstat -g4:uk -r | egrep -w \"" + HPMStatInclude + "\""
        SystemHPMGrp4 = subprocess.Popen(SystemHPMGrp4Cmd, shell=True, stdout=subprocess.PIPE).stdout
        SystemHPMGrp4 = SystemHPMGrp4.read().decode().split('\n')
        
        Smt2Inst = SystemHPMGrp4[0].split()[-1]
        Smt4Inst = SystemHPMGrp4[1].split()[-1]
        Smt8Inst = SystemHPMGrp4[2].split()[-1]
        
        FinalString = str(FinalString) + '\"smtmode\":{\"smt2\":' + Smt2Inst + ',\"smt4\":' + Smt4Inst + ',\"smt8\":' + Smt8Inst + ','
        
        # Defining which values we want to take from output for group 3
        # If some fields are added or remove, all the code has to be reviewed as positions will change...
        HPMStatInclude = "PM_RUN_CYC_ST_MODE"
        
        # Gathering data from OS command
        SystemHPMGrp3Cmd = "/usr/bin/hpmstat -g3:uk -r | egrep -w \"" + HPMStatInclude + "\""
        SystemHPMGrp3 = subprocess.Popen(SystemHPMGrp3Cmd, shell=True, stdout=subprocess.PIPE).stdout
        SystemHPMGrp3 = SystemHPMGrp3.read().decode().split('\n')
        
        Smt1Inst = SystemHPMGrp3[0].split()[-1]
        
        FinalString = str(FinalString) + '\"smt1\":' + Smt1Inst + '},'
        
        # Defining which values we want to take from output for group 53
        # If some fields are added or remove, all the code has to be reviewed as positions will change...
        HPMStatInclude = "PM_DATA_FROM_L2MISS_MOD|PM_DATA_FROM_LMEM|PM_DATA_FROM_RMEM|PM_DATA_FROM_DMEM"
        
        # Gathering data from OS command
        SystemHPMGrp53Cmd = "/usr/bin/hpmstat -g53:uk -r | egrep -w \"" + HPMStatInclude + "\""
        SystemHPMGrp53 = subprocess.Popen(SystemHPMGrp53Cmd, shell=True, stdout=subprocess.PIPE).stdout
        SystemHPMGrp53 = SystemHPMGrp53.read().decode().split('\n')
        
        L2cachemiss = SystemHPMGrp53[0].split()[-1]
        LocalCacheReload = SystemHPMGrp53[1].split()[-1]
        RemoteCacheReload = SystemHPMGrp53[2].split()[-1]
        DistantCacheReload = SystemHPMGrp53[3].split()[-1]
        
        FinalString = str(FinalString) + '\"cpucachereload\":{\"l2cachemiss\":' + L2cachemiss + ',\"local\":' + LocalCacheReload + ',\"remote\":' + RemoteCacheReload + ',\"distant\":' + DistantCacheReload + '},'
    
    # Cache reload counters only exists for Power7 in differrent and separate group id
    if LPARArch == "PowerPC_POWER7":
        # Defining which values we want to take from output for group 53
        # If some fields are added or remove, all the code has to be reviewed as positions will change...
        HPMStatInclude = "PM_DATA_FROM_LMEM|PM_DATA_FROM_RMEM|PM_DATA_FROM_DMEM"
        
        # Gathering data from OS command
        SystemHPMGrp53Cmd = "/usr/bin/hpmstat -g97,99:uk -r | egrep -w \"" + HPMStatInclude + "\""
        SystemHPMGrp53 = subprocess.Popen(SystemHPMGrp53Cmd, shell=True, stdout=subprocess.PIPE).stdout
        SystemHPMGrp53 = SystemHPMGrp53.read().decode().split('\n')
        
        LocalCacheReload = SystemHPMGrp53[1].split()[-1]
        RemoteCacheReload = SystemHPMGrp53[2].split()[-1]
        DistantCacheReload = SystemHPMGrp53[0].split()[-1]
        
        FinalString = str(FinalString) + '\"local\":' + LocalCacheReload + ',\"remote\":' + RemoteCacheReload + ',\"distant\":' + DistantCacheReload + '},'
        
    # Gathering data from OS command
    SystemHPMMpStat = subprocess.Popen("mpstat -d 1 1 | grep ALL", shell=True, stdout=subprocess.PIPE).stdout
    SystemHPMMpStat = SystemHPMMpStat.read().decode().split()
    
    S0rd = str(float(SystemHPMMpStat[8]) / 100)
    S1rd = str(float(SystemHPMMpStat[9]) / 100)
    S2rd = str(float(SystemHPMMpStat[10]) / 100)
    S3rd = str(float(SystemHPMMpStat[11]) / 100)
    S4rd = str(float(SystemHPMMpStat[12]) / 100)
    S5rd = str(float(SystemHPMMpStat[13]) / 100)
    S3hrd = str(float(SystemHPMMpStat[16]) / 100)
    S4hrd = str(float(SystemHPMMpStat[17]) / 100)
    S5hrd = str(float(SystemHPMMpStat[18]) / 100)

    FinalString = str(FinalString) + '\"cpuaffinity\":{\"S0rd\":' + S0rd + ',\"S1rd\":' + S1rd + ',\"S2rd\":' + S2rd + ',\"S3rd\":' + S3rd + ',\"S4rd\":' + S4rd + ',\"S5rd\":' + S5rd + ',\"S3hrd\":' + S3hrd + ',\"S4hrd\":' + S4hrd + ',\"S5hrd\":' + S5hrd + '}}'
      
    # Let's construct JSON output for SystemHPMStat (ELK 7.5.0)
    JSONToSendValues = (''
    '\"system\":{\"hpmstat\":' + FinalString + '}')
    
    # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
    JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
    
    # Adding JSON metrics values gathered from OS command
    JSONToSend = JSONToSend + ',' + JSONToSendValues
    
    # Gathering "ELK formatted" dynamic JSON values
    RetDynamicJSON = GenerateDynamicJson("system.hpmstat", "system", "hpmstat", ComparisonTimer)
    
    # Adding dynamic "ELK Formating" JSON values into final JSON message
    JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
    
    # Let's send JSON message to ELK servers
    SendToELK(JSONToSend)
    
    # print('\JSONToSend\n', JSONToSend, '\n')
    pass

def SystemHypervisor(ComparisonTimer):
    """ This function analyse LPAR activity with lparstat command and send metrics to ElasticSearch server

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """

    # Generating current ELK timestamp for JSON message
    SeqIdTimestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")
    
    # Defining which values we want to take from output
    # If some fields are added or remove, all the code has to be reviewed as positions will change...
    PHYPInclude = "cede|confer|prod|read|enter|remove|bulk_remove|get_ppp|set_ppp|clear_ref|eoi|ipi|cppr|migrate_dma|send_logical_lan|add_logicl_lan_buf|xirr|PURR|others|page_init"
    
    # Gathering data from OS command
    SystemHypervisorCmd = "lparstat -H 1 1 | egrep -w \"" + PHYPInclude + "\""
    SystemHypervisor = subprocess.Popen(SystemHypervisorCmd, shell=True, stdout=subprocess.PIPE).stdout
    SystemHypervisor = SystemHypervisor.read().decode().split('\n')
    
    # Defining the Final String containing all JSON fieslds and values
    FinalStringConcat = ""
    
    # Looping on each line of the OS command output
    for lineID in range(len(SystemHypervisor)):
        # Discarding empty line
        if len(SystemHypervisor[lineID]) != 0:
            FinalString = ""
            # Splitting the current line in list
            SystemHypervisorSplitted = SystemHypervisor[lineID].split()
            
            # Filling vars with values
            SystemHypervisorName = SystemHypervisorSplitted[0]
            SystemHypervisorNbcall = str(SystemHypervisorSplitted[1])
            SystemHypervisorAVGCalltime = str(SystemHypervisorSplitted[4])
            SystemHypervisorMAXCalltime = str(SystemHypervisorSplitted[5])
            # Making some calculation for percentages
            SystemHypervisorLpartimePCT = str(round((float(SystemHypervisorSplitted[2]) / 100), 2))
            SystemHypervisorHypervisortimePCT = str(round((float(SystemHypervisorSplitted[3]) / 100), 2))
            
            # Generating JSON for the current line
            FinalString = '\"' + SystemHypervisorName + '\":{\"nbcall\":' + SystemHypervisorNbcall + ',\"lpartimepct\":' + SystemHypervisorLpartimePCT + ',\"phyptimepct\":' + SystemHypervisorHypervisortimePCT + ',\"avgcalltime\":' + SystemHypervisorAVGCalltime + ',\"maxcalltime\":' + SystemHypervisorMAXCalltime + '}'

            # Adding the JSON line to Final String
            FinalStringConcat = FinalStringConcat + ',' +  FinalString
    
    # Removeing the first caracter of FinalStringConcat which is a ","
    FinalStringConcat = FinalStringConcat[1:]
    
    # Let's construct JSON output for SystemHypervisor (ELK 7.5.0)
    JSONToSendValues = (''
    '\"system\":{\"hypervisor\":{' + FinalStringConcat + '}}')
    
    # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
    JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
    
    # Adding JSON metrics values gathered from OS command
    JSONToSend = JSONToSend + ',' + JSONToSendValues
    
    # Gathering "ELK formatted" dynamic JSON values
    RetDynamicJSON = GenerateDynamicJson("system.hypervisor", "system", "hypervisor", ComparisonTimer)
    
    # Adding dynamic "ELK Formating" JSON values into final JSON message
    JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
    
    # Let's send JSON message to ELK servers
    SendToELK(JSONToSend)
    # print('\JSONToSend\n', JSONToSend, '\n')
    pass

def SystemProcess(ComparisonTimer):
    """ This function analyse processes CPU and RAM activity and send metrics to ElasticSearch server

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """
    
    # Setting vars for threads follow up
    global SystemProcessThread
    SystemProcessThread = threading.currentThread()

    # Generating current ELK timestamp for JSON message
    SeqIdTimestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")

    # Checking if a limit is set for Processes analysis
    if TopProcesses != 0:
        # Get Processs CPU usage for a limited number of processes
        # The maximum number of processes in defined in TopProcesses var
        SystemProcessCmd = "ps -ef -o pid,ppid,pgid,command,status,vsz,rssize,pmem,uname,time,pcpu,start,args | sed '1 d' | sort -rk 11 | head -" + str(TopProcesses)
        SystemProcess = subprocess.Popen(SystemProcessCmd, shell=True, stdout=subprocess.PIPE).stdout
    else:
        # Get CPU usage values from ps command for all processes -- current version
        SystemProcess = subprocess.Popen("ps -ef -o pid,ppid,pgid,command,status,vsz,rssize,pmem,uname,time,pcpu,start,args | sed '1 d'", shell=True, stdout=subprocess.PIPE).stdout

    # Split result into list
    SystemProcess = SystemProcess.read().decode().split('\n')

    for CPUProcessLine in SystemProcess:
        # Removing empty lines
        if(len(CPUProcessLine) != 0):
            CPUProcessLine = CPUProcessLine.split()
            ProcessPid = CPUProcessLine[0]
            ProcessPpid = CPUProcessLine[1]
            ProcessPgid = CPUProcessLine[2]
            ProcessName = CPUProcessLine[3]
            ProcessState = CPUProcessLine[4]

            # Handling defunct/exiting/idle process cases
            ExcludedProcesses = ['<defunct>', '<exiting>', '<idle>']
            if any(ExcludedProcess in ProcessName for ExcludedProcess in ExcludedProcesses):
                # If it is a defunct process
                ProcessMemorySize = 0
                ProcessMemoryRssBytes = 0
                ProcessMemoryRssPCT = 0
                ProcessUsername = 0
                ProcessCpuTotalValue = 0
                ProcessCpuTotalPCTNorm = 0
                ProcessCpuTotalPCT = 0
                ProcessFDPath = 0
                ProcessFdOpen = 0
                ProcessCpuStartTime = 0
                ProcessCmdline = 0
                ProcessMemoryShare = 0
                ProcessFdOpenlimitSoft = 0
                ProcessFdOpenlimitHard = 0
                ProcessCwd = 0
                ProcessExecutable = 0
                FinalCmdString = str(0)
                ProcessPpid = CPUProcessLine = 0
                ProcessPgid = CPUProcessLine = 0

            else:
                # For other processes
                ProcessMemorySize = CPUProcessLine[5]
                # Converting KB in B
                ProcessMemorySize = ProcessMemorySize + '000'
                ProcessMemoryRssBytes = CPUProcessLine[6]
                # Converting KB in B
                ProcessMemoryRssBytes = ProcessMemoryRssBytes + '000'
                ProcessMemoryRssPCT = CPUProcessLine[7]
                # Formating pct to ELK format
                ProcessMemoryRssPCT = float(ProcessMemoryRssPCT) / 100
                ProcessUsername = CPUProcessLine[8]
                ProcessCpuTotalValue = CPUProcessLine[9]
                ProcessCpuTotalPCTNorm = CPUProcessLine[10]

                # Converting CPU total value HH:MM:SS in seconds
                # Detecting if value contains a number of days as a prefix
                if len(ProcessCpuTotalValue.split('-')) == 2:
                    # If yes, making a custom formating and converting values
                    ProcessCpuDays = ProcessCpuTotalValue.split('-')[0]
                    ProcessCpuDaysInSec = (int(ProcessCpuDays) * 24) * 3600
                    ProcessCpuDateTime = ProcessCpuTotalValue.split('-')[1]
                    ProcessCpuDateTime = ProcessCpuDateTime.split(':')
                    ProcessCpuTotalValue = ProcessCpuDaysInSec + (int(ProcessCpuDateTime[0]) * 3600) + (int(ProcessCpuDateTime[1]) * 60) + int(ProcessCpuDateTime[2])
                else:
                    ProcessCpuTotalValueSplit = ProcessCpuTotalValue.split(':')
                    ProcessCpuTotalValue = (int(ProcessCpuTotalValueSplit[0]) * 3600) + (int(ProcessCpuTotalValueSplit[1]) * 60) + int(ProcessCpuTotalValueSplit[2])

                # Calculating values for CPUTotal metrics
                if ProcessCpuTotalPCTNorm != "0,0":
                    # Changing . in , for calculation and converting to ELK PCT format
                    ProcessCpuTotalPCTNorm = float(ProcessCpuTotalPCTNorm.replace(',', '.')) / 100
                    ProcessCpuTotalPCT = ProcessCpuTotalPCTNorm * int(NumProc)
                else:
                    ProcessCpuTotalPCTNorm = 0
                    ProcessCpuTotalPCT = 0

                # Detecting and gathering file opened metric with try/catch
                try:
                    ProcessFDPath = '/proc/' + ProcessPid + '/fd'
                    if os.path.exists(ProcessFDPath):
                        path, dirs, files = next(os.walk(ProcessFDPath))
                        ProcessFdOpen = len(files) + len(dirs)
                    else:
                        ProcessFdOpen=0
                except:
                    ProcessFdOpen=0

                # Calculating ProcessCpuStartTime depending on its format
                ProcessCpuStartTime = CPUProcessLine[11]
                ProcessCmdline = ''
                if ':' in ProcessCpuStartTime:
                    # Formating output to ELK timestamp
                    ProcessCmdDate = datetime.datetime.now().strftime("%Y-%m-%dT")
                    ProcessCpuStartTime = str(ProcessCmdDate) + ProcessCpuStartTime

                    # Filling the CmdLine metric with the rest of the columns, starting at 12
                    for i in range(12, len(CPUProcessLine)):
                        # Removing invalid caracters from JSON message
                        ProcessCmdline = ProcessCmdline + " " + re.sub('[\\\[\]\(\)\{\}"\b\t\n\a\r]', ' ', str(CPUProcessLine[i]))
                else:
                    month = ProcessCpuStartTime
                    day = CPUProcessLine[12]
                    year = str(datetime.datetime.now().strftime("%Y"))

                    # Making switch case for formating month
                    if "Jan" in month:
                        mon = '01'
                    elif "Feb" in month:
                        mon = '02'
                    elif "Mar" in month:
                        mon = '03'
                    elif "Apr" in month:
                        mon = '04'
                    elif "May" in month:
                        mon = '05'
                    elif "Jun" in month:
                        mon = '06'
                    elif "Jul" in month:
                        mon = '07'
                    elif "Aug" in month:
                        mon = '08'
                    elif "Sep" in month:
                        mon = '09'
                    elif "Oct" in month:
                        mon = '10'
                    elif "Nov" in month:
                        mon = '11'
                    elif "Dec" in month:
                        mon = '12'

                    # Formating output to ELK timestamp
                    ProcessCpuStartTime = year + '-' + mon + '-' + day + 'T00:00:00.000'

                    # Filling the CmdLine metric with the rest of the columns, starting at 13
                    for i in range(13, len(CPUProcessLine)):
                        # Removing invalid caracters from JSON message
                        ProcessCmdline = ProcessCmdline + " " + re.sub('[\\\[\]\(\)\{\}"\b\t\n\a\r]', ' ', str(CPUProcessLine[i]))

                # COST -- Too much CPU consumer.... Need IPCS or svmon information grouping functions...
                ProcessMemoryShare=0

                # COST -- The following is very costly, need to know if really necessary
                # We can change and force the limit to 0 anyway, saving CPU
                # If problem on that, we should see saturation on graphs...
                # ProcessFdOpenlimitSoft=0
                # ProcessFdOpenlimitHard=0

                # Get soft ulimit values from OS command
                ProcessFdOpenlimitSoftCmd = "su " + ProcessUsername + " -c ulimit -Sa | egrep -w 'nofiles|open files' | awk '{print $NF}'\n"
                ProcessFdOpenlimitSoft = subprocess.Popen(ProcessFdOpenlimitSoftCmd, shell=True, stdout=subprocess.PIPE).stdout
                ProcessFdOpenlimitSoft = ProcessFdOpenlimitSoft.read().decode().split('\n')
                ProcessFdOpenlimitSoft = ProcessFdOpenlimitSoft[0]

                # Setting limit to zero if unlimited
                if (ProcessFdOpenlimitSoft == 'unlimited') or (ProcessFdOpenlimitSoft == '-1'):
                    # We set limit to 0 as unlimited
                    ProcessFdOpenlimitSoft = 0

                # Get hard ulimit values from OS command
                ProcessFdOpenlimitHardCmd = "su " + ProcessUsername + " -c ulimit -Ha  | egrep -w 'nofiles|open files' | awk '{print $NF}'\n"
                ProcessFdOpenlimitHard = subprocess.Popen(ProcessFdOpenlimitHardCmd, shell=True, stdout=subprocess.PIPE).stdout
                ProcessFdOpenlimitHard = ProcessFdOpenlimitHard.read().decode().split('\n')
                ProcessFdOpenlimitHard = ProcessFdOpenlimitHard[0]

                # Checking if hard limits are set
                if ProcessFdOpenlimitHard == "":
                    ProcessFdOpenlimitHard = ProcessFdOpenlimitSoft
                else:
                    # Setting limit to zero if unlimited
                    if ProcessFdOpenlimitHard == 'unlimited':
                        # We set limit to 0 as unlimited
                        ProcessFdOpenlimitHard = 0

                # Get CWD value from OS command
                ProcessCwdCmd = "lsof -p " + ProcessPid + " | grep cwd | awk '{print $9}'"
                ProcessCwd = subprocess.Popen(ProcessCwdCmd, shell=True, stdout=subprocess.PIPE, stderr = devnull).stdout
                # , stdout = subprocess.PIPE, stderr = subprocess.PIPE
                ProcessCwd = ProcessCwd.read().decode().split('\n')
                ProcessCwd = ProcessCwd[0]

                # Generate args value into a JSON tab format
                FinalCmdString = '[\"' + ProcessCmdline.replace(' ','\",\"') + '\"]'
                # Then removing first empty entrie
                FinalCmdString = FinalCmdString.replace('"",','')

                # Gathering process executable name
                ProcessExecutableSplited = ProcessCmdline.split('/')
                ProcessExecutableSplitedLen = len(ProcessExecutableSplited) - 1
                ProcessExecutable = ProcessExecutableSplited[ProcessExecutableSplitedLen]

            # Convert vars into strings
            CPUProcessLine = str(CPUProcessLine)
            ProcessPid = str(ProcessPid)
            ProcessPpid = str(ProcessPpid)
            ProcessPgid = str(ProcessPgid)
            ProcessName = str(ProcessName)
            ProcessState = str(ProcessState)
            ProcessMemorySize = str(ProcessMemorySize)
            ProcessMemoryRssBytes = str(ProcessMemoryRssBytes)
            ProcessMemoryRssPCT = str(ProcessMemoryRssPCT)
            ProcessUsername = str(ProcessUsername)
            ProcessCpuTotalValue = str(ProcessCpuTotalValue)
            ProcessCpuTotalPCTNorm = str(ProcessCpuTotalPCTNorm)
            ProcessCpuTotalPCT = str(ProcessCpuTotalPCT)
            ProcessFDPath = str(ProcessFDPath)
            ProcessFdOpen = str(ProcessFdOpen)
            ProcessCpuStartTime = str(ProcessCpuStartTime)
            ProcessCmdline = str(ProcessCmdline)
            ProcessMemoryShare = str(ProcessMemoryShare)
            ProcessFdOpenlimitSoft = str(ProcessFdOpenlimitSoft)
            ProcessFdOpenlimitHard = str(ProcessFdOpenlimitHard)
            ProcessCwd = str(ProcessCwd)
            ProcessExecutable = str(ProcessExecutable).replace('"', '\"')
            
            # Let's construct JSON output for SystemProcess (ELK 7.5.0)
            JSONToSendValues = (''
            '\"system\": {'
            '\"process\": {'
            '\"memory\": {'
            '\"rss\": {'
            '\"pct\": ' + ProcessMemoryRssPCT + ','
            '\"bytes\": ' + ProcessMemoryRssBytes + '},'
            '\"share\": ' + ProcessMemoryShare + ','
            '\"size\": ' + ProcessMemorySize + '},'
            '\"cmdline\": \"' + ProcessCmdline + '\",'
            '\"cpu\": {'
            '\"total\": {'
            '\"value\": ' + ProcessCpuTotalValue + ','
            '\"pct\": ' + ProcessCpuTotalPCT + ','
            '\"norm\": {'
            '\"pct\": ' + ProcessCpuTotalPCTNorm + '}},'
            '\"start_time\": \"' + ProcessCpuStartTime + '\"},'
            '\"fd\": {'
            '\"open\": ' + ProcessFdOpen + ','
            '\"limit\": {'
            '\"soft\": ' + ProcessFdOpenlimitSoft + ','
            '\"hard\": ' + ProcessFdOpenlimitHard + '}},'
            '\"state\": \"' + ProcessState + '\"}},'
            '\"process\": {'
            '\"args\": ' + FinalCmdString + ','
            '\"name\": \"' + ProcessExecutable + '\",'
            '\"pid\": ' + ProcessPid + ','
            '\"ppid\": ' + ProcessPpid + ','
            '\"pgid\": ' + ProcessPgid + ','
            '\"working_directory\": \"' + ProcessCwd + '\",'
            '\"executable\": \"' + ProcessName + '\"},'
            '\"user\": {'
            '\"name\": \"' + ProcessUsername + '\"}')
           
           
            # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
            JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
            
            # Adding JSON metrics values gathered from OS command
            JSONToSend = JSONToSend + ',' + JSONToSendValues
            
            # Gathering "ELK formatted" dynamic JSON values
            RetDynamicJSON = GenerateDynamicJson("system.process", "system", "process", ComparisonTimer)
            
            # Adding dynamic "ELK Formating" JSON values into final JSON message
            JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
            
            # Let's send JSON message to ELK servers
            SendToELK(JSONToSend)
            # print('\JSONToSend\n', JSONToSend, '\n')
            pass

def SystemSocketSummary(ComparisonTimer):
    """ This function analyse processes network connections activity and send metrics to ElasticSearch server

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """
            
    # Generating current ELK timestamp for JSON message
    SeqIdTimestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")

    # Gathering data from OS command for SystemSocketSummarySummary
    SystemSocketSummary = subprocess.Popen("netstat -an -f inet | tail -n +3", shell=True, stdout=subprocess.PIPE).stdout
    SystemSocketSummary = SystemSocketSummary.read().decode().split('\n')
    
    # Initializing counters
    TotalConnections = 0
    CountTCP = 0
    CountUDP = 0
    CountSYNRECEIVED = 0
    CountSYNSEND = 0
    CountESTABLISHED = 0
    CountLISTEN = 0
    CountFIN_WAIT_1 = 0
    CountTIMED_WAIT = 0
    CountCLOSE_WAIT = 0
    CountFIN_WAIT_2 = 0
    CountLAST_ACK = 0
    CountCLOSED = 0
    
    for ConnectId in range(len(SystemSocketSummary)):
        # Removing last empty line
        if SystemSocketSummary[ConnectId] != "":
            # Incrementing total counter
            TotalConnections = TotalConnections + 1
            
            # Splitting connection line
            SplitedConnection = SystemSocketSummary[ConnectId].split()

            # Checking connection protocol
            if ('tcp' or 'tcp4' or 'tcp6') in SplitedConnection[0]:
                # Incrementing TCP connection counter
                CountTCP = CountTCP + 1
                # Checking connection state for TCP
                if 'SYN_RECEIVED' in SplitedConnection[5]:
                    # Incrementing SYN_RECEIVED counter
                    # Server just received SYN from the client
                    CountSYNRECEIVED = CountSYNRECEIVED + 1
                elif 'ESTABLISHED' in SplitedConnection[5]:
                    # Incrementing ESTABLISHED counter
                    # Client received server's SYN and session is established
                    CountESTABLISHED = CountESTABLISHED + 1
                elif 'SYN_SEND' in SplitedConnection[5]:
                    # Incrementing SYN_SEND counter
                    # Indicates active open
                    CountSYNSEND = CountSYNSEND + 1
                elif 'LISTEN' in SplitedConnection[5]:
                    # Incrementing LISTEN counter
                    # Server is ready to accept connection
                    CountLISTEN = CountLISTEN + 1
                elif 'FIN_WAIT_1' in SplitedConnection[5]:
                    # Incrementing FIN_WAIT_1 counter
                    # Indicates active close
                    CountFIN_WAIT_1 = CountFIN_WAIT_1 + 1
                elif 'TIMED_WAIT' in SplitedConnection[5]:
                    # Incrementing TIMED_WAIT counter
                    # Client enters this state after active close
                    CountTIMED_WAIT = CountTIMED_WAIT + 1
                elif 'CLOSE_WAIT' in SplitedConnection[5]:
                    # Incrementing CLOSE_WAIT counter
                    # Indicates passive close. Server just received first FIN from a client
                    CountCLOSE_WAIT = CountCLOSE_WAIT + 1
                elif 'FIN_WAIT_2' in SplitedConnection[5]:
                    # Incrementing FIN_WAIT_2 counter
                    # Client just received acknowledgment of its first FIN from the server
                    CountFIN_WAIT_2 = CountFIN_WAIT_2 + 1
                elif 'LAST_ACK' in SplitedConnection[5]:
                    # Incrementing LAST_ACK counter
                    # Server is in this state when it sends its own FIN
                    CountLAST_ACK = CountLAST_ACK + 1
                elif 'CLOSED' in SplitedConnection[5]:
                    # Incrementing CLOSED counter
                    # Server received ACK from client and connection is closed
                    CountCLOSED = CountCLOSED + 1
       
            elif ('udp' or 'udp4' or 'udp6') in SplitedConnection[0]:
                # Incrementing UDP connection counter
                CountUDP = CountUDP + 1

    # Converting to string
    TotalConnections = str(TotalConnections)
    CountTCP = str(CountTCP)
    CountUDP = str(CountUDP)
    CountSYNRECEIVED = str(CountSYNRECEIVED)
    CountSYNSEND = str(CountSYNSEND)
    CountESTABLISHED = str(CountESTABLISHED)
    CountLISTEN = str(CountLISTEN)
    CountFIN_WAIT_1 = str(CountFIN_WAIT_1)
    CountTIMED_WAIT = str(CountTIMED_WAIT)
    CountCLOSE_WAIT = str(CountCLOSE_WAIT)
    CountFIN_WAIT_2 = str(CountFIN_WAIT_2)
    CountLAST_ACK = str(CountLAST_ACK)
    CountCLOSED = str(CountCLOSED)
    
    # Let's construct JSON output for SystemSocketSummary (ELK 7.5.0)
    JSONToSendValues = (''
    '\"system\":{'
    '\"socket\":{'
    '\"summary\":{'
    '\"all\":{'
    '\"count\":' + TotalConnections + ','
    '\"listening\":' + CountLISTEN + '},'
    '\"tcp\":{'
    '\"memory\":0,'
    '\"all\":{'
    '\"listening\":' + CountLISTEN + ','
    '\"established\":' + CountESTABLISHED + ','
    '\"close_wait\":' + CountCLOSE_WAIT + ','
    '\"time_wait\":' + CountTIMED_WAIT + ','
    '\"fin_wait1\":' + CountFIN_WAIT_1 + ','
    '\"fin_wait2\":' + CountFIN_WAIT_2 + ','
    '\"closed\":' + CountCLOSED + ','
    '\"syn_send\":' + CountSYNSEND + ','
    '\"last_ack\":' + CountLAST_ACK + ','
    '\"syn_received\":' + CountSYNRECEIVED + ','
    '\"orphan\":0,'
    '\"count\":' + CountTCP + '}},'
    '\"udp\":{'
    '\"all\":{'
    '\"count\":' + CountUDP + '},'
    '\"memory\":0}}'
    '}}')
    
    # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
    JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
    
    # Adding JSON metrics values gathered from OS command
    JSONToSend = JSONToSend + ',' + JSONToSendValues
    
    # Gathering "ELK formatted" dynamic JSON values
    RetDynamicJSON = GenerateDynamicJson("system.socket.summary", "system", "socket_summary", ComparisonTimer)
    
    # Adding dynamic "ELK Formating" JSON values into final JSON message
    JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
    
    # Let's send JSON message to ELK servers
    SendToELK(JSONToSend)
    # print('\JSONToSend\n', JSONToSend, '\n')
    pass

def SystemSocket(ComparisonTimer):
    """ This function analyse  network connections activity and send metrics to ElasticSearch server
            
        WARNING: For now, this is not enabled (very big timer). It is working but requires too much CPU. To improve !!!
        
        TODO: Describe the function, all vars and its content
    """
    # Setting vars for threads follow up
    global SystemSocketThread
    SystemSocketThread = threading.currentThread()
            
    # Generating current ELK timestamp for JSON message
    SeqIdTimestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%f")

    # Gathering data from OS command for SystemSocketSummarySummary
    SystemSocketSummary = subprocess.Popen("netstat -an -f inet | tail -n +3", shell=True, stdout=subprocess.PIPE).stdout
    SystemSocketSummary = SystemSocketSummary.read().decode().split('\n')
    
    # Lopping into all cmd Results
    for ConnectId in range(len(SystemSocketSummary)):
        # Removing last empty line
        if SystemSocketSummary[ConnectId] != "":
            # Splitting connection line
            SplitedConnection = SystemSocketSummary[ConnectId].split()

            # Checking connection protocol
            if ('tcp' or 'tcp4' or 'tcp6') in SplitedConnection[0]:
                    
                # Sending JSON message for current connection for ESTABLISHED or LISTEN connections only
                TargetStates = ['ESTABLISHED', 'LISTEN']
                if any(x in SplitedConnection[-1] for x in TargetStates):
                
                    # Detecting wich type of TCP connection it is IPV4 or IPV6
                    if ('tcp' or 'tcp4') in SplitedConnection[0]:
                        ConnectionIPVersion = "ipv4"
                        ConnectionIPVersionId = ConnectionIPVersion[-1]
                    else:
                        ConnectionIPVersion = "ipv6"
                        ConnectionIPVersionId = ConnectionIPVersion[-1]
                    
                    # Defining Local and Remote ports
                    LocalPort = SplitedConnection[3].split('.')[-1]
                    RemotePort = SplitedConnection[4].split('.')[-1]
                    
                    # Checking if local tcp port is linked with an AIX service
                    LocalPortTranslate = ""
                    try:
                        PortTranslateCmd = 'cat /etc/services | grep -w ' + LocalPort + '/tcp'
                        LocalPortTranslate = subprocess.Popen(PortTranslateCmd, shell=True, stdout=subprocess.PIPE).stdout
                        LocalPortTranslate = LocalPortTranslate.read().decode().split()[0]
                    except:
                        # Nothing to do
                        pass
 
                    # Checking if remote tcp port is linked with an AIX service
                    RemotePortTranslate = ""
                    try:
                        PortTranslateCmd = 'cat /etc/services | grep -w ' + RemotePort + '/tcp'
                        RemotePortTranslate = subprocess.Popen(PortTranslateCmd, shell=True, stdout=subprocess.PIPE).stdout
                        RemotePortTranslate = RemotePortTranslate.read().decode().split()[0]
                    except:
                        # Nothing to do
                        pass
                    
                    # Gathering local address and split it. Then, we force definition if port is listening
                    LocalAddrSplit = SplitedConnection[3].split('.')
                    if len(LocalAddrSplit) != 2:
                        LocalAddr = LocalAddrSplit[0] + '.' + LocalAddrSplit[1] + '.' + LocalAddrSplit[2] + '.' + LocalAddrSplit[3]
                    else:
                        LocalAddr = "0.0.0.0"
                    
                    # Gathering remote address and split it. Then, we force definition if port is listening
                    RemoteAddrSplit = SplitedConnection[4].split('.')
                    if len(RemoteAddrSplit) != 2:
                        RemoteAddr = RemoteAddrSplit[0] + '.' + RemoteAddrSplit[1] + '.' + RemoteAddrSplit[2] + '.' + RemoteAddrSplit[3]
                    else:
                        RemoteAddr = "0.0.0.0"
                        RemotePortTranslate = "LISTEN"
                    
                    # Checking if there is some match between AIX service ports and current TCP port
                    if RemotePortTranslate and (isinstance(RemotePortTranslate, str)):
                        if LocalPortTranslate and (isinstance(LocalPortTranslate, str)):
                            # local and remote TCP ports are matching an AIX service. Taking that name for LSOF command
                            LsofCmd = 'lsof -i :' + LocalPortTranslate + ' | grep ' + RemotePortTranslate + '| tail -1'
                        else:
                            # Remote TCP ports is matching an AIX service. Taking that name for LSOF command
                            LsofCmd = 'lsof -i :' + LocalPort + ' | grep ' + RemotePortTranslate + '| tail -1'
                    else:
                        if LocalPortTranslate and (isinstance(LocalPortTranslate, str)):
                            # Local TCP ports is matching an AIX service. Taking that name for LSOF command
                            LsofCmd = 'lsof -i :' + LocalPortTranslate + ' | grep ' + RemotePort + '| tail -1'
                        else:
                            # local and remote TCP ports are not matching an AIX service. 
                            LsofCmd = 'lsof -i :' + LocalPort + ' | grep ' + RemotePort + '| tail -1'
                    
                    # Getting process information linked to the current connection
                    LSOFResult = subprocess.Popen(LsofCmd, shell=True, stdout=subprocess.PIPE, stderr = devnull).stdout
                    LSOFResult = LSOFResult.read().decode().split()
                    
                    # Filling vars with LSOF results
                    if LSOFResult:
                        ConnectionProcess = LSOFResult[0]
                        ConnectionProcessPID = LSOFResult[1]
                        ConnectionProcessuser = LSOFResult[2]
                    else:
                        # Defaulting values with "none" string
                        ConnectionProcess = "none"
                        ConnectionProcessPID = "0"
                        ConnectionProcessuser = "none"
                        ConnectionProcessArgsJoined = ""
                        
                    # Get CPU args value from ps command for current process if PID defined
                    if ConnectionProcessPID != "none":
                        CheckPSCommandCmd = "ps -ef -o pid,args | grep -w " + ConnectionProcessPID + " | grep -v grep"
                        CheckPSCommand = subprocess.Popen(CheckPSCommandCmd, shell=True, stdout=subprocess.PIPE).stdout
                        
                        # Split result into list
                        ConnectionProcessArgs = CheckPSCommand.read().decode().split()
                        
                        # Removing first line because it is the pid
                        del ConnectionProcessArgs[0]
                        
                        # Removing invalid ELK JSON caracters
                        for i in range(0, len(ConnectionProcessArgs)):
                            ConnectionProcessArgs[i] = re.sub('[\\\[\]\(\)\{\}"\b\t\n\a\r]', ' ', str(ConnectionProcessArgs[i]))
                        
                        # Joining list to make the final arg string
                        StrSeparator = " "
                        ConnectionProcessArgsJoined = StrSeparator.join(ConnectionProcessArgs)
                        
                        # Then, transforming this string to JSON tab format
                        FinalCmdString = '[\"' + ConnectionProcessArgsJoined.replace(' ','\",\"') + '\"]'
                        # Then removing first empty entrie
                        FinalCmdString = FinalCmdString.replace('"",','')
                        
                    # Defining username id if not "none"
                    # Tryin that into try/catch if grep is not giving any results
                    try:
                        if ConnectionProcessuser != "none":
                            # Executing OS command
                            CheckUserIdCmd = 'lsuser -a id ' + ConnectionProcessuser
                            CheckPSUserId = subprocess.Popen(CheckUserIdCmd, shell=True, stdout=subprocess.PIPE).stdout
                            CheckPSUserId = CheckPSUserId.read().decode().split('=')
                            
                            # Split result into list and defining user ID                        
                            ConnectionProcessuserId = CheckPSUserId[1].replace('\n', '')
                        else:
                            # Defaulting user id to "none"
                            ConnectionProcessuserId = "none"
                    except:
                        # Defaulting user id to "none"
                        ConnectionProcessuserId = "none"
                        
                    # Checking if remote or local port is egal to "*". If yes, forcing it to 0
                    if RemotePort == "*":
                        RemotePort = "0"
                    if LocalPort == "*":
                        LocalPort = "0"
                    
                    # Triggering JSON sending
                    JSONTriggered = "yes"
                else:
                    # Avoid sending JSON for this connection
                    JSONTriggered = "no"
                    
            # Connection is UDP type
            elif ('udp' or 'udp4' or 'udp6') in SplitedConnection[0]:

                # Checcking IP version
                if ('udp' or 'udp4') in SplitedConnection[0]:
                    ConnectionIPVersion = "ipv4"
                    ConnectionIPVersionId = ConnectionIPVersion[-1]
                else:
                    ConnectionIPVersion = "ipv6"
                    ConnectionIPVersionId = ConnectionIPVersion[-1]
                
                # Defining Local and Remote ports
                LocalPort = SplitedConnection[3].split('.')[-1]
                RemotePort = SplitedConnection[4].split('.')[-1]
                
                # Checking if local UDP port is linked with an AIX service
                LocalPortTranslate = ""
                try:
                    PortTranslateCmd = 'cat /etc/services | grep -w ' + LocalPort + '/udp'
                    LocalPortTranslate = subprocess.Popen(PortTranslateCmd, shell=True, stdout=subprocess.PIPE).stdout
                    LocalPortTranslate = LocalPortTranslate.read().decode().split()[0]
                except:
                    pass

                # Checking if remote UDP port is linked with an AIX service
                RemotePortTranslate = ""
                try:
                    PortTranslateCmd = 'cat /etc/services | grep -w ' + RemotePort + '/udp'
                    RemotePortTranslate = subprocess.Popen(PortTranslateCmd, shell=True, stdout=subprocess.PIPE).stdout
                    RemotePortTranslate = RemotePortTranslate.read().decode().split()[0]
                except:
                    # Nothing to do
                    pass
                
                # Gathering local address and split it. Then, we force definition if port is listening
                LocalAddrSplit = SplitedConnection[3].split('.')
                if len(LocalAddrSplit) != 2:
                    LocalAddr = LocalAddrSplit[0] + '.' + LocalAddrSplit[1] + '.' + LocalAddrSplit[2] + '.' + LocalAddrSplit[3]
                else:
                    LocalAddr = "0.0.0.0"
                
                # Gathering remote address and split it. Then, we force definition if port is listening
                RemoteAddrSplit = SplitedConnection[4].split('.')
                if len(RemoteAddrSplit) != 2:
                    RemoteAddr = RemoteAddrSplit[0] + '.' + RemoteAddrSplit[1] + '.' + RemoteAddrSplit[2] + '.' + RemoteAddrSplit[3]
                else:
                    RemoteAddr = "0.0.0.0"
                    RemotePortTranslate = "LISTEN"
                
                # Checking if there is some match between AIX service ports and current UDP port
                if RemotePortTranslate and (isinstance(RemotePortTranslate, str)):
                    if LocalPortTranslate and (isinstance(LocalPortTranslate, str)):
                        # local and remote UDP ports are matching an AIX service. Taking that name for LSOF command
                        LsofCmd = 'lsof -i :' + LocalPortTranslate + '| tail -1'
                    else:
                        # Remote UDP ports is matching an AIX service. Taking that name for LSOF command
                        LsofCmd = 'lsof -i :' + LocalPort + ' | tail -1'
                else:
                    if LocalPortTranslate and (isinstance(LocalPortTranslate, str)):
                        # Local UDP ports is matching an AIX service. Taking that name for LSOF command
                        LsofCmd = 'lsof -i :' + LocalPortTranslate + ' | tail -1'
                    else:
                        # local and remote UDP ports are not matching an AIX service. 
                        LsofCmd = 'lsof -i :' + LocalPort + ' | tail -1'
                
                # Getting process information linked to the current connection
                # Gathering data from OS command for SystemSocketSummarySummary
                LSOFResult = subprocess.Popen(LsofCmd, shell=True, stdout=subprocess.PIPE, stderr = devnull).stdout
                LSOFResult = LSOFResult.read().decode().split()
                
                # Filling vars with LSOF results
                if LSOFResult:
                    ConnectionProcess = LSOFResult[0]
                    ConnectionProcessPID = LSOFResult[1]
                    ConnectionProcessuser = LSOFResult[2]
                else:
                    # Defaulting values with "none" string
                    ConnectionProcess = "none"
                    ConnectionProcessPID = "0"
                    ConnectionProcessuser = "none"
                    ConnectionProcessArgsJoined = ""
                
                # Get CPU args value from ps command for current process if PID defined
                if ConnectionProcessPID != "none":
                    CheckPSCommandCmd = "ps -ef -o pid,args | grep -w " + ConnectionProcessPID + " | grep -v grep"
                    CheckPSCommand = subprocess.Popen(CheckPSCommandCmd, shell=True, stdout=subprocess.PIPE).stdout
                    
                    # Split result into list
                    ConnectionProcessArgs = CheckPSCommand.read().decode().split()
                    
                    # Removing first line because it is the pid
                    del ConnectionProcessArgs[0]
                    
                    # Removing invalid ELK JSON caracters
                    for i in range(0, len(ConnectionProcessArgs)):
                        ConnectionProcessArgs[i] = re.sub('[\\\[\]\(\)\{\}"\b\t\n\a\r]', ' ', str(ConnectionProcessArgs[i]))
                    
                    # Joining list to make the final arg string
                    StrSeparator = " "
                    ConnectionProcessArgsJoined = StrSeparator.join(ConnectionProcessArgs)
                    
                    # Then, transforming this string to JSON tab format
                    FinalCmdString = '[\"' + ConnectionProcessArgsJoined.replace(' ','\",\"') + '\"]'
                    # Then removing first empty entrie
                    FinalCmdString = FinalCmdString.replace('"",','')
                    
                # Defining username id if not "none"
                # Tryin that into try/catch if grep is not giving any results
                try:
                    if ConnectionProcessuser != "none":
                        # Executing OS command
                        CheckUserIdCmd = 'lsuser -a id ' + ConnectionProcessuser
                        CheckPSUserId = subprocess.Popen(CheckUserIdCmd, shell=True, stdout=subprocess.PIPE).stdout
                        CheckPSUserId = CheckPSUserId.read().decode().split('=')
                        
                        # Split result into list and defining user ID                        
                        ConnectionProcessuserId = CheckPSUserId[1].replace('\n', '')
                    else:
                        # Defaulting user id to "none"
                        ConnectionProcessuserId = "none"
                except:
                    # Defaulting user id to "none"
                    ConnectionProcessuserId = "none"
                
                # Checking if remote or local port is egal to "*". If yes, forcing it to 0
                if RemotePort == "*":
                    RemotePort = "0"
                if LocalPort == "*":
                    LocalPort = "0"

                # Triggering JSON sending
                JSONTriggered = "yes"
            
            else:        
                # Avoid sending JSON for this connection
                JSONTriggered = "no"
                
            # IMPROVEMENT - How to know it the connection is inbound or outbound other than ephemeral port analysis and comparison ? For now, it is set to "unknown"

            # Sending JSON message only for TCP ESTABLISED/LISTEN and UDP connections
            if JSONTriggered == "yes":
                
                # Formatings vars to string
                ConnectionIPVersionId = str(ConnectionIPVersionId)
                ConnectionProcessuserId = str(ConnectionProcessuserId)
                LocalPort = str(LocalPort)
                RemotePort = str(RemotePort)
                
                # Let's construct JSON output for SystemSocket (ELK 7.5.0)
                JSONToSendValues=(''
                '\"network\":{'
                '\"type\":\"' + ConnectionIPVersion + '\",'
                '\"iana_number\":\"' + ConnectionIPVersionId + '\",'
                '\"direction\":\"unknown\"'
                '},'
                '\"user\":{'
                '\"id\":\"' + ConnectionProcessuserId + '\",'
                '\"name\":\"' + ConnectionProcessuser + '\",'
                '\"full_name\":\"' + ConnectionProcessuser + '\"'
                '},'
                '\"process\":{'
                '\"pid\":' + ConnectionProcessPID + ','
                '\"executable\":\"' + ConnectionProcess + '",'
                '\"name\":\"' + ConnectionProcess + '\",'
                '\"args\":' + FinalCmdString + ''
                '},'
                '\"system\":{'
                '\"socket\":{'
                '\"local\":{'
                '\"ip\":\"' + LocalAddr + '\",'
                '\"port\":' + LocalPort + ''
                '},'
                '\"process\":{'
                '\"cmdline\":\"' + ConnectionProcess + '\"'
                '},'
                '\"remote\":{'
                '\"ip\":\"' + RemoteAddr + '\",'
                '\"port\":' + RemotePort + ''
                '}'
                '}'
                '},'
                '\"destination\":{'
                '\"ip\":\"' + RemoteAddr + '\",'
                '\"port\":' + RemotePort + ''
                '},'
                '\"source\":{'
                '\"ip\":\"' + LocalAddr + '\",'
                '\"port\":' + LocalPort + ''
                '}')
                
                # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
                JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + '\",' + StaticJSON
                
                # Adding JSON metrics values gathered from OS command
                JSONToSend = JSONToSend + ',' + JSONToSendValues
                
                # Gathering "ELK formatted" dynamic JSON values
                RetDynamicJSON = GenerateDynamicJson("system.socket.summary", "system", "socket_summary", ComparisonTimer)
                
                # Adding dynamic "ELK Formating" JSON values into final JSON message
                JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
                
                # Let's send JSON message to ELK servers
                SendToELK(JSONToSend)
                
                # print('\nJSONToSend\n', JSONToSend, '\n')
                pass

def StartLogging():
    """ This function handle logging different informations into file 

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """
    global LogFilePath
    global StartupLogFile
    global StartupLogFileLast
    global MainLogFile
    global MainLogFileLast
    global CrashDumpCheckLog
    global CrashDumpSendLog
    global CrashDumpDaemon
    
    # Adding trailing / if necessary to LogFilePath
    if not LogFilePath.endswith('/'):
        LogFilePath = LogFilePath + '/'
        
    # Checking if log folder exists
    if not os.path.exists(LogFilePath):
        print('Error: Log directory does not exists. Exiting')
        sys.exit(2)
    
    # Defining Startup logging informations
    StartupLogFile = LogFilePath + "Metraixbeat.Startup.log"
    StartupLogFileLast = LogFilePath + "Metraixbeat.Startup.log.last"
    
    # print('LogFilePath: ', LogFilePath)
    # print('StartupLogFile: ', StartupLogFile)
    
    # sys.exit(2)
    
    # Defining Monitoring logging informations
    MainLogFile = LogFilePath + "Metraixbeat.log"
    MainLogFileLast = LogFilePath + "Metraixbeat.log.last"
    
    # Defining Crash logging informations
    CrashDumpCheckLog = LogFilePath + "Metraixbeat.crash.HTTP-Check.log"
    CrashDumpCheckLogLast = LogFilePath + "Metraixbeat.crash.HTTP-Check.log.last"
    CrashDumpSendLog = LogFilePath + "Metraixbeat.crash.HTTP-Send.log"
    CrashDumpSendLogLast = LogFilePath + "Metraixbeat.crash.HTTP-Send.log.last"
    CrashDumpDaemon = LogFilePath + "Metraixbeat.crash.Daemon.log"
    CrashDumpDaemonLast = LogFilePath + "Metraixbeat.crash.Daemon.log.last"

    # Checking if old Startup log file exists
    if os.path.exists(StartupLogFile):
        # Rotating logs
        shutil.move(StartupLogFile, StartupLogFileLast)

    # Checking if old Monitoring log file exists
    if os.path.exists(MainLogFile):
        # Rotating logs
        logging.shutdown()
        shutil.move(MainLogFile, MainLogFileLast)
        
    # Checking if old crash log files exist
    if os.path.exists(CrashDumpCheckLog):
        # Rotating logs
        logging.shutdown()
        shutil.move(CrashDumpCheckLog, CrashDumpCheckLogLast)
    if os.path.exists(CrashDumpSendLog):
        # Rotating logs
        logging.shutdown()
        shutil.move(CrashDumpSendLog, CrashDumpSendLogLast)
    if os.path.exists(CrashDumpDaemon):
        # Rotating logs
        logging.shutdown()
        shutil.move(CrashDumpDaemon, CrashDumpDaemonLast)

    # Configuring Startup log file options
    logging.basicConfig(filename=MainLogFile, level=logging.INFO, format='%(asctime)s %(message)s     ')

def main(argv):
    """ This is the main function
    
        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """
    
    # Define global variables
    global LogFilePath
    global ConfigFilePath
    global LogDir
    global ELKServers
    global ELKUsername
    global ELKPassword
    global ELKPort
    global ELKWebProtocol
    global ELKServers
    global ELKMetricIndexName
    global ELKLogIndexName
    global ELKServersAvailable
    global ELKServersFailed
    global devnull
    global LPARName
    global ExecutionTimers
    global TargetFileCurrentPosArray
    global TargetFileMTimeCountArray
    global SendQueueArray
    global FilebeatSendQueueArray
    global FilebeatConfigsArray
    global CustomMetricsConfigsArray
    global TailStateArray
    global PingPlotterArray
    global SystemProcessThread
    global SystemSocketThread
    global SystemDiskIOThread
    global SystemMemoryThread
    global PingPlotterThread
    global SystemHPMStatThread
    global StartupLogFile
    global StartupLogFileLast
    global MainLogFile
    global MainLogFileLast
    global CrashDumpCheckLog
    global CrashDumpSendLog
    
    # Define null var
    devnull = subprocess.DEVNULL
    # Defining LPAR Name
    LPARName = socket.gethostname()
    # Define ExecutionTimers dictionary
    ExecutionTimers = {}
    # Defining arrays for ELK servers follow-up
    ELKServers = []
    ELKServersAvailable = []
    ELKServersFailed = []
    # Define TargetFileCurrentPosArray dictionary
    TargetFileCurrentPosArray = {}
    # Define TargetFileCTargetFileMTimeCountArrayurrentPosArray dictionary
    TargetFileMTimeCountArray = {}
    # Define SendQueueArray list
    SendQueueArray = []
    # Define FilebeatSendQueueArray list
    FilebeatSendQueueArray = []
    # Define FilebeatConfigsArray for storing Filebeat config files content
    FilebeatConfigsArray = {}
    # Define FilebeatConfigsArray for storing Custom Metrics config files content
    CustomMetricsConfigsArray = {}
    # Define FilebeatConfigsArray for storing Tail process states
    TailStateArray = {}

    # Initializing background threads for long executions
    SystemProcessThread = threading.Thread(target=SystemProcess, args=())
    SystemDiskIOThread = threading.Thread(target=SystemDiskIO, args=())
    SystemSocketThread = threading.Thread(target=SystemSocket, args=())
    SystemMemoryThread = threading.Thread(target=SystemMemory, args=())
    PingPlotterThread = threading.Thread(target=PingPlotter, args=())
    SystemHPMStatThread = threading.Thread(target=SystemHPMStat, args=())
    
    # Define Empty vars for daemon config file and PID file
    LogFilePath = ''
    ConfigFilePath = ''
    
    # Checking given arguments with try/catch
    try:
        # Testing args
        opts, args = getopt.getopt(argv,"hc:l:",["ConfigFilePath=","LogFilePath="])
    except getopt.GetoptError:
        # Required args not provided
        print('Usage:')
        print('metraixbeat -c <Parameters.conf file path> -l <Logs directory path>')
        sys.exit(2)
        
    # Looping on each args given
    for opt, arg in opts:
        # Case of help
        if opt == '-h':
            print('Usage:')
            print('metraixbeat -c <Parameters.conf file path> -l <Logs directory path>')
            sys.exit()
        elif opt in ("-c", "--ConfigFilePath"):
            ConfigFilePath = arg
        elif opt in ("-l", "--LogFilePath"):
            LogFilePath = arg
            
    # Checking given parameters
    if not ConfigFilePath or not LogFilePath:
        print('Usage:')
        print('metraixbeat -c <Parameters.conf file path> -l <Logs directory path>')
        sys.exit()

    # Start logging events
    StartLogging()
    
    # Log
    logging.info('==> Starting "Metraixbeat" daemon...\n')

    # Loading daemon's configuration
    LoadDaemonConfig()

    # Gather all LPAR informations
    GetLPARInformations()

    # Loading plugin's configurations
    LoadPluginConfig()

    # Log
    logging.info("==> Configuration loaded, switching to Realtime monitoring !\n")

    # Creating Startup log file from cuurent logging file
    logging.shutdown()
    shutil.move(MainLogFile, StartupLogFile)

    # Main loop / Infinite loop
    while True:
        # Checking if old Monitoring log file exists
        if os.path.exists(MainLogFile):
            # Rotating logs
            logging.shutdown()
            shutil.move(MainLogFile, MainLogFileLast)
            
        # Log
        logging.info("\n ---------------------------------------------SUMMARY------------------------------------------------------\n")
    
        # Check ELK server's availability
        CheckELKServers()

        # Compare timers and schedule work for each metricset
        WorkOnMetrics()

        # Analyse and schedule work for each Tail plugins if timer is reached
        ComparisonTimer = CompareTimer("TailPluginsRefresh", TailRefreshValue)
        if ComparisonTimer != devnull:
            for TailPlugin in FilebeatConfigsArray:
                # Launch analyse/work on current Tail plugin config
                FilebeatTail(TailPlugin)

        # Log threading summary
        logging.info('INFO     ==> Running threads: ' + str(threading.enumerate()))

        # Enable the following loop to print internal daemon vars for stability
        logging.info('INFO     ==> Internal vars count: ' + str(len(globals())))

        # Sleep for specified time
        time.sleep(CycleSleepTime)


# Script entry point
if __name__ == "__main__":

    # Setting PID file location
    PIDFile = "/var/run/metraixbeat.pid"
    
    # Checking if PID file already exists
    if os.path.exists(PIDFile):
        # If yes, checking if process is running or not
        OpenedPIDFile = open(PIDFile,"r")
        CurrentPID = OpenedPIDFile.read()
        
        # Checking if process is still running
        PSCommand = "ps -ef | grep " + CurrentPID + " | grep -v grep"
        GrepCurrentPID = subprocess.Popen(PSCommand, shell=True, stdout=subprocess.PIPE).stdout
        # GrepCurrentPID = GrepCurrentPID.read().decode().split('\n')
        GrepCurrentPID = GrepCurrentPID.read().decode()
        
        # Checking result
        if GrepCurrentPID:
            # Process is already running, exiting...
            print('Process already running \n' + GrepCurrentPID + '\nExiting...')
            sys.exit(2)
        else:
            # Cleaning vars
            del PSCommand
            del GrepCurrentPID
    
    # Do the Unix double-fork magic; 
    # see Stevens's book "Advanced Programming in the UNIX Environment" (Addison-Wesley) for details
    try:
        pid = os.fork()
        if pid > 0:
            # Exit first parent
            sys.exit(0)

    except OSError as e:
        sys.exit(1)

    # Decouple from parent environment
    os.chdir("/")
    os.setsid()
    os.umask(0)

    # Do second fork
    try:
        pid = os.fork()
        if pid > 0:
            # Creating the PID file
            OpenedPIDFile = open(PIDFile,"w+")

            # Filling it with data
            OpenedPIDFile.write(str(pid))

            # Close the PID file
            OpenedPIDFile.close()

            # Exit first parent
            sys.exit(0)

    except OSError as e:
        sys.exit(1)

    # Start the daemon main loop with Parameter.conf file as arg
    main(sys.argv[1:])
    