#!/usr/bin/python
import json
import os
import random
import re
import subprocess
import datetime
import logging
import threading
from time import time
import traceback
import pingparsing

from urllib.error import HTTPError

# import 
import values
from samples import ConvertFileName, GenerateEphemeralID




def GetLPARInformations():
    """ Get necessary LPAR informations.
        All those values are used to format JSON messages sent to ELK.

        FQDN:
        Adding DNS suffix on hostname which doesn't have one.

        LPARArch and AIXVersion:
        Get LPAR AIX version and Power architecture (Power8/7/6+).

        NetworkCards:
        List of network cards is taken from "ifconfig -a" OS command.

        FcCards:
        List of Fiber Channel cards which are correctly connected and linked.

        IDs:
        LPARRndID is fixed across reboot. Defined one time if ID file does not exist.
        AgentID is fixed across reboot. Defined one time if ID file does not exist.
        EphemeralID is generated on each daemon execution and generated by 'GenerateEphemeralID' method.

        NumProc:
        Define the number of CPU threads available on LPAR.
    """

  

    # Checking if LPAR name contains DNS suffix. Adding if not
    # Checking first if var FQDN is not null
    if values.FQDN != "":
        if not values.FQDN in values.LPARName:
            values.LPARName = values.LPARName + values.FQDN

    # Log
    logging.info(" - Checking LPAR hostname... Done !")

    # Get LPAR hardware architecture
    CmdLine = "prtconf | egrep \"Processor Type|Serial Number\" | awk '{print $NF}'"
    LPARPrtConf =  subprocess.Popen(CmdLine, shell=True, stdout=subprocess.PIPE).stdout
    LPARPrtConf = LPARPrtConf.read().decode().split('\n')
    values.LPARArch = LPARPrtConf[1]
    values.LPARHost = LPARPrtConf[0]
    
    
    # Disabled for now as hosting frame has been moved to labels
    # Tags.append(LPARHost)
    # Tagz = Tagz + ',' + LPARHost
    
    # Adding hosting frame to Labels list
    LabelHostStr = 'powersn:' + str(values.LPARHost)
    values.Labels.append(LabelHostStr)
    
    # Log
    logging.info(" - Checking LPAR Architecture... Done !")
    
    # Get LPAR AIX OS Version
    values.AIXVersion =  subprocess.Popen("oslevel -s", shell=True, stdout=subprocess.PIPE).stdout
    #print(values.AIXVersion)
    values.AIXVersion =  values.AIXVersion.read().decode().replace("\n","")
    
    # Checking if LPAR or VIOS
    if os.path.exists("/usr/ios/cli/ioscli"):
        values.IfVIOS = True
    else:
        values.IfVIOS = False

    # Log
    logging.info(" - Checking LPAR AIX version ... Done !")
    
    # Get current SMT mode  
    LPARSMTModeCmdLine = "smtctl | grep has | awk '{print $3}' | head -1"    
    values.LPARSMTMode =  subprocess.Popen(LPARSMTModeCmdLine, shell=True, stdout=subprocess.PIPE).stdout
    values.LPARSMTMode =  values.LPARSMTMode.read().decode().replace("\n","")
    
    # Defining config path from ConfigFilePath
    ConfigPathtmp = values.ConfigFilePath.split('/').pop()
    ConfigPath = values.ConfigFilePath.replace(ConfigPathtmp,'')

    
    # Defining LPAR ID file name
    RandomIDFile = ConfigPath + 'Host.ID'

    # If LPAR ID file exists, let's get content as defined scripts vars
    if os.path.exists(RandomIDFile):
        # Get LPAR Unique ID
        # IMPROVEMENT / Can be converted into python style
        LPARRndIDCmd = "cat " + RandomIDFile + " | grep LPARRndID | awk '{print $2}'"
        values.LPARRndID = subprocess.Popen(LPARRndIDCmd, shell=True, stdout=subprocess.PIPE).stdout
        values.LPARRndID = values.LPARRndID.read().decode().replace('\r','')
        values.LPARRndID = values.LPARRndID.replace('\n','')
        # Get LPAR ephemeral ID
        # IMPROVEMENT / Can be converted into python style
        AgentIDCmd = "cat " + RandomIDFile + " | grep AgentID | awk '{print $2}'"
        values.AgentID = subprocess.Popen(AgentIDCmd, shell=True, stdout=subprocess.PIPE).stdout
        values.AgentID = values.AgentID.read().decode().replace('\r','')
        values.AgentID = values.AgentID.replace('\n','')

    # If not, let's create it, fill it and define scripts vars
    else:
        # Generating LPAR ID in specific format
        # IMPROVEMENT / Can be converted into python style
        values.LPARRndID = subprocess.Popen("cat /dev/urandom | tr -dc 'a-z0-9' | fold -w 32 | head -n 1", shell=True, stdout=subprocess.PIPE).stdout
        values.LPARRndID = values.LPARRndID.read().decode().replace("\n","")

        # Generating unique formated ID for uniqueAgent ID value
        # as it is same format than ephemeral ID
        GenerateEphemeralID()
        values.AgentID = values.EphemeralID

        # Creating the LPAR ID file
        OpenedFile = open(RandomIDFile,"w+")

        # Filling it with data
        OpenedFile.write("LPARRndID ")
        OpenedFile.write(values.LPARRndID)
        OpenedFile.write("\r\n")
        OpenedFile.write("AgentID ")
        OpenedFile.write(values.AgentID)
        OpenedFile.write("\r\n")

        # Close the LPAR ID file
        OpenedFile.close()

    # Generating unique and ephemeral ID for current script execution
    GenerateEphemeralID()

    # Log
    logging.info(" - Checking LPAR Metricbeat unique ID... Done !")

    # Defining the name of network adapters
    if values.EntRestricted == 'all':
        # No restriction, taking all network adapters
        values.NetworkCards = subprocess.Popen("ifconfig -a | grep ': ' | awk -F ':' '{print $1}' | grep -v 'lo0'", shell=True, stdout=subprocess.PIPE).stdout
        values.NetworkCards = values.NetworkCards.read().decode().split('\n')
        if values.IfVIOS:
            # Detecting SEA adapters
            NetworkCardsSEA = subprocess.Popen("lsdev -Cc adapter | grep 'Shared Ethernet Adapter' | awk '{print $1}'", shell=True, stdout=subprocess.PIPE).stdout
            NetworkCardsSEA = NetworkCardsSEA.read().decode().split('\n')
            # Adding SEA to the list of monitored adapters
            values.NetworkCards = values.NetworkCards + NetworkCardsSEA
    else:
        # Restriction are specified. Setting Network card list to the given list
        values.NetworkCards = []
        values.NetworkCards = values.EntRestricted.split(',')
    
    # Log
    logging.info(" - Checking LPAR Network interface list ... Done !")

    # Defining the name of FC adapters
    if values.FcsRestricted == 'all':
        FCCardsTemp = subprocess.Popen("lsdev -Cc adapter | grep fcs | awk '{print $1}'", shell=True, stdout=subprocess.PIPE).stdout
        FCCardsTemp = FCCardsTemp.read().decode().split('\n')
    else:
        # Restriction are specified. Setting Network card list to the given list
        FCCardsTemp = []
        FCCardsTemp = values.FcsRestricted.split(',')

    # Log
    logging.info(" - Checking LPAR FC interface list ... Done !")

    # Checking FC link status and exclude unlinked cards
    for CurrLine in FCCardsTemp:
        if(len(CurrLine) != 0):
            FCCardFcStatCmd = "fcstat " + CurrLine
            FCCardFcStatState = subprocess.Popen(FCCardFcStatCmd, shell=True, stdout = subprocess.PIPE, stderr = subprocess.PIPE)
            stdout, stderr = FCCardFcStatState.communicate()
            # If link is ok, adding the FC card into definitive array
            if(FCCardFcStatState.returncode == 0):
                values.FcCards.append(CurrLine)

    # Log
    logging.info(" - Checking LPAR FC link status... Done !")

    # Defining number of CPU threads
    values.NumProc = subprocess.Popen("bindprocessor -q | awk '{print $NF}'", shell=True, stdout=subprocess.PIPE).stdout
    values.NumProc = values.NumProc.read().decode().split()
    values.NumProc = int(values.NumProc[0]) + 1
    values.NumProcString = str(values.NumProc)

    # Log
    logging.info(" - Checking LPAR CPU configuration... Done !")
    
    # Generate fix JSON values for LPAR
    GenerateStaticJSON()


def GenerateStaticJSON():
    """This function generate a static JSON message containing the pieces that will remain unchange until daemon restart

    Please refer to README.md

    TODO: Describe the function, all vars and its content
    """

    # Define tags for metricbeat
    JSONTags = (''
                '\"tags\": ' + str(values.Tags).replace('\'', '"') + ','
                '\"rollup\":{\"tagz\": \"' + str(values.Tagz) + '\",\"hostname\":\"' + values.LPARName + '\"},')

    # Define rollup tags for filebeat (adding extra tagz and hostname field with rolled-up naming convention for easy use in dashboard)
    JSONTagsFilebeat = (''
                        '\"tags\": ' + str(values.Tags).replace('\'', '"') + ','
                        '\"rollup\":{\"tagz\":{\"terms\":{\"value\": \"' + str(values.Tagz) + '\"}},\"hostname\":{\"terms\":{\"value\": \"' + values.LPARName + '\"}}},')

    # Define Agent JSON for metricbeat
    JSONAgent = (''
                 '\"agent\":{'
                 '\"version\":\"' + values.ELKMonitoringVersion + '\",'
                 '\"type\":\"metricbeat\",'
                 '\"ephemeral_id\":\"' + values.EphemeralID + '\",'
                 '\"hostname\":\"' + values.LPARName + '\",'
                 '\"id\":\"' + values.AgentID + '\"'
                 '},')

    # Define Agent JSON for filebeat
    JSONAgentFilebeat = (''
                         '\"agent\":{'
                         '\"version\":\"' + values.ELKMonitoringVersion + '\",'
                         '\"type\":\"filebeat\",'
                         '\"ephemeral_id\":\"' + values.EphemeralID + '\",'
                         '\"hostname\":\"' + values.LPARName + '\",'
                         '\"id\":\"' + values.AgentID + '\"'
                         '},')

    # Checking if LPAR is a VIOS
    if values.IfVIOS:
        values.OSFamily = 'VIOS'
    else:
        values.OSFamily = 'AIX'

    # Define Host JSON for metricbeat
 
    JSONHost = (''
    '\"host\":{'
    '\"name\":\"' + values.LPARName + '\",'
    '\"hostname\":\"' + values.LPARName + '\",'
    '\"architecture\":\"' + values.LPARArch + '\",'
    '\"id\":\"' + values.LPARRndID + '\",'
    '\"containerized\":false,'
    '\"os\":{'
    '\"platform\":\"AIX\",'
    '\"version\":\"' + values.AIXVersion + '\",'
    '\"family\":\"' + values.OSFamily + '\",'
    '\"name\":\"AIX\",'
    '\"kernel\":\"' + values.AIXVersion + '\",'
    '\"codename\":\"AIX\"'
    '}},')

    # Define ECS JSON
    JSONEcs = (''
               '\"ecs\":{'
               '\"version\":\"' + values.ECSVersion + '\"'
               '},')

    # Generating labels JSON with key/value pairs
    LabelString = '\"labels\":{'
    for Label in values.Labels:
        if len(Label) != 0:
            LabelString += '\"' + Label.replace(':', '\":\"') + '\",'

    LabelString = LabelString[:-1]
    LabelString += '}'

    values.StaticJSON = JSONTags + JSONAgent + JSONHost + JSONEcs + LabelString
    values.StaticJSONFilebeat = JSONTagsFilebeat + \
        JSONAgentFilebeat + JSONHost + JSONEcs + LabelString

    # print(StaticJSON)

    pass

def GenerateDynamicJson(EventDataset, ServiceType, MetricSet, ComparisonTimer):
    """This function generate a static JSON message containing the pieces that will change for each message

    Please refer to README.md

    TODO: Describe the function, all vars and its content
    """
    JSONServiceType = (''
                       '\"service\":{'
                       '\"type\":\"' + ServiceType + '\"'
                       '},')

    JSONEventDataset = (''
                        '\"event\":{'
                        '\"module\":\"' + ServiceType + '\",'
                        '\"duration\":' + ComparisonTimer + ','
                        '\"dataset\":\"' + EventDataset + '\"'
                        '},')

    JSONMetricSet = (''
                     '\"metricset\":{'
                     '\"name\":\"' + MetricSet + '\",'
                     '\"period\":1'
                     '}')

    DynamicJSON = JSONServiceType + JSONEventDataset + JSONMetricSet

    return DynamicJSON

def SendToMetricbeat(BulkJSON, BulkSize):
    """ This function send a JSON message to the Metricbeat Logstash server.
        If Logstash is not available, the JSON message is queued for further reprocessing.

        Please refer to README.md

        TODO: Describe the function, all vars and its content
    """   

    # Choosing one availale Elastic server in the pool, if Message bucket is fill enough
    ElasticServer = random.choice(values.ElasticServersAvailable)
    # print('Elastic server choosen: ', ElasticServer)
    # print('Available list: ', ElasticServersAvailable)

    # Send Data to Elastic server with a Try/Catch
    try:
        # Define ELK url depending of index and choosen Elastic server
        CurrentIndex = values.ElasticIndexName + '-' + values.ELKMonitoringVersion
        ElasticServerURL = values.ELKWebProtocol + '://' + ElasticServer + \
            ':' + values.ElasticPort + '/' + CurrentIndex + '/_bulk'

        # Defining credential for web request
        values.ELKCreds = (values.ELKUsername, values.ELKPassword)

        # Connect and send encoded data
        ELKAnswer = values.ElasticServerCnx.post(ElasticServerURL, data=BulkJSON.encode("utf8"), headers={
                                          "Content-Type": "application/x-ndjson; charset=utf-8"}, auth=values.values.ELKCreds, timeout=5, verify=values.ELKCertificate)

        # If the response was successful, no Exception will be raised
        ELKAnswer.raise_for_status()

        # print('DEBUG: ELK Answer: ', str(ELKAnswer))
        # print(str(ELKAnswer.text))

        # Removing those successfully sent messages from queue
        del values.SendQueueArray[0:BulkSize]

    except HTTPError as http_err:
        # We received HTTP exception
        # Elastic server has fall down, queing message and setting Elastic server as unavailable
        try:
            values.ElasticServersAvailable.remove(ElasticServer)
            values.ElasticServersFailed.append(ElasticServer)
        except:
            # Nothing to do, server already removed
            # traceback.print_exc()
            pass

        # Log
        logging.info("WARNING  ==> " + ElasticServer +
                     " became unvailable, queueing Metricbeat JSON messages")
        # print(str(ELKAnswer))
        # print(str(ELKAnswer.text))

        # Creating the error stack file and filling error stack informations
        with open(values.CrashDumpSendLog, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(http_err))
            f.write(traceback.format_exc())
            # f.write('\nJSON message:\n')
            # f.write(JSONToSend)
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('WARNING  ==> Please check ' +
                     values.CrashDumpSendLog + ' for more informations on HTTP errors')
        # traceback.print_exc()

    except Exception as err:
        # We received another type of exception
        # Elastic server has fall down, queing message and setting Elastic server as unavailable
        # Making that into a try/catch because some background threads can act on that also and clea the content
        try:
            values.ElasticServersAvailable.remove(ElasticServer)
            values.ElasticServersFailed.append(ElasticServer)
        except:
            # Nothing to do, server already removed
            # traceback.print_exc()
            pass

        # Log
        logging.info("WARNING  ==> " + ElasticServer +
                     " became unvailable, queueing Metricbeat JSON messages")
        # print(str(ELKAnswer))
        # print(str(ELKAnswer.text))

        # Creating the error stack file and filling error stack informations
        with open(values.CrashDumpSendLog, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(err))
            f.write(traceback.format_exc())
            # f.write('\nJSON message:\n')
            # f.write(JSONToSend)
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('WARNING  ==> Please check ' +
                     values.CrashDumpSendLog + ' for more informations on HTTP errors')
        # traceback.print_exc()
        pass

def SendToFilebeat(BulkJSON, BulkSize):
    """ This function send a JSON message to the Metricbeat Logstash server.
        If Logstash is not available, the JSON message is queued for further reprocessing.

        Please refer to README.md

        TODO: Describe the function, all vars and its content
    """
   
    # Choosing one availale Filebeat server in the pool, if Message bucket is fill enough
    FilebeatServer = random.choice(values.FilebeatServersAvailable)
    # print('Filebeat server choosen: ', FilebeatServer)
    # print('Available list: ', FilebeatServersAvailable)

    # Send Data to Filebeat server with a Try/Catch
    try:
        # Define ELK url depending of index and choosen Filebeat server
        CurrentIndex = values.FilebeatIndexName + '-' + values.ELKMonitoringVersion
        FilebeatServerURL = values.ELKWebProtocol + '://' + FilebeatServer + \
            ':' + values.FilebeatPort + '/' + CurrentIndex + '/_bulk'

        # Defining credential for web request
        values.ELKCreds = (values.ELKUsername, values.ELKPassword)

        # Connect and send encoded data
        ELKAnswer = values.FilebeatServerCnx.post(FilebeatServerURL, data=BulkJSON.encode("utf8"), headers={
                                           "Content-Type": "application/x-ndjson; charset=utf-8"}, auth=values.values.ELKCreds, timeout=5, verify=values.ELKCertificate)

        # If the response was successful, no Exception will be raised
        ELKAnswer.raise_for_status()

        # print('DEBUG: ELK Answer: ', str(ELKAnswer))
        # print(str(ELKAnswer.text))

        # Removing those successfully sent messages from queue
        del values.FilebeatSendQueueArray[0:BulkSize]

    except HTTPError as http_err:
        # We received HTTP exception
        # Filebeat server has fall down, queing message and setting Filebeat server as unavailable
        try:
            values.FilebeatServersAvailable.remove(FilebeatServer)
            values.FilebeatServersFailed.append(FilebeatServer)
        except:
            # Nothing to do, server already removed
            # traceback.print_exc()
            pass

        # Log
        logging.info("WARNING  ==> " + FilebeatServer +
                     " became unvailable, queueing Filebeat JSON messages")
        # print(str(ELKAnswer))
        # print(str(ELKAnswer.text))

        # Creating the error stack file and filling error stack informations
        with open(values.CrashDumpSendLog, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(http_err))
            f.write(traceback.format_exc())
            # f.write('\nJSON message:\n')
            # f.write(JSONToSend)
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('WARNING  ==> Please check ' +
                    values.CrashDumpSendLog + ' for more informations on HTTP errors')
        # traceback.print_exc()

    except Exception as err:
        # We received another type of exception
        # Filebeat server has fall down, queing message and setting Filebeat server as unavailable
        # Making that into a try/catch because some background threads can act on that also and clea the content
        try:
            values.FilebeatServersAvailable.remove(FilebeatServer)
            values.FilebeatServersFailed.append(FilebeatServer)
        except:
            # Nothing to do, server already removed
            # traceback.print_exc()
            pass

        # Log
        logging.info("WARNING  ==> " + FilebeatServer +
                     " became unvailable, queueing Filebeat JSON messages")

        # print(str(ELKAnswer))
        # print(str(ELKAnswer.text))

        # Creating the error stack file and filling error stack informations
        with open(values.CrashDumpSendLog, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(err))
            f.write(traceback.format_exc())
            # f.write('\nJSON message:\n')
            # f.write(JSONToSend)
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('WARNING  ==> Please check ' +
                     values.CrashDumpSendLog + ' for more informations on HTTP errors')
        # traceback.print_exc()
        pass

def SendToLogstash(BulkJSON, BulkSize):
    """ This function send a JSON message to the Metricbeat Logstash server.
        If Logstash is not available, the JSON message is queued for further reprocessing.

        Please refer to README.md

        TODO: Describe the function, all vars and its content
    """
    
    # Choosing one availale Logstash server in the pool, if Message bucket is fill enough
    LogstashServer = random.choice(values.LogstashServersAvailable)
    # print('Logstash server choosen: ', LogstashServer)
    # print('Available list: ', LogstashServersAvailable)

    # Send Data to Logstash server with a Try/Catch
    try:
        # Define ELK url depending of index and choosen Logstash server
        LogstashServerURL = values.ELKWebProtocol + '://' + \
            LogstashServer + ':' + values.LogstashPort + '/_bulk'

        # Defining credential for web request
        values.ELKCreds = (values.ELKUsername, values.ELKPassword)

        # Connect and send encoded data
        ELKAnswer = values.LogstashServerCnx.post(LogstashServerURL, data=BulkJSON.encode("utf8"), headers={
                                           "Content-Type": "application/x-ndjson; charset=utf-8"}, auth=values.values.ELKCreds, timeout=5, verify=values.ELKCertificate)

        # If the response was successful, no Exception will be raised
        ELKAnswer.raise_for_status()

        # print('DEBUG: ELK Answer: ', str(ELKAnswer))
        # print(str(ELKAnswer.text))

        # Removing those successfully sent messages from queue
        del values.LogstashSendQueueArray[0:BulkSize]

    except HTTPError as http_err:
        # We received HTTP exception
        # Logstash server has fall down, queing message and setting Logstash server as unavailable
        try:
            values.LogstashServersAvailable.remove(LogstashServer)
            values.LogstashServersFailed.append(LogstashServer)
        except:
            # Nothing to do, server already removed
            # traceback.print_exc()
            pass

        # Log
        logging.info("WARNING  ==> " + LogstashServer +
                     " became unvailable, queueing Logstash JSON messages")
        # print(str(ELKAnswer))
        # print(str(ELKAnswer.text))

        # Creating the error stack file and filling error stack informations
        with open(values.CrashDumpSendLog, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(http_err))
            f.write(traceback.format_exc())
            # f.write('\nJSON message:\n')
            # f.write(JSONToSend)
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('WARNING  ==> Please check ' +
                    values.CrashDumpSendLog + ' for more informations on HTTP errors')
        # traceback.print_exc()

    except Exception as err:
        # We received another type of exception
        # Logstash server has fall down, queing message and setting Logstash server as unavailable
        # Making that into a try/catch because some background threads can act on that also and clea the content
        try:
            values.LogstashServersAvailable.remove(LogstashServer)
            values.LogstashServersFailed.append(LogstashServer)
        except:
            # Nothing to do, server already removed
            # traceback.print_exc()
            pass

        # Log
        logging.info("WARNING  ==> " + LogstashServer +
                     " became unvailable, queueing Logstash JSON messages")

        # print(str(ELKAnswer))
        # print(str(ELKAnswer.text))

        # Creating the error stack file and filling error stack informations
        with open(values.CrashDumpSendLog, 'a') as f:
            f.write('\n\n')
            f.write(str(datetime.datetime.now()))
            f.write(':\n')
            f.write(str(err))
            f.write(traceback.format_exc())
            # f.write('\nJSON message:\n')
            # f.write(JSONToSend)
            f.write('\n')

        # Print and log error stack for debug purpose
        logging.info('WARNING  ==> Please check ' +
                    values.CrashDumpSendLog + ' for more informations on HTTP errors')
        # traceback.print_exc()
        pass

def SendJSON(JSONToSend, MesssageOutput='Metricbeat'):
    """ This function send a JSON message to the Metricbeat Logstash server.
        If Logstash is not available, the JSON message is queued for further reprocessing.

        Please refer to README.md

        TODO: Describe the function, all vars and its content
    """

    # Metricbeat Pool
    if MesssageOutput == 'Metricbeat':

        # Checking if JSON message need to be queued or sent
        if (values.ElasticServersAvailable and len(values.SendQueueArray) >= values.BulkMaxSize and JSONToSend != 'Flush'):

            # Checking if history messages are in the queue
            if len(values.SendQueueArray) == values.BulkMaxSize:
                # No historical message present
                if values.BulkMaxSize <= 1:
                    # If BulkMaxSize is 0 or 1, we sent the JSON message directly
                    BulkJSON = '\n{ "index":{} }\n' + JSONToSend + '\n'

                else:
                    # Creating Bulk JSON message
                    separator = '\n{ "index":{} }\n'
                    BulkJSON = '{ "index":{} }\n' + \
                        str(separator.join(
                            values.SendQueueArray[0:values.BulkMaxSize])) + '\n'
                    BulkSize = values.BulkMaxSize

                    # Queuing the current message from function call, waiting for next loop execution
                    values.SendQueueArray.append(JSONToSend)

            else:
                # Historical messages are queued, let's send them one by one to the current message
                # Creating Bulk JSON message
                BulkJSON = '{ "index":{} }\n' + \
                    str(values.SendQueueArray[0]) + \
                    '\n{ "index":{} }\n' + JSONToSend + '\n'
                BulkSize = 1

            # Recording that queue has been flushed, or tried to
            values.MetricbeatQueueFlushed = True
            SendToMetricbeat(BulkJSON, BulkSize)

        else:
            # Checking if flush order has been sent
            if JSONToSend == 'Flush':
                # Creating Bulk JSON message and flushing the queue content
                if values.BulkMaxSize <= 1:
                    # If BulkMaxSize is 0 or 1, we sent the message directly
                    BulkJSON = '\n{ "index":{} }\n' + values.SendQueueArray[0] + '\n'

                    # Log
                    # LogString = "INFO  ==>  " + str(len(SendQueueArray)) + " JSON Metricbeat messages has been FORCE flushed"
                    # logging.info(LogString)

                    # Recording that queue has been flushed, or tried to
                    values.MetricbeatQueueFlushed = True
                    SendToMetricbeat(BulkJSON, 1)

                else:
                    # Checking for the correct size of the flush
                    if len(values.SendQueueArray) > values.BulkMaxSize:
                        BulkSize = values.BulkMaxSize
                    else:
                        BulkSize = len(values.SendQueueArray)

                    separator = '\n{ "index":{} }\n'
                    BulkJSON = '{ "index":{} }\n' + \
                        str(separator.join(
                            values.SendQueueArray[0:len(values.SendQueueArray)])) + '\n'

                    # Log
                    # LogString = "INFO  ==>  " + str(len(SendQueueArray)) + " JSON Metricbeat messages has been flushed"
                    # logging.info(LogString)

                    # Recording that queue has been flushed, or tried to
                    values.MetricbeatQueueFlushed = True
                    SendToMetricbeat(BulkJSON, BulkSize)

            else:
                # No Elastic server available or Bulk max size not reached, waiting for next loop
                values.SendQueueArray.append(JSONToSend)

    # Filebeat Pool
    if MesssageOutput == 'Filebeat':

        # Checking if JSON message need to be queued or sent
        if (values.FilebeatServersAvailable and len(values.FilebeatSendQueueArray) >= values.BulkMaxSize and JSONToSend != 'Flush'):

            # Checking if history messages are in the queue
            if len(values.FilebeatSendQueueArray) == values.BulkMaxSize:
                # No historical message present
                if values.BulkMaxSize <= 1:
                    # If BulkMaxSize is 0 or 1, we sent the JSON message directly
                    BulkJSON = '\n{ "index":{} }\n' + JSONToSend + '\n'

                else:
                    # Creating Bulk JSON message
                    separator = '\n{ "index":{} }\n'
                    BulkJSON = '{ "index":{} }\n' + \
                        str(separator.join(
                            values.FilebeatSendQueueArray[0:values.BulkMaxSize])) + '\n'
                    BulkSize = values.BulkMaxSize

                    # Queuing the current message from function call, waiting for next loop execution
                    values.FilebeatSendQueueArray.append(JSONToSend)

            else:
                # Historical messages are queued, let's send them one by one to the current message
                # Creating Bulk JSON message
                BulkJSON = '{ "index":{} }\n' + \
                    str(values.FilebeatSendQueueArray[0]) + \
                    '\n{ "index":{} }\n' + JSONToSend + '\n'
                BulkSize = 1

            # Recording that queue has been flushed, or tried to
            values.FilebeatQueueFlushed = True
            SendToFilebeat(BulkJSON, BulkSize)

        else:
            # Checking if flush order has been sent
            if JSONToSend == 'Flush':
                # Creating Bulk JSON message and flushing the queue content
                if values.BulkMaxSize <= 1:
                    # If BulkMaxSize is 0 or 1, we sent the message directly
                    BulkJSON = '\n{ "index":{} }\n' + \
                        values.FilebeatSendQueueArray[0] + '\n'

                    # Log
                    # LogString = "INFO  ==>  " + str(len(FilebeatSendQueueArray)) + " JSON Filebeat messages has been FORCE flushed"
                    # logging.info(LogString)

                    # Recording that queue has been flushed, or tried to
                    values.FilebeatQueueFlushed = True
                    SendToFilebeat(BulkJSON, 1)

                else:
                    # Checking for the correct size of the flush
                    if len(values.FilebeatSendQueueArray) > values.BulkMaxSize:
                        BulkSize = values.BulkMaxSize
                    else:
                        BulkSize = len(values.FilebeatSendQueueArray)

                    separator = '\n{ "index":{} }\n'
                    BulkJSON = '{ "index":{} }\n' + \
                        str(separator.join(
                            values.FilebeatSendQueueArray[0:BulkSize])) + '\n'

                    # Log
                    # LogString = "INFO  ==>  " + str(len(FilebeatSendQueueArray)) + " JSON Filebeat messages has been flushed"
                    # logging.info(LogString)

                    # Recording that queue has been flushed, or tried to
                    values.FilebeatQueueFlushed = True
                    SendToFilebeat(BulkJSON, BulkSize)

            else:
                # No Filebeat server available or Bulk max size not reached, waiting for next loop
                values.FilebeatSendQueueArray.append(JSONToSend)

    # Logstash Pool
    if MesssageOutput == 'Logstash':

        # Checking if JSON message need to be queued or sent
        if (values.LogstashServersAvailable and len(values.LogstashSendQueueArray) >= values.BulkMaxSize and JSONToSend != 'Flush'):

            # Checking if history messages are in the queue
            if len(values.LogstashSendQueueArray) == values.BulkMaxSize:
                # No historical message present
                if values.BulkMaxSize <= 1:
                    # If BulkMaxSize is 0 or 1, we sent the JSON message directly
                    BulkJSON = '\n{ "index":{} }\n' + JSONToSend + '\n'

                else:
                    # Creating Bulk JSON message
                    separator = '\n{ "index":{} }\n'
                    BulkJSON = '{ "index":{} }\n' + \
                        str(separator.join(
                            values.LogstashSendQueueArray[0:values.BulkMaxSize])) + '\n'
                    BulkSize = values.BulkMaxSize

                    # Queuing the current message from function call, waiting for next loop execution
                    values.LogstashSendQueueArray.append(JSONToSend)

            else:
                # Historical messages are queued, let's send them one by one to the current message
                # Creating Bulk JSON message
                BulkJSON = '{ "index":{} }\n' + \
                    str(values.LogstashSendQueueArray[0]) + \
                    '\n{ "index":{} }\n' + JSONToSend + '\n'
                BulkSize = 1

            # Recording that queue has been flushed, or tried to
            values.LogstashQueueFlushed = True
            SendToLogstash(BulkJSON, BulkSize)

        else:
            # Checking if flush order has been sent
            if JSONToSend == 'Flush':
                # Creating Bulk JSON message and flushing the queue content
                if values.BulkMaxSize <= 1:
                    # If BulkMaxSize is 0 or 1, we sent the message directly
                    BulkJSON = '\n{ "index":{} }\n' + \
                        values.LogstashSendQueueArray[0] + '\n'

                    # Log
                    # LogString = "INFO  ==>  " + str(len(LogstashSendQueueArray)) + " JSON Logstash messages has been FORCE flushed"
                    # logging.info(LogString)

                    # Recording that queue has been flushed, or tried to
                    values.LogstashQueueFlushed = True
                    SendToLogstash(BulkJSON, 1)

                else:
                    # Checking for the correct size of the flush
                    if len(values.LogstashSendQueueArray) > values.BulkMaxSize:
                        BulkSize = values.BulkMaxSize
                    else:
                        BulkSize = len(values.LogstashSendQueueArray)

                    separator = '\n{ "index":{} }\n'
                    BulkJSON = '{ "index":{} }\n' + str(separator.join(
                        values.LogstashSendQueueArray[0:len(values.LogstashSendQueueArray)])) + '\n'

                    # Log
                    # LogString = "INFO  ==>  " + str(len(LogstashSendQueueArray)) + " JSON Logstash messages has been flushed"
                    # logging.info(LogString)

                    # Recording that queue has been flushed, or tried to
                    values.LogstashQueueFlushed = True
                    SendToLogstash(BulkJSON, BulkSize)

            else:
                # No Logstash server available or Bulk max size not reached, waiting for next loop
                values.LogstashSendQueueArray.append(JSONToSend)

def FilebeatTail(FilebeatConfigName):
    """ This function will check if tailed file is still "alive" or "dead" in multiple way

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """
    

    # global TargetFileMTimeCountArray

    # Getting parameters from values.FilebeatConfigsArray
    ConfigSplitLine = values.FilebeatConfigsArray[FilebeatConfigName].split(',')
    CurrentTargetFile = ConfigSplitLine[0]
    TargetFile = ConfigSplitLine[1]
    PatternMatch = ConfigSplitLine[2]
    LastTailLine = ConfigSplitLine[3]
    StartInode = ConfigSplitLine[4]
    TargetSize = ConfigSplitLine[5]
    MultilineSeparator = ConfigSplitLine[6]
    Output = ConfigSplitLine[7]


    # Defining name for the thread
    FilebeatConfigThreadName = FilebeatConfigName + 'Thread'.replace('.','')
    FilebeatConfigThreadName = FilebeatConfigThreadName.replace('.','')

    # Checking if FilebeatTail process already running for this config file
    if (bool(re.search(FilebeatConfigThreadName, str(threading.enumerate())))):
        # This config has already a thread running, let's go checking CurrentTargetFile freshness
        # Log
        logging.info("   - Checking " + FilebeatConfigName + " Tail process status")

        # Checking if file is still here
        if not os.path.exists(CurrentTargetFile):
        # The file does not exists
            # Log
            logging.info("            * File is not there anymore: stopping Tail thread !")

            # Sending order to stop thread
            values.TailStateArray[FilebeatConfigName] = False

            # Resting last tailed line for CurrentTargetFile as the file has changed
            values.TargetFileCurrentPosArray[FilebeatConfigName] = 0

            # Exiting function
            return
        else:
            # Log
            logging.info("            * Presence check: Ok !")

        # Checking if CurrentTargetFile size is less then starting size
        CurrentTargetFileSize = os.stat(CurrentTargetFile).st_size
        if int(TargetSize) > CurrentTargetFileSize:
            # The reference size is less than the current size.
            # File has changed. Let's send order to stop background tail process on that CurrentTargetFile
            values.TailStateArray[FilebeatConfigName] = False

            # Resting last tailed line for CurrentTargetFile as the file has changed
            values.TargetFileCurrentPosArray[FilebeatConfigName] = 0

            # Log
            logging.info("            * The reference size is less than the current size: stopping Tail thread !")
            # Exiting function
            return
        else:
            # Log
            logging.info("            * Size check: Ok !")

        # Checking if CurrentTargetFile inode has changed
        CurrentTargetFileInode = os.stat(CurrentTargetFile).st_ino
        if int(StartInode) != CurrentTargetFileInode:
            # The inode has changed. Let's send order to stop background tail process on that CurrentTargetFile
            values.TailStateArray[FilebeatConfigName] = False

            # Resting last tailed line for CurrentTargetFile as the file has changed
            values.TargetFileCurrentPosArray[FilebeatConfigName] = 0

            # Log
            logging.info("            * The reference inode is different than the current inode: stopping Tail thread !")
            # Exiting function
            return
        else:
            # Log
            logging.info("            * Inode check: Ok !")
        
        # Converting date format from original TargetFile string, if any
        IfNewTargetFile = ConvertFileName(TargetFile)
        
        # Looking for corresponding file
        OSCommand = "ls -lart " + IfNewTargetFile + " | tail -n 1 | awk '{print $NF}' 2>/dev/null"
        CheckTargetFile = subprocess.Popen(OSCommand, shell=True, stdout=subprocess.PIPE, stderr = devnull).stdout
        CheckTargetFile = CheckTargetFile.read().decode().split()
  
        # Handling case where file not found from OS command
        if len(CheckTargetFile) != 0:
            # Checking if CurrentTargetFile name still match with original pattern
            if str(CheckTargetFile[0]) != CurrentTargetFile:
                # Mismatch detected, checking if a new file exists when applying Target File naming
                # Checking if file exists or not
                if os.path.exists(str(CheckTargetFile[0])):
                    # A new file currently exists
                    values.TailStateArray[FilebeatConfigName] = False
                    
                    # Resting last tailed line for CurrentTargetFile as the file has changed
                    values.TargetFileCurrentPosArray[FilebeatConfigName] = 0
                    
                    # Log
                    logging.info("            * File name pattern in configuration file does not match anymore with current file: stopping Tail thread !")
                    # Exiting function
                    return
            else:
                # Log
                logging.info("            * Refreshing filename: Ok !")  
        else:
            # No file exists corresponding to this name stopping, threads
            values.TailStateArray[FilebeatConfigName] = False
                
            # Resting last tailed line for CurrentTargetFile as the file has changed
            values.TargetFileCurrentPosArray[FilebeatConfigName] = 0
            
            # Log
            logging.info("            * File name pattern in configuration file does not match anymore with an existing file: stopping Tail thread !")
            # Exiting function
            return
    else:
        # No thread are running for this config, let's start a new one
        
        # Converting date format, if any, in target file  
        CurrentTargetFile = ConvertFileName(TargetFile)
        
        # Looking for corresponding file
        OSCommand = "ls -lart " + CurrentTargetFile + " | tail -n 1 | awk '{print $NF}' 2>/dev/null"
        CheckTargetFile = subprocess.Popen(OSCommand, shell=True, stdout=subprocess.PIPE, stderr = devnull).stdout
        CheckTargetFile = CheckTargetFile.read().decode().split()
        
        # Handling case where file not found from OS command
        if len(CheckTargetFile) != 0:
            # Checking if file exists or not
            if os.path.exists(str(CheckTargetFile[0])):
                # The file currently exists
                CurrentTargetFile = str(CheckTargetFile[0])
                
                # Log
                logging.info("    * Initiating tail process on " + CurrentTargetFile)

                # Setting thread as active
                values.TailStateArray[FilebeatConfigName] = True

                # Refreshing CurrentTargetFile inode and size
                CurrentTargetFileSize = str(os.stat(CurrentTargetFile).st_size)
                CurrentTargetFileInode = str(os.stat(CurrentTargetFile).st_ino)

                # Formating string and storing into
                ConfigString = CurrentTargetFile + ',' + TargetFile + ',' + PatternMatch + ',' + LastTailLine + ',' + CurrentTargetFileInode + ',' + CurrentTargetFileSize + ',' + MultilineSeparator + ',' + Output
                values.FilebeatConfigsArray[FilebeatConfigName] = ConfigString

                # Launching thread
                FilebeatConfigThread = threading.Thread(target=TailPythonStyle, name=FilebeatConfigThreadName, args=(FilebeatConfigName, CurrentTargetFile, PatternMatch, MultilineSeparator, Output))
                FilebeatConfigThread.start()
            else:
                # The file does not exists
                logging.info("    * Initiating tail process on Target File " + CurrentTargetFile + " impossible, the file does not exists !")
                
                # Initializing state value to start tail from line 1 when file will be available
                values.TargetFileCurrentPosArray[FilebeatConfigName] = 0
        else:
            # The file does not exists
            logging.info("    * Initiating tail process on Target File " + CurrentTargetFile + " impossible, the file does not exists !")
            
            # Initializing state value to start tail from line 1 when file will be available
            values.TargetFileCurrentPosArray[FilebeatConfigName] = 0

    # Debug
    # print('\nDEBUG\n')
    # print('<!> DEBUG ', 'ConfigSplitLine', ConfigSplitLine)
    # print('<!> DEBUG ', 'TargetFile', TargetFile)
    # print('<!> DEBUG ', 'TargetInfoPattern', TargetInfoPattern)
    # print('<!> DEBUG ', 'TargetWarningPattern', TargetWarningPattern)
    # print('<!> DEBUG ', 'TargetErrorPattern', TargetErrorPattern)
    # print('<!> DEBUG ', 'PatternMatch', PatternMatch)
    # print('<!> DEBUG ', 'LastTailLine', LastTailLine)
    # print('<!> DEBUG ', 'StartInode', StartInode)
    # print('<!> DEBUG ', 'TargetSize', TargetSize)
    # print('<!> DEBUG ', 'MultilineSeparator', MultilineSeparator)
    # print('\nFINDEBUG\n')
    pass

def TailPythonStyle(FilebeatConfigName, TargetFile, PatternMatch, MultilineSeparator, Output):
    """ This function will tail a txt file in python style, as tail does not esists :-(

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """ 
    
    # Remove "" from input vars
    PatternMatch = PatternMatch.replace('"','')
    MultilineSeparator = MultilineSeparator.replace('"','')

    # Setting vars for threads follow up
    CurrentThread = threading.current_thread()

    # Opening TargetFile
    OpenedTargetFile = open(TargetFile,'r')

    # Checking TargetFile size to start tail at the last line
    TargetFileStats = os.stat(TargetFile)
    TargetFileSize = TargetFileStats[6]

    # Positionning cursor at the end of the file
    # Checking if values.TargetFileCurrentPosArray[FilebeatConfigName] is empty with try/catch

    try:
        # If value exists in plugin state dictionary, that's to say parsing was already running in this daemon execution
        # We reset it to 0 to start parsing the new file from begining
        values.TargetFileCurrentPosArray[FilebeatConfigName]
        OpenedTargetFile.seek(0)
        
    except:
        # Na value inside dictionnary, that's to say this is the first daemon execution
        # We put the cursor on the last line of the file
        OpenedTargetFile.seek(TargetFileSize)

    # Multi-line mode
    
    if (len(MultilineSeparator) != 0):
        # Going to make all work into try/catch to avoid exception and daemon crash
        # if something unexpected happen to the TargetFile
        
        # Initializing vars
        LogLineStart = False
        SeqIdTimestamp = ''
        FinalLogLine = ''
        Matched = False
        TailTimer = 0
        
        try:
            # Start looping indefintly until TailStateArray[ThreadName] is different then True
            # It give control on this sub-thread from main thread to stop/clean it when necessary
            while (True and values.TailStateArray[FilebeatConfigName]):
                # Storing the current TargetFile's "last line" position
                values.TargetFileCurrentPosArray[FilebeatConfigName] = OpenedTargetFile.tell()
                # Gathering new lines if any
                NewLine = OpenedTargetFile.readline()

                # Checking if there are new lines happened to the TargetFile
                if not NewLine:
                    # No new line detected
                    # Then we set file position to the previous recoreded one
                    # to check any new lines in the next loop execution
                    OpenedTargetFile.seek(values.TargetFileCurrentPosArray[FilebeatConfigName])
                    
                    # Going to sleep 1 sec to save CPU
                    time.sleep(1)
                    
                    # Incrementing safety sleep before sending current message if matched
                    # Maybe a line will comes out ?!
                    if len(FinalLogLine) != 0:
                        TailTimer = TailTimer + 1
                    
                    # Checking if something to send and if we have wait enough
                    if TailTimer > 4:
                        if Matched:
                            # Removing \n at the end of the Message
                            # FinalLogLine = re.sub('\n', '\\\n', FinalLogLine)

                            # Formatting message
                            Message = re.sub('[\\\[\]\(\)\{\}"\b\t\a\r\n]', ' ', FinalLogLine)
                            
                            # Sending message
                            CustomJSONToSend = (''
                            ','
                            '\"log\":{'
                            '\"file\":{'
                            '\"path\":\"' + TargetFile + '\"'
                            '},'
                            '\"offset\":' + str(MatchLineOffset) + ','
                            '\"pattern\":\"' + PatternMatch + '\"},'
                            '\"message\":\"' + Message + '\",'
                            '\"input\":{'
                            '\"type\":\"log\"'
                            '},'
                            '\"service\":{'
                            '\"name\":\"customfilebeat\"'
                            '},'
                            '\"event\":{'
                            '\"timezone\":\"+00:00\",'
                            '\"module\":\"customfilebeat\",'
                            '\"dataset\":\"custom.' + FilebeatConfigName + '\"'
                            '}'
                            '')
                            
                            # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
                            JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + 'Z\",' + values.StaticJSONFilebeat + CustomJSONToSend + '}'
                            
                            # Let's send JSON message to Elastic, Filebeat or Logstash server
                            if Output == 'Metricbeat':
                                SendJSON(JSONToSend)
                            elif Output == 'Logstash':
                                SendJSON(JSONToSend, 'Logstash')
                            else:
                                SendJSON(JSONToSend, 'Filebeat')
                            # print('JSONToSend: ', JSONToSend)
                            
                        # Reseting vars for next log line
                        SeqIdTimestamp = ''
                        LogLineStart = False
                        Matched = False
                        FinalLogLine = ''
                        TailTimer = 0

                else:
                    # New line has been written, let's store datetime if not already
                    if (len(SeqIdTimestamp) == 0):
                        SeqIdTimestamp = datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%S.%f")
                    
                    # Checking if log line start
                    if (bool(re.search(MultilineSeparator, NewLine))) and (LogLineStart == False):
                        # Setting var to true
                        LogLineStart = True

                        # Storing line content
                        FinalLogLine = str(NewLine)
                        
                        # Checking if line content the pattern
                        if (bool(re.search(PatternMatch, NewLine)) and Matched == False):
                            Matched = True
                            
                            # Getting the current line of the file
                            MatchLineOffset = OpenedTargetFile.tell()
                        
                    elif (bool(re.search(MultilineSeparator, NewLine))) and (LogLineStart == True):
                        # New line detected, sending the message  if matched is detected
                        if Matched:
                            # Removing \n at the end of the Message
                            # FinalLogLine = re.sub('\n', '\\\n', FinalLogLine)
                            
                            # Formatting message
                            Message = re.sub('[\\\[\]\(\)\{\}"\b\t\a\r\n]', ' ', FinalLogLine)  
                            
                            # Sending message
                            CustomJSONToSend = (''
                            ','
                            '\"log\":{'
                            '\"file\":{'
                            '\"path\":\"' + TargetFile + '\"'
                            '},'
                            '\"offset\":' + str(MatchLineOffset) + ','
                            '\"pattern\":\"' + PatternMatch + '\"},'
                            '\"message\":\"' + Message + '\",'
                            '\"input\":{'
                            '\"type\":\"log\"'
                            '},'
                            '\"service\":{'
                            '\"name\":\"customfilebeat\"'
                            '},'
                            '\"event\":{'
                            '\"timezone\":\"+00:00\",'
                            '\"module\":\"customfilebeat\",'
                            '\"dataset\":\"custom.' + FilebeatConfigName + '\"'
                            '}'
                            '')
                            
                            # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
                            JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + 'Z\",' + values.StaticJSONFilebeat + CustomJSONToSend + '}'
                            
                            # Let's send JSON message to Elastic, Filebeat or Logstash server
                            if Output == 'Metricbeat':
                                SendJSON(JSONToSend)
                            elif Output == 'Logstash':
                                SendJSON(JSONToSend, 'Logstash')
                            else:
                                SendJSON(JSONToSend, 'Filebeat')
                            # print('JSONToSend: ', JSONToSend)
                        
                        # Reseting vars for next log line
                        SeqIdTimestamp = ''
                        LogLineStart = False
                        Matched = False
                        FinalLogLine = ''
                        TailTimer = 0

                        # Fetching file to the previous position
                        OpenedTargetFile.seek(values.TargetFileCurrentPosArray[FilebeatConfigName])
                        
                        # Going to the next loop execution
                        continue

                    else:    
                        # This line is the continuation of the previous one, let's add it and loop
                        FinalLogLine = FinalLogLine + str(NewLine)
                        
                        # Checking if line content the pattern
                        if (bool(re.search(PatternMatch, NewLine)) and Matched == False):
                            Matched = True
                            
                            # Getting the current line of the file
                            MatchLineOffset = OpenedTargetFile.tell()
  
                    # Going to sleep 0,1 sec to wait for next line for CPU reasons
                    time.sleep(0.1)   

            # When we reached that point, thread is dead by order
            # print('thread ' + CurrentThread.name + ' is dead by order')
            pass
        except:
            # Something happened to the thread, thread is dead by crash
            print('thread ' + CurrentThread.name + ' is dead by crash')
            # traceback.print_exc()
            pass
    else:
        # No Multi-line mode
        
        # Going to make all work into try/catch to avoid exception and daemon crash
        # if something unexpected happen to the TargetFile
        try:
            # Start looping indefintly until values.TailStateArray[ThreadName] is different then True
            # It give control on this sub-thread from main thread to stop/clean it when necessary
            while (True and values.TailStateArray[FilebeatConfigName]):
                # Storing the current TargetFile's "last line" position
                values.TargetFileCurrentPosArray[FilebeatConfigName] = OpenedTargetFile.tell()
                # Gathering new lines if any
                NewLine = OpenedTargetFile.readline()

                # Checking if there are new lines happened to the TargetFile
                if not NewLine:
                    # No new line detected
                    # Then we set file position to the previous recoreded one
                    # to check any new lines in the next loop execution
                    OpenedTargetFile.seek(values.TargetFileCurrentPosArray[FilebeatConfigName])
                    # Going to sleep 1 sec to save CPU
                    time.sleep(1)

                else:
                    # Removing \n at the end of thr readline() output
                    NewLine = re.sub('\n$', '', NewLine)
                    # New line has been written, let's store datetime and check for patterns
                    SeqIdTimestamp = datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%S.%f")
                    
                    # We check if pattern is detected
                    if bool(re.search(PatternMatch, NewLine)):
                        
                        # Getting the current line of the file
                        MatchLineOffset = OpenedTargetFile.tell()
                        
                        CustomJSONToSend = (''
                        ','
                        '\"log\":{'
                        '\"file\":{'
                        '\"path\":\"' + TargetFile + '\"'
                        '},'
                        '\"offset\":' + str(MatchLineOffset) + ','
                        '\"pattern\":\"' + PatternMatch + '\"},'
                        '\"message\":\"' + re.sub('[\\\[\]\(\)\{\}"\b\t\n\a\r]', ' ', NewLine) + '\",'
                        '\"input\":{'
                        '\"type\":\"log\"'
                        '},'
                        '\"service\":{'
                        '\"name\":\"customfilebeat\"'
                        '},'
                        '\"event\":{'
                        '\"timezone\":\"+00:00\",'
                        '\"module\":\"customfilebeat\",'
                        '\"dataset\":\"custom.' + FilebeatConfigName + '\"'
                        '}'
                        '')
                        
                        # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
                        JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + 'Z\",' + values.StaticJSONFilebeat + CustomJSONToSend + '}'
                        
                        # Let's send JSON message to Elastic, Filebeat or Logstash server
                        if Output == 'Metricbeat':
                            SendJSON(JSONToSend)
                        elif Output == 'Logstash':
                            SendJSON(JSONToSend, 'Logstash')
                        else:
                            SendJSON(JSONToSend, 'Filebeat')
                            
                        # print('JSONToSend: ', JSONToSend)

            # When we reached that point, thread is dead by order
            # print('thread ' + CurrentThread.name + ' is dead by order')
            pass
        except:
            # Something happened to the thread, thread is dead by crash
            print('thread ' + CurrentThread.name + ' is dead by crash')
            # traceback.print_exc()
            pass
    pass

def PingPlotter(ComparisonTimer):
    """ This function analyse ping against multiple configured targets and send metrics to ElasticSearch server

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """
    
    # Setting vars for threads follow up
    values.PingPlotterThread = threading.currentThread()
    
    # Making test ping for all target in config file
    for PingTarget in values.PingPlotterTargets.split(','):
        if len(PingTarget) != 0:
            # Generating current ELK timestamp for JSON message
            SeqIdTimestamp = datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%S.%f")
            
            # To move both fields to Parameter.conf file
            values.PingSamples = 5
            values.PingTimeout = "1s"
            
            # Configuring pingparsin package
            ping_parser = pingparsing.PingParsing()
            transmitter = pingparsing.PingTransmitter()

            # Setting ping destination and timeout
            transmitter.destination = PingTarget
            transmitter.timeout = values.PingTimeout

            # Setting the number of ping to execute
            transmitter.count = values.PingSamples
            PingResult = transmitter.ping()

            if 'returncode=0' in str(PingResult):
                # Ping is ok, let's format output and JSON message
                ResultDict = json.loads(json.dumps(ping_parser.parse(PingResult).as_dict()))
                
                # Let's construct JSON output for SystemPingPlotter(ELK 7.5.0)
                JSONToSendValues = (''
                '\"system\":{\"pingplotter\":{\"destination\":\"' + str(ResultDict["destination"]) + '\",'
                '\"packet_loss_rate\":' + str(float(ResultDict["packet_loss_rate"]) / 100) + ',\"rtt_avg\":' + str(ResultDict["rtt_avg"]) + ','
                '\"rtt_max\":' + str(ResultDict["rtt_max"]) + ',\"packet_duplicate_rate\":' + str(float(ResultDict["packet_duplicate_rate"]) / 100) + '}}')
               
                # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
                JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + 'Z\",' + values.StaticJSON
                
                # Adding JSON metrics values gathered from OS command
                JSONToSend = JSONToSend + ',' + JSONToSendValues
                
                # Gathering "ELK formatted" dynamic JSON values
                RetDynamicJSON = GenerateDynamicJson("system.pingplotter", "system", "pingplotter", ComparisonTimer)
                
                # Adding dynamic "ELK Formating" JSON values into final JSON message
                JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
                
                # Let's send JSON message to ELK servers
                SendJSON(JSONToSend)
                
                # print('\JSONToSend\n', JSONToSend, '\n')
                
            else:
                # An error has been thrown by pingparsing, maybe resolution error. 
                # Setting all counters to 0 and packet lost to 100%
                
                # Let's construct JSON output for SystemPingPlotter(ELK 7.5.0)
                JSONToSendValues = (''
                '\"system\":{\"pingplotter\":{\"destination\":\"' + PingTarget + '\",\"packet_loss_rate\":100,\"rtt_avg\":0,\"rtt_max\":0,\"packet_duplicate_rate\":0}}')
               
                # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
                JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + 'Z\",' + values.StaticJSON
                
                # Adding JSON metrics values gathered from OS command
                JSONToSend = JSONToSend + ',' + JSONToSendValues
                
                # Gathering "ELK formatted" dynamic JSON values
                RetDynamicJSON = GenerateDynamicJson("system.customping", "system", "customping", ComparisonTimer)
                
                # Adding dynamic "ELK Formating" JSON values into final JSON message
                JSONToSend = JSONToSend + ',' + RetDynamicJSON + '}'
                
                # Let's send JSON message to ELK servers
                SendJSON(JSONToSend)
                
                # print('\JSONToSend\n', JSONToSend, '\n')
                pass

def ErrptLog(ComparisonTimer):
    """ This function detects new Errorlogs and send them to ElasticSearch server

        Please refer to README.md
    
        TODO: Describe the function, all vars and its content
    """
    global LastErrpt
    
    # Checking if value is initialized with try/catch
    try:
        # Gathering last errorlog line
        CurrErrpt = subprocess.Popen("errpt -a | grep \"Sequence Number\" | awk '{print $NF}'", shell=True, stdout=subprocess.PIPE).stdout
        CurrErrpt = CurrErrpt.read().decode().split('\n')

        # Checking if last errlog entrie is the same or new
        if str(CurrErrpt[0]) not in str(LastErrpt):
            # New log detected, gathering its id in list
            for i in range(0, len(CurrErrpt)): 
                if str(CurrErrpt[i]) in str(LastErrpt): 
                    CurrErrptID = i
                    break
                    
            # Sending all required error log entries in correct order
            for i in reversed(range(0, CurrErrptID)):
                ErrorLogMessageCmd = 'errpt -l ' + str(CurrErrpt[i]) + ' | tail -n +2'
                ErrorLogMessage = subprocess.Popen(ErrorLogMessageCmd, shell=True, stdout=subprocess.PIPE).stdout
                ErrorLogMessage = ErrorLogMessage.read().decode()
                
                # Generating current ELK timestamp for JSON message
                SeqIdTimestamp = datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%S.%f")
                
                # Custom message
                CustomJSONToSend = (''
                '\"log\":{'
                '\"file\":{'
                '\"path\":\"AIX ERRORLOG\"'
                '},'
                '\"offset\":' + str(CurrErrptID) + ','
                '\"pattern\":\"AIX ERRORLOG\"},'
                '\"message\":\"' + re.sub('[\\\[\]\(\)\{\}"\b\t\n\a\r]', ' ', str(ErrorLogMessage)) + '\",'
                '\"input\":{'
                '\"type\":\"log\"'
                '},'
                '\"service\":{'
                '\"name\":\"aixerrorlog\"'
                '},'
                '\"event\":{'
                '\"timezone\":\"+00:00\",'
                '\"module\":\"aixerrorlog\",'
                '\"dataset\":\"custom.aixerrorlog\"'
                '}'
                '')
                
                # Adding Timestamp and static "ELK Formating" JSON values into final JSON message
                JSONToSend = '{\"@timestamp\":\"' + SeqIdTimestamp + 'Z\",' + values.StaticJSONFilebeat + ',' + CustomJSONToSend + '}'
                
                # Let's send JSON message to ELK Filebeat server
                SendJSON(JSONToSend, 'Filebeat')
                # print('JSONToSend: ', JSONToSend)
                
            # Setting last errorlog to the first occurence of the list
            LastErrpt = CurrErrpt[0]
            
    except:
        # First execution, gathering last errorlog line and setting LastErrpt
        LastErrpt = subprocess.Popen("errpt -a | grep \"Sequence Number\" | awk '{print $NF}' | head -1", shell=True, stdout=subprocess.PIPE).stdout
        LastErrpt = LastErrpt.read().decode()

